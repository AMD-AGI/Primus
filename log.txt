[NODE-0(gpu-40)] [INFO] ==========Training cluster info==========
[NODE-0(gpu-40)] [INFO] MASTER_ADDR: localhost
[NODE-0(gpu-40)] [INFO] MASTER_PORT: 1234
[NODE-0(gpu-40)] [INFO] NNODES: 1
[NODE-0(gpu-40)] [INFO] NODE_RANK: 0
[NODE-0(gpu-40)] [INFO] GPUS_PER_NODE: 8

WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.

[notice] A new release of pip is available: 25.0.1 -> 25.2
[notice] To update, run: pip install --upgrade pip
[NODE-0(gpu-40)] [INFO] ==========Training info==========
[NODE-0(gpu-40)] [INFO] EXP: examples/megatron/configs/llama3.1_8B-pretrain.yaml
[NODE-0(gpu-40)] [INFO] TRAIN_LOG: output/log_torchrun_pretrain_llama3.1_8B-pretrain.txt
[NODE-0(gpu-40)] [INFO] PRIMUS_PATH: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI
[NODE-0(gpu-40)] [INFO] DATA_PATH: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/data
[NODE-0(gpu-40)] [INFO] HF_HOME: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/data/huggingface

Info: Skipping bonded or storage device mlx5_bond_0
[NODE-0(gpu-40)] [INFO] ==========NCCL and Network Settings==========
[NODE-0(gpu-40)] [INFO] NCCL_DEBUG:
[NODE-0(gpu-40)] [INFO] NCCL_CHECKS_DISABLE: 1
[NODE-0(gpu-40)] [INFO] NCCL_IB_GID_INDEX: 3
[NODE-0(gpu-40)] [INFO] NCCL_CROSS_NIC: 0
[NODE-0(gpu-40)] [INFO] NCCL_IB_HCA: mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_9:1
[NODE-0(gpu-40)] [INFO] NCCL_SOCKET_IFNAME: ens6np0
[NODE-0(gpu-40)] [INFO] GLOO_SOCKET_IFNAME: ens6np0

[NODE-0(gpu-40)] [INFO] ==========AMD-specific GPU optimizations==========
[NODE-0(gpu-40)] [INFO] HSA_ENABLE_SDMA: 1
[NODE-0(gpu-40)] [INFO] HSA_NO_SCRATCH_RECLAIM: 0
[NODE-0(gpu-40)] [INFO] RCCL_MSCCL_ENABLE: 0
[NODE-0(gpu-40)] [INFO] RCCL_MSCCLPP_ENABLE: 0
[NODE-0(gpu-40)] [INFO] RCCL_MSCCLPP_FORCE_ENABLE: 0
[NODE-0(gpu-40)] [INFO] RCCL_MSCCLPP_THRESHOLD: 1073741824
[NODE-0(gpu-40)] [INFO] MSCCLPP_DISABLE_CHANNEL_CACHE: FALSE
[NODE-0(gpu-40)] [INFO] TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK: 0

[NODE-0(gpu-40)] [INFO] ==========Performance tuning==========
[NODE-0(gpu-40)] [INFO] GPU_MAX_HW_QUEUES: 2
[NODE-0(gpu-40)] [INFO] CUDA_DEVICE_MAX_CONNECTIONS: 1
[NODE-0(gpu-40)] [INFO] TORCH_NCCL_HIGH_PRIORITY: 1
[NODE-0(gpu-40)] [INFO] CUDA_DEVICE_MAX_CONNECTIONS: 1
[NODE-0(gpu-40)] [INFO] TORCH_NCCL_HIGH_PRIORITY: 1
[NODE-0(gpu-40)] [INFO] NCCL_PXN_DISABLE: 1
[NODE-0(gpu-40)] [INFO] NCCL_P2P_NET_CHUNKSIZE: 524288
[NODE-0(gpu-40)] [INFO] NVTE_CK_USES_BWD_V3: 1
[NODE-0(gpu-40)] [INFO] NVTE_USE_CAST_TRANSPOSE_TRITON: 1
[NODE-0(gpu-40)] [INFO] NVTE_USE_OPTIMIZED_HIPIFIED_CAST_TRANSPOSE: 0

[NODE-0(gpu-40)] [INFO] Skip bnxt rebuild. REBUILD_BNXT=0, PATH_TO_BNXT_TAR_PACKAGE=
[NODE-0(gpu-40)] [INFO] ========== Training tuning info ==========
[NODE-0(gpu-40)] [INFO] TE_HIPBLASLT_TUNING:
[NODE-0(gpu-40)] [INFO] TE_HIPBLASLT_TUNING_RUN_COUNT:
[NODE-0(gpu-40)] [INFO] TE_HIPBLASLT_TUNING_ALGO_COUNT:
[NODE-0(gpu-40)] [INFO] PRIMUS_HIPBLASLT_TUNING_STAGE:
[NODE-0(gpu-40)] [INFO] HIPBLASLT_LOG_MASK:
[NODE-0(gpu-40)] [INFO] HIPBLASLT_LOG_FILE:
[NODE-0(gpu-40)] [INFO] HIPBLASLT_LOG_LEVEL:
[NODE-0(gpu-40)] [INFO] HIPBLASLT_TUNING_OVERRIDE_FILE:

[NODE-0(gpu-40)] [INFO] Running backend prepare: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/examples/megatron/prepare.py
[NODE-0(gpu-40)] [INFO] ========== Prepare Megatron dataset ==========
[NODE-0(gpu-40)] [INFO] BACKEND_PATH None
[NODE-0(gpu-40)] [INFO] PRIMUS_PATH is set to: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI
[NODE-0(gpu-40)] [INFO] DATA_PATH is set to: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/data
[NODE-0(gpu-40)] [INFO] EXP is set to: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/examples/megatron/configs/llama3.1_8B-pretrain.yaml
[NODE-0(gpu-40)] [INFO] PATCH-ARGS is set to: /tmp/primus_patch_args.NLHyCX.yaml
[NODE-0(gpu-40)] [INFO] 'mock_data: true', Skipping dataset preparation.
[NODE-0(gpu-40)] [INFO] No backend_path provided, falling back to: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM
[NODE-0(gpu-40)] [INFO] Building Megatron dataset helper in /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/core/datasets
make: Nothing to be done for 'default'.
[NODE-0(gpu-40)] [INFO] Loading patch args from /tmp/primus_patch_args.NLHyCX.yaml
[NODE-0(gpu-40)] [INFO] Patched TRAIN args: --backend_path /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM
[NODE-0(gpu-40)] [INFO] Launching distributed training with command: torchrun --nproc_per_node 8 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 1234  primus/train.py --config examples/megatron/configs/llama3.1_8B-pretrain.yaml --backend_path /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM --framework light-megatron
W0814 05:36:09.100000 17023 torch/distributed/run.py:766]
W0814 05:36:09.100000 17023 torch/distributed/run.py:766] *****************************************
W0814 05:36:09.100000 17023 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0814 05:36:09.100000 17023 torch/distributed/run.py:766] *****************************************
[Primus] sys.path.insert: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM
[Primus] sys.path.insert: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM
[Primus] sys.path.insert: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM
[Primus] sys.path.insert: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM
[Primus] sys.path.insert: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM
[Primus] sys.path.insert: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM
[Primus] sys.path.insert: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM
[Primus] sys.path.insert: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM
[[32m20250814 05:36:09[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[------pre_trainer.py:21] : init light-megatron[0m
[[32m20250814 05:36:09[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:85] : [PatchArgs] set env 'CUDA_DEVICE_MAX_CONNECTIONS' to 1[0m
[[32m20250814 05:36:09[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:97] : [PatchArgs] -save:  /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/output/amd/root/llama3.1_8B-pretrain/checkpoints[0m
[[32m20250814 05:36:09[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[launcher_adapter.py:107] : [PatchArgs] -disable_tensorboard:True[0m
[[32m20250814 05:36:09[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[launcher_adapter.py:108] : [PatchArgs] -tensorboard_dir:None[0m
[[32m20250814 05:36:09[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:128]: [PatchArgs] args.wandb_project is disabled, as args.disable_wandb=True.[0m
[[32m20250814 05:36:09[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[launcher_adapter.py:129] : [PatchArgs] -disable_wandb:True[0m
[[32m20250814 05:36:09[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[launcher_adapter.py:135] : [PatchArgs]  -wandb_project:None[0m
[[32m20250814 05:36:09[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[launcher_adapter.py:136] : [PatchArgs]  -wandb_exp_name:None[0m
[[32m20250814 05:36:09[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[launcher_adapter.py:137] : [PatchArgs]  -wandb_save_dir:None[0m
[[32m20250814 05:36:09[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[launcher_adapter.py:138] : [PatchArgs]  -wandb_entity:None[0m
[[32m20250814 05:36:18[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1mSupported flash-attn versions are >= 2.1.1, <= 2.7.3. Found flash-attn 3.0.0.post1.[0m
[[32m20250814 05:36:18[0m][[36mrank-5/8[0m][[33m[1mWARNING[0m] [33m[1mSupported flash-attn versions are >= 2.1.1, <= 2.7.3. Found flash-attn 3.0.0.post1.[0m
[[32m20250814 05:36:18[0m][[36mrank-7/8[0m][[33m[1mWARNING[0m] [33m[1mSupported flash-attn versions are >= 2.1.1, <= 2.7.3. Found flash-attn 3.0.0.post1.[0m
[[32m20250814 05:36:18[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1mSupported flash-attn versions are >= 2.1.1, <= 2.7.3. Found flash-attn 3.0.0.post1.[0m
[[32m20250814 05:36:18[0m][[36mrank-6/8[0m][[33m[1mWARNING[0m] [33m[1mSupported flash-attn versions are >= 2.1.1, <= 2.7.3. Found flash-attn 3.0.0.post1.[0m
[[32m20250814 05:36:18[0m][[36mrank-3/8[0m][[33m[1mWARNING[0m] [33m[1mSupported flash-attn versions are >= 2.1.1, <= 2.7.3. Found flash-attn 3.0.0.post1.[0m
[[32m20250814 05:36:18[0m][[36mrank-4/8[0m][[33m[1mWARNING[0m] [33m[1mSupported flash-attn versions are >= 2.1.1, <= 2.7.3. Found flash-attn 3.0.0.post1.[0m
[[32m20250814 05:36:18[0m][[36mrank-2/8[0m][[33m[1mWARNING[0m] [33m[1mSupported flash-attn versions are >= 2.1.1, <= 2.7.3. Found flash-attn 3.0.0.post1.[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:37] : [MegatronLauncherAdapter] tokenizer_type 'Llama3Tokenizer' replaced with 'HuggingFaceTokenizer'[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: log_batch_size_to_tensorboard:True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: log_learning_rate_to_tensorboard:True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: trainable:True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: standalone_embedding_stage:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: log_avg_skip_iterations:2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: log_avg_reset_interval:50[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: inference_max_requests:8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: parallel_output:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: file_sink_level:DEBUG[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: stderr_sink_level:DEBUG[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: disable_tensorboard:True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: disable_wandb:True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: disable_compile_dependencies:True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: disable_profiler_activity_cpu:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: auto_continue_train:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: disable_last_saving:True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: no_fp8_weight_transpose_cache:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: attn_warmup:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: dump_pp_data:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: enable_primus_turbo:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: use_turbo_attention:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: use_turbo_row_parallel_linear:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: use_turbo_layer_norm_column_parallel_linear:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: use_turbo_column_parallel_linear:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: use_turbo_column_parallel_linear_torch:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: use_turbo_grouped_mlp:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: enable_turbo_attention_float8:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: enable_turbo_gemm_float8:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: name:pre_trainer[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: framework:light-megatron[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: disable_primus_topk_router:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: moe_router_force_load_balancing:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: use_deprecated_20241209_moe_layer:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: moe_use_fused_router_with_aux_score:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1m[launcher_adapter.py:197]: [PatchParseArgs] Unknown key ignored: fused_padded_mla_attention:False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_layers] = 32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[encoder_num_layers] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[decoder_num_layers] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[hidden_size] = 4096[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ffn_hidden_size] = 14336[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_attention_heads] = 32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[attention_backend] = AttnBackend.auto[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[kv_channels] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[group_query_attention] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_query_groups] = 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[max_position_embeddings] = 8192[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[position_embedding_type] = rope[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[relative_attention_num_buckets] = 32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[relative_attention_max_distance] = 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_rotary_position_embeddings] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[rotary_base] = 500000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[rotary_percent] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[rotary_interleaved] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[rotary_seq_len_interpolation_factor] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_rope_scaling] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[rope_scaling_factor] = 8.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[no_rope_freq] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[add_position_embedding] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mrope_section] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[make_vocab_size_divisible_by] = 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[normalization] = RMSNorm[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[norm_epsilon] = 1e-06[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[apply_layernorm_1p] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[apply_residual_connection_post_layernorm] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[openai_gelu] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[squared_relu] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[swiglu] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[onnx_safe] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[bert_binary_head] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[untie_embeddings_and_output_weights] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[multi_latent_attention] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mtp_num_layers] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mtp_loss_scaling_factor] = 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[attention_dropout] = 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[hidden_dropout] = 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[weight_decay] = 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[start_weight_decay] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[end_weight_decay] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[weight_decay_incr_style] = constant[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[clip_grad] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[adam_beta1] = 0.9[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[adam_beta2] = 0.95[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[adam_eps] = 1e-08[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[sgd_momentum] = 0.9[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[micro_batch_size] = 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[batch_size] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[global_batch_size] = 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[rampup_batch_size] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[decrease_batch_size_if_needed] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[recompute_activations] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[recompute_granularity] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[check_for_nan_in_loss_and_grad] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[check_for_spiky_loss] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[check_for_large_grads] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[distribute_saved_activations] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[recompute_method] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[recompute_num_layers] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[recompute_modules] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[clone_scatter_output_in_embedding] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[profile] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[profile_step_start] = 10[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[profile_step_end] = 12[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[iterations_to_skip] = [][0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[result_rejected_tracker_filename] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[enable_gloo_process_groups] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_pytorch_profiler] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[profile_ranks] = [0][0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[record_memory_history] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[memory_snapshot_path] = snapshot.pickle[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tp_comm_overlap] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tp_comm_overlap_cfg] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tp_comm_overlap_ag] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tp_comm_overlap_rs] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tp_comm_overlap_rs_dgrad] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tp_comm_bulk_dgrad] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tp_comm_bulk_wgrad] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tp_comm_bootstrap_backend] = nccl[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_cpu_initialization] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[empty_unused_memory_level] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[deterministic_mode] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[check_weight_hash_across_dp_replicas_interval] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[calculate_per_token_loss] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[train_sync_interval] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[checkpoint_activations] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[train_iters] = 50[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[train_samples] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[log_interval] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[exit_interval] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[exit_duration_in_mins] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[exit_signal_handler] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tensorboard_dir] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[masked_softmax_fusion] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[bias_gelu_fusion] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[bias_swiglu_fusion] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[bias_dropout_fusion] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[apply_rope_fusion] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[cross_entropy_loss_fusion] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[cross_entropy_fusion_impl] = native[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_flash_attn] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[add_bias_linear] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[add_qkv_bias] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[optimizer] = adam[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[optimizer_cpu_offload] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[optimizer_offload_fraction] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_torch_optimizer_for_cpu_offload] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[overlap_cpu_optimizer_d2h_h2d] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[pin_cpu_grads] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[pin_cpu_params] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dataloader_type] = cyclic[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[async_tensor_model_parallel_allreduce] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[no_persist_layer_norm] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[sequence_parallel] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[gradient_accumulation_fusion] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[deprecated_use_mcore_models] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_legacy_models] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[manual_gc] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[manual_gc_interval] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[manual_gc_eval] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tp_comm_split_ag] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tp_comm_split_rs] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[pipeline_model_parallel_comm_backend] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[seed] = 1234[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[data_parallel_random_init] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[init_method_std] = 0.008[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[init_method_xavier_uniform] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lr] = 1e-05[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lr_decay_style] = cosine[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lr_wsd_decay_style] = exponential[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lr_decay_iters] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lr_decay_samples] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lr_wsd_decay_samples] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lr_wsd_decay_iters] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lr_warmup_fraction] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lr_warmup_iters] = 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lr_warmup_samples] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lr_warmup_init] = 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[warmup] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[min_lr] = 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[override_opt_param_scheduler] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_checkpoint_opt_param_scheduler] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[decoupled_lr] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[decoupled_min_lr] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[save] = /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/output/amd/root/llama3.1_8B-pretrain/checkpoints[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[save_interval] = 20000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[no_save_optim] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[no_save_rng] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[load] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[no_load_optim] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[no_load_rng] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[non_persistent_save_interval] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[non_persistent_ckpt_type] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[non_persistent_global_ckpt_dir] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[non_persistent_local_ckpt_dir] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[non_persistent_local_ckpt_algo] = fully_parallel[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[finetune] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[pretrained_checkpoint] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ckpt_step] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[perform_initialization] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_checkpoint_args] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_mp_args_from_checkpoint_args] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_tokenizer_model_from_checkpoint_args] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[exit_on_missing_checkpoint] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_dist_ckpt_deprecated] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_persistent_ckpt_worker] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[auto_detect_ckpt_format] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dist_ckpt_format_deprecated] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ckpt_format] = torch[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ckpt_convert_format] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ckpt_convert_save] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ckpt_convert_update_legacy_dist_opt_format] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ckpt_fully_parallel_save_deprecated] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ckpt_fully_parallel_save] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[async_save] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ckpt_fully_parallel_load] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ckpt_assume_constant_structure] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dist_ckpt_strictness] = assume_ok_unexpected[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[fp16] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[bf16] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[grad_reduce_in_bf16] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[loss_scale] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[initial_loss_scale] = 4294967296.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[min_loss_scale] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[loss_scale_window] = 1000.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[hysteresis] = 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[fp32_residual_connection] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[apply_query_key_layer_scaling] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[attention_softmax_in_fp32] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[accumulate_allreduce_grads_in_fp32] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[fp16_lm_cross_entropy] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[disable_bf16_reduced_precision_matmul] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tensor_model_parallel_size] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[encoder_tensor_model_parallel_size] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[pipeline_model_parallel_size] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[encoder_pipeline_model_parallel_size] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[pipeline_model_parallel_split_rank] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[decoder_first_pipeline_num_layers] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[decoder_last_pipeline_num_layers] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[model_parallel_size] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_layers_per_virtual_pipeline_stage] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_virtual_stages_per_pipeline_rank] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[microbatch_group_size_per_vp_stage] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[overlap_p2p_comm] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[overlap_p2p_comm_warmup_flush] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[distributed_backend] = nccl[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[distributed_timeout_minutes] = 60[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[overlap_grad_reduce] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[defer_embedding_wgrad_compute] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[wgrad_deferral_limit] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[align_grad_reduce] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ddp_num_buckets] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ddp_bucket_size] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ddp_pad_buckets_for_high_nccl_busbw] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ddp_average_in_collective] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[overlap_param_gather] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[overlap_param_gather_with_optimizer_step] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[align_param_gather] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[scatter_gather_tensors_in_pipeline] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_ring_exchange_p2p] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[local_rank] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[lazy_mpu_init] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[account_for_embedding_in_pipeline_split] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[account_for_loss_in_pipeline_split] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_distributed_optimizer] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_custom_fsdp] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[init_model_with_meta_device] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[data_parallel_sharding_strategy] = no_shard[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[gradient_reduce_div_fusion] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[suggested_communication_unit_size] = 400000000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[keep_fp8_transpose_cache_when_using_custom_fsdp] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_distributed_optimizer_instances] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_torch_fsdp2] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[torch_fsdp2_reshard_after_forward] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[context_parallel_size] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[cp_comm_type] = ['p2p'][0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[hierarchical_context_parallel_sizes] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[nccl_communicator_config_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_tp_pp_dp_mapping] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[replication] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[replication_jump] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[replication_factor] = 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[eval_iters] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[eval_interval] = 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[test_mode] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[skip_train] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[data_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[split] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[train_data_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[valid_data_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[test_data_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[data_args_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[per_split_data_args_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[data_cache_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mmap_bin_files] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mock_data] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[seq_length] = 8192[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[encoder_seq_length] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[decoder_seq_length] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retriever_seq_length] = 256[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[sample_rate] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mask_prob] = 0.15[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[short_seq_prob] = 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_workers] = 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[reset_position_ids] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[reset_attention_mask] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[eod_mask_loss] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[create_attention_mask_in_dataloader] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_dataset_builder_threads] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[object_storage_cache_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mid_level_dataset_surplus] = 0.005[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[vocab_size] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[vocab_file] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[merge_file] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[vocab_extra_ids] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tokenizer_type] = HuggingFaceTokenizer[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tokenizer_model] = meta-llama/Llama-3.1-8B[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tiktoken_pattern] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tiktoken_num_special_tokens] = 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tiktoken_special_tokens] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[adlr_autoresume] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[adlr_autoresume_interval] = 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ict_head_size] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[biencoder_projection_dim] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[biencoder_shared_query_context_model] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[ict_load] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[bert_load] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[titles_data_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[query_in_block_prob] = 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_one_sent_docs] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[evidence_data_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retriever_report_topk_accuracies] = [][0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retriever_score_scaling] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[block_data_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[embedding_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[indexer_batch_size] = 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[indexer_log_interval] = 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_classes] = 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[img_h] = 224[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[img_w] = 224[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_channels] = 3[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[patch_dim] = 16[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[classes_fraction] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[data_per_class_fraction] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[data_sharding] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[head_lr_mult] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[vision_pretraining] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[vision_pretraining_type] = classify[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[vision_backbone_type] = vit[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[swin_backbone_type] = tiny[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mask_type] = random[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mask_factor] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[iter_per_epoch] = 1250[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dino_local_img_size] = 96[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dino_local_crops_number] = 10[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dino_head_hidden_size] = 2048[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dino_bottleneck_size] = 256[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dino_freeze_last_layer] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dino_norm_last_layer] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dino_warmup_teacher_temp] = 0.04[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dino_teacher_temp] = 0.07[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[dino_warmup_teacher_temp_epochs] = 30[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[qk_layernorm] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[expert_model_parallel_size] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[expert_tensor_parallel_size] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_experts] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_layer_freq] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_ffn_hidden_size] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_shared_expert_intermediate_size] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_shared_expert_overlap] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_grouped_gemm] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_use_legacy_grouped_gemm] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_layer_recompute] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_extended_tp] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_use_upcycling] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_router_load_balancing_type] = aux_loss[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_router_dtype] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_router_score_function] = softmax[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_router_topk] = 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_router_pre_softmax] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_router_num_groups] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_router_group_topk] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_router_topk_scaling_factor] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_router_enable_expert_bias] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_router_bias_update_rate] = 0.001[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_aux_loss_coeff] = 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_z_loss_coeff] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_input_jitter_eps] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_per_layer_logging] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_token_dispatcher_type] = allgather[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_enable_deepep] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_permute_fusion] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_expert_capacity_factor] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_pad_expert_input_to_capacity] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[moe_token_drop_policy] = probs[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[q_lora_rank] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[kv_lora_rank] = 32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[qk_head_dim] = 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[qk_pos_emb_head_dim] = 64[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[v_head_dim] = 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[rotary_scaling_factor] = 40.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mscale] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mscale_all_dim] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[heterogeneous_layers_config_path] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[heterogeneous_layers_config_encoded_json] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[log_params_norm] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[log_num_zeros_in_grad] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[log_throughput] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[log_progress] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[timing_log_level] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[barrier_with_L1_time] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[timing_log_option] = minmax[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tensorboard_log_interval] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[tensorboard_queue_size] = 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[log_timers_to_tensorboard] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[log_loss_scale_to_tensorboard] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[log_validation_ppl_to_tensorboard] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[log_memory_to_tensorboard] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[log_world_size_to_tensorboard] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[wandb_project] = [0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[wandb_exp_name] = [0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[wandb_save_dir] = [0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[logging_level] = 10[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[log_straggler] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[disable_straggler_on_startup] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[straggler_ctrlr_port] = 65535[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[straggler_minmax_count] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[run_workload_inspector_server] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inference_batch_times_seqlen_threshold] = -1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[max_tokens_to_oom] = 12000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[output_bert_embeddings] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[bert_embedder_type] = megatron[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[flash_decode] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[enable_cuda_graph] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[cuda_graph_warmup_steps] = 3[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[external_cuda_graph] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[cuda_graph_scope] = full[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inference_max_batch_size] = 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inference_max_seq_length] = 2560[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inference_dynamic_batching] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inference_dynamic_batching_buffer_size_gb] = 40.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inference_dynamic_batching_chunk_size] = 256[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inference_dynamic_batching_buffer_guaranteed_fraction] = 0.2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inference_dynamic_batching_buffer_overflow_factor] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inference_dynamic_batching_max_requests_override] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inference_dynamic_batching_max_tokens_override] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[symmetric_ar_type] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[nccl_all_reduce_for_prefill] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mlp_chunks_for_prefill] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[fp8] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[fp8_recipe] = delayed[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[fp8_margin] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[fp8_interval] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[fp8_amax_history_len] = 1024[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[fp8_amax_compute_algo] = max[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[fp8_wgrad] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[transformer_impl] = transformer_engine[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[fp8_param_gather] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[first_last_layers_bf16] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_layers_at_start_in_bf16] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[num_layers_at_end_in_bf16] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[te_rng_tracker] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inference_rng_tracker] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retro_project_dir] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retro_add_retriever] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retro_cyclic_train_iters] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retro_encoder_layers] = 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retro_encoder_hidden_dropout] = 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retro_encoder_attention_dropout] = 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retro_num_neighbors] = 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retro_num_retrieved_chunks] = 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retro_attention_gate] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[retro_verify_neighbor_count] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[spec] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[hybrid_attention_ratio] = 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[hybrid_mlp_ratio] = 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[hybrid_override_pattern] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mamba_state_dim] = 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mamba_head_dim] = 64[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mamba_num_groups] = 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[mamba_num_heads] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[is_hybrid_model] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[disable_mamba_mem_eff_path] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[yaml_cfg] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[use_precision_aware_optimizer] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[main_grads_dtype] = fp32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[main_params_dtype] = fp32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[exp_avg_dtype] = fp32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[exp_avg_sq_dtype] = fp32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[enable_one_logger] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[one_logger_project] = megatron-lm[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[one_logger_run_name] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[one_logger_async] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[app_tag_run_name] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[app_tag_run_version] = 0.0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_restart] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_max_iterations] = None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_monitor_thread_interval] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_monitor_process_interval] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_progress_watchdog_interval] = 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_heartbeat_interval] = 30[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_soft_timeout] = 60[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_hard_timeout] = 90[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_heartbeat_timeout] = 60[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_barrier_timeout] = 120[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_completion_timeout] = 120[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_last_call_wait] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_termination_grace_time] = 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_granularity] = node[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_active_world_size] = 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[inprocess_empty_cuda_cache] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[enable_ft_package] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[calc_ft_timeouts] = False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[config_logger_dir] = [0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[error_injection_rate] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[error_injection_type] = transient_error[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[rerun_mode] = disabled[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[enable_msc] = True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[world_size] = 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:56] : [MegatronLauncherAdapter] known_args[rank] = 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:61] : [MegatronLauncherAdapter] Successfully patched Megatron parse_args[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[------pre_trainer.py:30] : run light-megatron[0m
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/training/arguments.py:796: UserWarning: Disabling sequence parallelism because tensor model parallelism is disabled
  warnings.warn("Disabling sequence parallelism because tensor model parallelism is disabled")
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/training/arguments.py:796: UserWarning: Disabling sequence parallelism because tensor model parallelism is disabled
  warnings.warn("Disabling sequence parallelism because tensor model parallelism is disabled")
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/training/arguments.py:796: UserWarning: Disabling sequence parallelism because tensor model parallelism is disabled
  warnings.warn("Disabling sequence parallelism because tensor model parallelism is disabled")
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:364] : using world size: 8, data-parallel size: 8, context-parallel size: 1, hierarchical context-parallel sizes: Nonetensor-model-parallel size: 1, encoder-tensor-model-parallel size: 0, pipeline-model-parallel size: 1, encoder-pipeline-model-parallel size: 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:529] : Number of virtual stages per pipeline stage: None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:632] : accumulate and all-reduce gradients in fp32 for bfloat16 data type.[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------arguments.py:636] : using torch.bfloat16 for parameters ...[0m
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/training/arguments.py:796: UserWarning: Disabling sequence parallelism because tensor model parallelism is disabled
  warnings.warn("Disabling sequence parallelism because tensor model parallelism is disabled")
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1021] : ------------------------ arguments ------------------------[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   account_for_embedding_in_pipeline_split ......... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   account_for_loss_in_pipeline_split .............. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   accumulate_allreduce_grads_in_fp32 .............. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   adam_beta1 ...................................... 0.9[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   adam_beta2 ...................................... 0.95[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   adam_eps ........................................ 1e-08[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   add_bias_linear ................................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   add_position_embedding .......................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   add_qkv_bias .................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   adlr_autoresume ................................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   adlr_autoresume_interval ........................ 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   align_grad_reduce ............................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   align_param_gather .............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   app_tag_run_name ................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   app_tag_run_version ............................. 0.0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   apply_layernorm_1p .............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   apply_query_key_layer_scaling ................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   apply_residual_connection_post_layernorm ........ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   apply_rope_fusion ............................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   async_save ...................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   async_tensor_model_parallel_allreduce ........... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   attention_backend ............................... AttnBackend.auto[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   attention_dropout ............................... 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   attention_softmax_in_fp32 ....................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   auto_detect_ckpt_format ......................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   barrier_with_L1_time ............................ True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   bert_binary_head ................................ True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   bert_embedder_type .............................. megatron[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   bert_load ....................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   bf16 ............................................ True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   bias_dropout_fusion ............................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   bias_gelu_fusion ................................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   bias_swiglu_fusion .............................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   biencoder_projection_dim ........................ 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   biencoder_shared_query_context_model ............ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   block_data_path ................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   calc_ft_timeouts ................................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   calculate_per_token_loss ........................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   check_for_large_grads ........................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   check_for_nan_in_loss_and_grad .................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   check_for_spiky_loss ............................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   check_weight_hash_across_dp_replicas_interval ... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ckpt_assume_constant_structure .................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ckpt_convert_format ............................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ckpt_convert_save ............................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ckpt_convert_update_legacy_dist_opt_format ...... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ckpt_format ..................................... torch[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ckpt_fully_parallel_load ........................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ckpt_fully_parallel_save ........................ True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ckpt_fully_parallel_save_deprecated ............. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ckpt_step ....................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   classes_fraction ................................ 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   clip_grad ....................................... 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   clone_scatter_output_in_embedding ............... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   config_logger_dir ............................... [0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   consumed_train_samples .......................... 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   consumed_valid_samples .......................... 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   context_parallel_size ........................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   cp_comm_type .................................... ['p2p'][0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   create_attention_mask_in_dataloader ............. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   cross_entropy_fusion_impl ....................... native[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   cross_entropy_loss_fusion ....................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   cuda_graph_scope ................................ full[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   cuda_graph_warmup_steps ......................... 3[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   data_args_path .................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   data_cache_path ................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   data_parallel_random_init ....................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   data_parallel_sharding_strategy ................. no_shard[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   data_parallel_size .............................. 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   data_path ....................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   data_per_class_fraction ......................... 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   data_sharding ................................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dataloader_type ................................. cyclic[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ddp_average_in_collective ....................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ddp_bucket_size ................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ddp_num_buckets ................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ddp_pad_buckets_for_high_nccl_busbw ............. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   decoder_first_pipeline_num_layers ............... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   decoder_last_pipeline_num_layers ................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   decoder_num_layers .............................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   decoder_seq_length .............................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   decoupled_lr .................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   decoupled_min_lr ................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   decrease_batch_size_if_needed ................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   defer_embedding_wgrad_compute ................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   deprecated_use_mcore_models ..................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   deterministic_mode .............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dino_bottleneck_size ............................ 256[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dino_freeze_last_layer .......................... 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dino_head_hidden_size ........................... 2048[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dino_local_crops_number ......................... 10[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dino_local_img_size ............................. 96[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dino_norm_last_layer ............................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dino_teacher_temp ............................... 0.07[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dino_warmup_teacher_temp ........................ 0.04[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dino_warmup_teacher_temp_epochs ................. 30[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   disable_bf16_reduced_precision_matmul ........... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   disable_mamba_mem_eff_path ...................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   disable_straggler_on_startup .................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dist_ckpt_format_deprecated ..................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   dist_ckpt_strictness ............................ assume_ok_unexpected[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   distribute_saved_activations .................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   distributed_backend ............................. nccl[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   distributed_timeout_minutes ..................... 60[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   embedding_path .................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   empty_unused_memory_level ....................... 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   enable_cuda_graph ............................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   enable_ft_package ............................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   enable_gloo_process_groups ...................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   enable_msc ...................................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   enable_one_logger ............................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   encoder_num_layers .............................. 32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   encoder_pipeline_model_parallel_size ............ 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   encoder_seq_length .............................. 8192[0m
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/training/arguments.py:796: UserWarning: Disabling sequence parallelism because tensor model parallelism is disabled
  warnings.warn("Disabling sequence parallelism because tensor model parallelism is disabled")
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/training/arguments.py:796: UserWarning: Disabling sequence parallelism because tensor model parallelism is disabled
  warnings.warn("Disabling sequence parallelism because tensor model parallelism is disabled")
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   encoder_tensor_model_parallel_size .............. 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   end_weight_decay ................................ 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   eod_mask_loss ................................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   error_injection_rate ............................ 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   error_injection_type ............................ transient_error[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   eval_interval ................................... 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   eval_iters ...................................... 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   evidence_data_path .............................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   exit_duration_in_mins ........................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   exit_interval ................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   exit_on_missing_checkpoint ...................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   exit_signal_handler ............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   exp_avg_dtype ................................... torch.float32[0m
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/training/arguments.py:796: UserWarning: Disabling sequence parallelism because tensor model parallelism is disabled
  warnings.warn("Disabling sequence parallelism because tensor model parallelism is disabled")
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   exp_avg_sq_dtype ................................ torch.float32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   expert_model_parallel_size ...................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   expert_tensor_parallel_size ..................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   external_cuda_graph ............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ffn_hidden_size ................................. 14336[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   finetune ........................................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   first_last_layers_bf16 .......................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   flash_decode .................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   fp16 ............................................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   fp16_lm_cross_entropy ........................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   fp32_residual_connection ........................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   fp8 ............................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   fp8_amax_compute_algo ........................... max[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   fp8_amax_history_len ............................ 1024[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   fp8_interval .................................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   fp8_margin ...................................... 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   fp8_param_gather ................................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   fp8_recipe ...................................... delayed[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   fp8_wgrad ....................................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   global_batch_size ............................... 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   grad_reduce_in_bf16 ............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   gradient_accumulation_fusion .................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   gradient_reduce_div_fusion ...................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   group_query_attention ........................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   head_lr_mult .................................... 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   heterogeneous_layers_config_encoded_json ........ None[0m
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/training/arguments.py:796: UserWarning: Disabling sequence parallelism because tensor model parallelism is disabled
  warnings.warn("Disabling sequence parallelism because tensor model parallelism is disabled")
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   heterogeneous_layers_config_path ................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   hidden_dropout .................................. 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   hidden_size ..................................... 4096[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   hierarchical_context_parallel_sizes ............. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   hybrid_attention_ratio .......................... 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   hybrid_mlp_ratio ................................ 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   hybrid_override_pattern ......................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   hysteresis ...................................... 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ict_head_size ................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   ict_load ........................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   img_h ........................................... 224[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   img_w ........................................... 224[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   indexer_batch_size .............................. 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   indexer_log_interval ............................ 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inference_batch_times_seqlen_threshold .......... -1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inference_dynamic_batching ...................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inference_dynamic_batching_buffer_guaranteed_fraction  0.2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inference_dynamic_batching_buffer_overflow_factor  None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inference_dynamic_batching_buffer_size_gb ....... 40.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inference_dynamic_batching_chunk_size ........... 256[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inference_dynamic_batching_max_requests_override  None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inference_dynamic_batching_max_tokens_override .. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inference_max_batch_size ........................ 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inference_max_seq_length ........................ 2560[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inference_rng_tracker ........................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   init_method_std ................................. 0.008[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   init_method_xavier_uniform ...................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   init_model_with_meta_device ..................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   initial_loss_scale .............................. 4294967296.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_active_world_size ..................... 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_barrier_timeout ....................... 120[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_completion_timeout .................... 120[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_empty_cuda_cache ...................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_granularity ........................... node[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_hard_timeout .......................... 90[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_heartbeat_interval .................... 30[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_heartbeat_timeout ..................... 60[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_last_call_wait ........................ 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_max_iterations ........................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_monitor_process_interval .............. 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_monitor_thread_interval ............... 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_progress_watchdog_interval ............ 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_restart ............................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_soft_timeout .......................... 60[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   inprocess_termination_grace_time ................ 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   is_hybrid_model ................................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   iter_per_epoch .................................. 1250[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   iterations_to_skip .............................. [][0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   keep_fp8_transpose_cache_when_using_custom_fsdp . False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   kv_channels ..................................... 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   kv_lora_rank .................................... 32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lazy_mpu_init ................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   load ............................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   local_rank ...................................... 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   log_interval .................................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   log_loss_scale_to_tensorboard ................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   log_memory_to_tensorboard ....................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   log_num_zeros_in_grad ........................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   log_params_norm ................................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   log_progress .................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   log_straggler ................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   log_throughput .................................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   log_timers_to_tensorboard ....................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   log_validation_ppl_to_tensorboard ............... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   log_world_size_to_tensorboard ................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   logging_level ................................... 10[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   loss_scale ...................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   loss_scale_window ............................... 1000.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lr .............................................. 1e-05[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lr_decay_iters .................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lr_decay_samples ................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lr_decay_style .................................. cosine[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lr_warmup_fraction .............................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lr_warmup_init .................................. 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lr_warmup_iters ................................. 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lr_warmup_samples ............................... 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lr_wsd_decay_iters .............................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lr_wsd_decay_samples ............................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   lr_wsd_decay_style .............................. exponential[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   main_grads_dtype ................................ torch.float32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   main_params_dtype ............................... torch.float32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   make_vocab_size_divisible_by .................... 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mamba_head_dim .................................. 64[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mamba_num_groups ................................ 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mamba_num_heads ................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mamba_state_dim ................................. 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   manual_gc ....................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   manual_gc_eval .................................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   manual_gc_interval .............................. 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mask_factor ..................................... 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mask_prob ....................................... 0.15[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mask_type ....................................... random[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   masked_softmax_fusion ........................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   max_position_embeddings ......................... 8192[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   max_tokens_to_oom ............................... 12000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   memory_snapshot_path ............................ snapshot.pickle[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   merge_file ...................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   micro_batch_size ................................ 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   microbatch_group_size_per_vp_stage .............. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mid_level_dataset_surplus ....................... 0.005[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   min_loss_scale .................................. 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   min_lr .......................................... 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mlp_chunks_for_prefill .......................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mmap_bin_files .................................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mock_data ....................................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_aux_loss_coeff .............................. 0.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_enable_deepep ............................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_expert_capacity_factor ...................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_extended_tp ................................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_ffn_hidden_size ............................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_grouped_gemm ................................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_input_jitter_eps ............................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_layer_freq .................................. 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_layer_recompute ............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_pad_expert_input_to_capacity ................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_per_layer_logging ........................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_permute_fusion .............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_router_bias_update_rate ..................... 0.001[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_router_dtype ................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_router_enable_expert_bias ................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_router_group_topk ........................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_router_load_balancing_type .................. aux_loss[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_router_num_groups ........................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_router_pre_softmax .......................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_router_score_function ....................... softmax[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_router_topk ................................. 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_router_topk_scaling_factor .................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_shared_expert_intermediate_size ............. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_shared_expert_overlap ....................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_token_dispatcher_type ....................... allgather[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_token_drop_policy ........................... probs[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_use_legacy_grouped_gemm ..................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_use_upcycling ............................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   moe_z_loss_coeff ................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mrope_section ................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mscale .......................................... 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mscale_all_dim .................................. 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mtp_loss_scaling_factor ......................... 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   mtp_num_layers .................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   multi_latent_attention .......................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   nccl_all_reduce_for_prefill ..................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   nccl_communicator_config_path ................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   no_load_optim ................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   no_load_rng ..................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   no_persist_layer_norm ........................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   no_rope_freq .................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   no_save_optim ................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   no_save_rng ..................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   non_persistent_ckpt_type ........................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   non_persistent_global_ckpt_dir .................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   non_persistent_local_ckpt_algo .................. fully_parallel[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   non_persistent_local_ckpt_dir ................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   non_persistent_save_interval .................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   norm_epsilon .................................... 1e-06[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   normalization ................................... RMSNorm[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_attention_heads ............................. 32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_channels .................................... 3[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_classes ..................................... 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_dataset_builder_threads ..................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_distributed_optimizer_instances ............. 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_experts ..................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_layers ...................................... 32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_layers_at_end_in_bf16 ....................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_layers_at_start_in_bf16 ..................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_layers_per_virtual_pipeline_stage ........... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_query_groups ................................ 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_virtual_stages_per_pipeline_rank ............ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   num_workers ..................................... 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   object_storage_cache_path ....................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   one_logger_async ................................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   one_logger_project .............................. megatron-lm[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   one_logger_run_name ............................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   onnx_safe ....................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   openai_gelu ..................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   optimizer ....................................... adam[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   optimizer_cpu_offload ........................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   optimizer_offload_fraction ...................... 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   output_bert_embeddings .......................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   overlap_cpu_optimizer_d2h_h2d ................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   overlap_grad_reduce ............................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   overlap_p2p_comm ................................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   overlap_p2p_comm_warmup_flush ................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   overlap_param_gather ............................ True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   overlap_param_gather_with_optimizer_step ........ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   override_opt_param_scheduler .................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   params_dtype .................................... torch.bfloat16[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   patch_dim ....................................... 16[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   per_split_data_args_path ........................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   perform_initialization .......................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   pin_cpu_grads ................................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   pin_cpu_params .................................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   pipeline_model_parallel_comm_backend ............ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   pipeline_model_parallel_size .................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   pipeline_model_parallel_split_rank .............. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   position_embedding_type ......................... rope[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   pretrained_checkpoint ........................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   profile ......................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   profile_ranks ................................... [0][0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   profile_step_end ................................ 12[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   profile_step_start .............................. 10[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   q_lora_rank ..................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   qk_head_dim ..................................... 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   qk_layernorm .................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   qk_pos_emb_head_dim ............................. 64[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   query_in_block_prob ............................. 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   rampup_batch_size ............................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   rank ............................................ 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   recompute_granularity ........................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   recompute_method ................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   recompute_modules ............................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   recompute_num_layers ............................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   record_memory_history ........................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   relative_attention_max_distance ................. 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   relative_attention_num_buckets .................. 32[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   replication ..................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   replication_factor .............................. 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   replication_jump ................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   rerun_mode ...................................... disabled[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   reset_attention_mask ............................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   reset_position_ids .............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   result_rejected_tracker_filename ................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retriever_report_topk_accuracies ................ [][0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retriever_score_scaling ......................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retriever_seq_length ............................ 256[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retro_add_retriever ............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retro_attention_gate ............................ 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retro_cyclic_train_iters ........................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retro_encoder_attention_dropout ................. 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retro_encoder_hidden_dropout .................... 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retro_encoder_layers ............................ 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retro_num_neighbors ............................. 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retro_num_retrieved_chunks ...................... 2[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retro_project_dir ............................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   retro_verify_neighbor_count ..................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   rope_scaling_factor ............................. 8.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   rotary_base ..................................... 500000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   rotary_interleaved .............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   rotary_percent .................................. 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   rotary_scaling_factor ........................... 40.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   rotary_seq_len_interpolation_factor ............. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   run_workload_inspector_server ................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   sample_rate ..................................... 1.0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   save ............................................ /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/output/amd/root/llama3.1_8B-pretrain/checkpoints[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   save_interval ................................... 20000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   scatter_gather_tensors_in_pipeline .............. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   seed ............................................ 1234[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   seq_length ...................................... 8192[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   sequence_parallel ............................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   sgd_momentum .................................... 0.9[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   short_seq_prob .................................. 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   skip_train ...................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   skipped_train_samples ........................... 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   spec ............................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   split ........................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   squared_relu .................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   start_weight_decay .............................. 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   straggler_ctrlr_port ............................ 65535[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   straggler_minmax_count .......................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   suggested_communication_unit_size ............... 400000000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   swiglu .......................................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   swin_backbone_type .............................. tiny[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   symmetric_ar_type ............................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   te_rng_tracker .................................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tensor_model_parallel_size ...................... 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tensorboard_dir ................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tensorboard_log_interval ........................ 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tensorboard_queue_size .......................... 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   test_data_path .................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   test_mode ....................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tiktoken_num_special_tokens ..................... 1000[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tiktoken_pattern ................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tiktoken_special_tokens ......................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   timing_log_level ................................ 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   timing_log_option ............................... minmax[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   titles_data_path ................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tokenizer_model ................................. meta-llama/Llama-3.1-8B[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tokenizer_type .................................. HuggingFaceTokenizer[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   torch_fsdp2_reshard_after_forward ............... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tp_comm_bootstrap_backend ....................... nccl[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tp_comm_bulk_dgrad .............................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tp_comm_bulk_wgrad .............................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tp_comm_overlap ................................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tp_comm_overlap_ag .............................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tp_comm_overlap_cfg ............................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tp_comm_overlap_rs .............................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tp_comm_overlap_rs_dgrad ........................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tp_comm_split_ag ................................ True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   tp_comm_split_rs ................................ True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   train_data_path ................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   train_iters ..................................... 50[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   train_samples ................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   train_sync_interval ............................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   transformer_impl ................................ transformer_engine[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   transformer_pipeline_model_parallel_size ........ 1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   untie_embeddings_and_output_weights ............. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_checkpoint_args ............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_checkpoint_opt_param_scheduler .............. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_cpu_initialization .......................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_custom_fsdp ................................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_dist_ckpt ................................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_dist_ckpt_deprecated ........................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_distributed_optimizer ....................... True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_flash_attn .................................. True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_legacy_models ............................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_mp_args_from_checkpoint_args ................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_one_sent_docs ............................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_persistent_ckpt_worker ...................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_precision_aware_optimizer ................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_pytorch_profiler ............................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_ring_exchange_p2p ........................... False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_rope_scaling ................................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_rotary_position_embeddings .................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_tokenizer_model_from_checkpoint_args ........ True[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_torch_fsdp2 ................................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_torch_optimizer_for_cpu_offload ............. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   use_tp_pp_dp_mapping ............................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   v_head_dim ...................................... 128[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   valid_data_path ................................. None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   variable_seq_lengths ............................ False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   virtual_pipeline_model_parallel_size ............ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   vision_backbone_type ............................ vit[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   vision_pretraining .............................. False[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   vision_pretraining_type ......................... classify[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   vocab_extra_ids ................................. 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   vocab_file ...................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   vocab_size ...................................... None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   wandb_exp_name .................................. [0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   wandb_project ................................... [0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   wandb_save_dir .................................. [0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   weight_decay .................................... 0.1[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   weight_decay_incr_style ......................... constant[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   wgrad_deferral_limit ............................ 0[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   world_size ...................................... 8[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1028] :   yaml_cfg ........................................ None[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------arguments.py:1029] : -------------------- end of arguments ---------------------[0m
[[32m20250814 05:36:20[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[--------tokenizer.py:23] : > building HuggingFaceTokenizer tokenizer ...[0m
[[32m20250814 05:36:21[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----global_vars.py:215] : WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it[0m
[[32m20250814 05:36:21[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------tokenizer.py:117] :  > padded vocab (size: 128256) with 0 dummy tokens (new size: 128256)[0m
[[32m20250814 05:36:21[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1mRerunStateMachine initialized in mode disabled[0m
[[32m20250814 05:36:21[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------initialize.py:315] : > initializing torch distributed ...[0m
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank [Gloo] Rank 56 is connected to  is connected to 77 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 77

[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank [Gloo] Rank 6 is connected to 77 peer ranks.  is connected to Expected number of connected peer ranks is : 77 peer ranks. Expected number of connected peer ranks is :
7
[[32m20250814 05:36:21[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------initialize.py:366] : > initialized tensor model parallel with size 1[0m
[[32m20250814 05:36:21[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------initialize.py:370] : > initialized pipeline model parallel with size 1[0m
[[32m20250814 05:36:21[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[------initialize.py:125] : > setting random seeds to 1234 ...[0m
[[32m20250814 05:36:21[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m[-launcher_adapter.py:67] : [MegatronLauncherAdapter] Skipped Megatron _compile_dependencies()[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : time to initialize megatron (seconds): 3.607[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : [after megatron is initialized] datetime: 2025-08-14 05:36:24 [0m
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:94: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:94: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:94: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:94: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:94: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:94: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:94: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : building GPT model ...[0m
/shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:94: UserWarning: The fp8 argument in "get_gpt_layer_with_transformer_engine_spec" has been deprecated and will be removed soon. Please update your code accordingly.
  warnings.warn(
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-------training.py:1050] :  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 8030261248[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1mSetting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=40000000, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=400000000, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False)[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1mNumber of buckets for gradient all-reduce / reduce-scatter: 98
Params for bucket 1 (525336576 elements, 525336576 padded size):
	module.output_layer.weight
Params for bucket 2 (58724352 elements, 58724352 padded size):
	module.decoder.final_layernorm.weight
	module.decoder.layers.31.mlp.linear_fc2.weight
Params for bucket 3 (117440512 elements, 117440512 padded size):
	module.decoder.layers.31.mlp.linear_fc1.weight
Params for bucket 4 (41951232 elements, 41951232 padded size):
	module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.31.self_attention.linear_qkv.weight
	module.decoder.layers.31.self_attention.linear_proj.weight
Params for bucket 5 (58720256 elements, 58720256 padded size):
	module.decoder.layers.30.mlp.linear_fc2.weight
Params for bucket 6 (117440512 elements, 117440512 padded size):
	module.decoder.layers.30.mlp.linear_fc1.weight
Params for bucket 7 (41951232 elements, 41951232 padded size):
	module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.30.self_attention.linear_qkv.weight
	module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.30.self_attention.linear_proj.weight
Params for bucket 8 (58720256 elements, 58720256 padded size):
	module.decoder.layers.29.mlp.linear_fc2.weight
Params for bucket 9 (117440512 elements, 117440512 padded size):
	module.decoder.layers.29.mlp.linear_fc1.weight
Params for bucket 10 (41951232 elements, 41951232 padded size):
	module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.29.self_attention.linear_proj.weight
	module.decoder.layers.29.self_attention.linear_qkv.weight
	module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight
Params for bucket 11 (58720256 elements, 58720256 padded size):
	module.decoder.layers.28.mlp.linear_fc2.weight
Params for bucket 12 (117440512 elements, 117440512 padded size):
	module.decoder.layers.28.mlp.linear_fc1.weight
Params for bucket 13 (41951232 elements, 41951232 padded size):
	module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.28.self_attention.linear_qkv.weight
	module.decoder.layers.28.self_attention.linear_proj.weight
Params for bucket 14 (58720256 elements, 58720256 padded size):
	module.decoder.layers.27.mlp.linear_fc2.weight
Params for bucket 15 (117440512 elements, 117440512 padded size):
	module.decoder.layers.27.mlp.linear_fc1.weight
Params for bucket 16 (41951232 elements, 41951232 padded size):
	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.27.self_attention.linear_qkv.weight
	module.decoder.layers.27.self_attention.linear_proj.weight
	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
Params for bucket 17 (58720256 elements, 58720256 padded size):
	module.decoder.layers.26.mlp.linear_fc2.weight
Params for bucket 18 (117440512 elements, 117440512 padded size):
	module.decoder.layers.26.mlp.linear_fc1.weight
Params for bucket 19 (41951232 elements, 41951232 padded size):
	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.26.self_attention.linear_proj.weight
	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.26.self_attention.linear_qkv.weight
Params for bucket 20 (58720256 elements, 58720256 padded size):
	module.decoder.layers.25.mlp.linear_fc2.weight
Params for bucket 21 (117440512 elements, 117440512 padded size):
	module.decoder.layers.25.mlp.linear_fc1.weight
Params for bucket 22 (41951232 elements, 41951232 padded size):
	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.25.self_attention.linear_qkv.weight
	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.25.self_attention.linear_proj.weight
Params for bucket 23 (58720256 elements, 58720256 padded size):
	module.decoder.layers.24.mlp.linear_fc2.weight
Params for bucket 24 (117440512 elements, 117440512 padded size):
	module.decoder.layers.24.mlp.linear_fc1.weight
Params for bucket 25 (41951232 elements, 41951232 padded size):
	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.24.self_attention.linear_proj.weight
	module.decoder.layers.24.self_attention.linear_qkv.weight
Params for bucket 26 (58720256 elements, 58720256 padded size):
	module.decoder.layers.23.mlp.linear_fc2.weight
Params for bucket 27 (117440512 elements, 117440512 padded size):
	module.decoder.layers.23.mlp.linear_fc1.weight
Params for bucket 28 (41951232 elements, 41951232 padded size):
	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.23.self_attention.linear_qkv.weight
	module.decoder.layers.23.self_attention.linear_proj.weight
Params for bucket 29 (58720256 elements, 58720256 padded size):
	module.decoder.layers.22.mlp.linear_fc2.weight
Params for bucket 30 (117440512 elements, 117440512 padded size):
	module.decoder.layers.22.mlp.linear_fc1.weight
Params for bucket 31 (41951232 elements, 41951232 padded size):
	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.22.self_attention.linear_qkv.weight
	module.decoder.layers.22.self_attention.linear_proj.weight
	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
Params for bucket 32 (58720256 elements, 58720256 padded size):
	module.decoder.layers.21.mlp.linear_fc2.weight
Params for bucket 33 (117440512 elements, 117440512 padded size):
	module.decoder.layers.21.mlp.linear_fc1.weight
Params for bucket 34 (41951232 elements, 41951232 padded size):
	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.21.self_attention.linear_qkv.weight
	module.decoder.layers.21.self_attention.linear_proj.weight
Params for bucket 35 (58720256 elements, 58720256 padded size):
	module.decoder.layers.20.mlp.linear_fc2.weight
Params for bucket 36 (117440512 elements, 117440512 padded size):
	module.decoder.layers.20.mlp.linear_fc1.weight
Params for bucket 37 (41951232 elements, 41951232 padded size):
	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.20.self_attention.linear_qkv.weight
	module.decoder.layers.20.self_attention.linear_proj.weight
Params for bucket 38 (58720256 elements, 58720256 padded size):
	module.decoder.layers.19.mlp.linear_fc2.weight
Params for bucket 39 (117440512 elements, 117440512 padded size):
	module.decoder.layers.19.mlp.linear_fc1.weight
Params for bucket 40 (41951232 elements, 41951232 padded size):
	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.19.self_attention.linear_qkv.weight
	module.decoder.layers.19.self_attention.linear_proj.weight
Params for bucket 41 (58720256 elements, 58720256 padded size):
	module.decoder.layers.18.mlp.linear_fc2.weight
Params for bucket 42 (117440512 elements, 117440512 padded size):
	module.decoder.layers.18.mlp.linear_fc1.weight
Params for bucket 43 (41951232 elements, 41951232 padded size):
	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.18.self_attention.linear_qkv.weight
	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.18.self_attention.linear_proj.weight
Params for bucket 44 (58720256 elements, 58720256 padded size):
	module.decoder.layers.17.mlp.linear_fc2.weight
Params for bucket 45 (117440512 elements, 117440512 padded size):
	module.decoder.layers.17.mlp.linear_fc1.weight
Params for bucket 46 (41951232 elements, 41951232 padded size):
	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.17.self_attention.linear_proj.weight
	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.17.self_attention.linear_qkv.weight
Params for bucket 47 (58720256 elements, 58720256 padded size):
	module.decoder.layers.16.mlp.linear_fc2.weight
Params for bucket 48 (117440512 elements, 117440512 padded size):
	module.decoder.layers.16.mlp.linear_fc1.weight
Params for bucket 49 (41951232 elements, 41951232 padded size):
	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.16.self_attention.linear_qkv.weight
	module.decoder.layers.16.self_attention.linear_proj.weight
Params for bucket 50 (58720256 elements, 58720256 padded size):
	module.decoder.layers.15.mlp.linear_fc2.weight
Params for bucket 51 (117440512 elements, 117440512 padded size):
	module.decoder.layers.15.mlp.linear_fc1.weight
Params for bucket 52 (41951232 elements, 41951232 padded size):
	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.15.self_attention.linear_qkv.weight
	module.decoder.layers.15.self_attention.linear_proj.weight
	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
Params for bucket 53 (58720256 elements, 58720256 padded size):
	module.decoder.layers.14.mlp.linear_fc2.weight
Params for bucket 54 (117440512 elements, 117440512 padded size):
	module.decoder.layers.14.mlp.linear_fc1.weight
Params for bucket 55 (41951232 elements, 41951232 padded size):
	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.14.self_attention.linear_proj.weight
	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.14.self_attention.linear_qkv.weight
Params for bucket 56 (58720256 elements, 58720256 padded size):
	module.decoder.layers.13.mlp.linear_fc2.weight
Params for bucket 57 (117440512 elements, 117440512 padded size):
	module.decoder.layers.13.mlp.linear_fc1.weight
Params for bucket 58 (41951232 elements, 41951232 padded size):
	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.13.self_attention.linear_qkv.weight
	module.decoder.layers.13.self_attention.linear_proj.weight
Params for bucket 59 (58720256 elements, 58720256 padded size):
	module.decoder.layers.12.mlp.linear_fc2.weight
Params for bucket 60 (117440512 elements, 117440512 padded size):
	module.decoder.layers.12.mlp.linear_fc1.weight
Params for bucket 61 (41951232 elements, 41951232 padded size):
	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.12.self_attention.linear_qkv.weight
	module.decoder.layers.12.self_attention.linear_proj.weight
	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
Params for bucket 62 (58720256 elements, 58720256 padded size):
	module.decoder.layers.11.mlp.linear_fc2.weight
Params for bucket 63 (117440512 elements, 117440512 padded size):
	module.decoder.layers.11.mlp.linear_fc1.weight
Params for bucket 64 (41951232 elements, 41951232 padded size):
	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.11.self_attention.linear_qkv.weight
	module.decoder.layers.11.self_attention.linear_proj.weight
	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
Params for bucket 65 (58720256 elements, 58720256 padded size):
	module.decoder.layers.10.mlp.linear_fc2.weight
Params for bucket 66 (117440512 elements, 117440512 padded size):
	module.decoder.layers.10.mlp.linear_fc1.weight
Params for bucket 67 (41951232 elements, 41951232 padded size):
	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.10.self_attention.linear_qkv.weight
	module.decoder.layers.10.self_attention.linear_proj.weight
	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
Params for bucket 68 (58720256 elements, 58720256 padded size):
	module.decoder.layers.9.mlp.linear_fc2.weight
Params for bucket 69 (117440512 elements, 117440512 padded size):
	module.decoder.layers.9.mlp.linear_fc1.weight
Params for bucket 70 (41951232 elements, 41951232 padded size):
	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.9.self_attention.linear_proj.weight
	module.decoder.layers.9.self_attention.linear_qkv.weight
	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
Params for bucket 71 (58720256 elements, 58720256 padded size):
	module.decoder.layers.8.mlp.linear_fc2.weight
Params for bucket 72 (117440512 elements, 117440512 padded size):
	module.decoder.layers.8.mlp.linear_fc1.weight
Params for bucket 73 (41951232 elements, 41951232 padded size):
	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.8.self_attention.linear_qkv.weight
	module.decoder.layers.8.self_attention.linear_proj.weight
Params for bucket 74 (58720256 elements, 58720256 padded size):
	module.decoder.layers.7.mlp.linear_fc2.weight
Params for bucket 75 (117440512 elements, 117440512 padded size):
	module.decoder.layers.7.mlp.linear_fc1.weight
Params for bucket 76 (41951232 elements, 41951232 padded size):
	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.7.self_attention.linear_qkv.weight
	module.decoder.layers.7.self_attention.linear_proj.weight
	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
Params for bucket 77 (58720256 elements, 58720256 padded size):
	module.decoder.layers.6.mlp.linear_fc2.weight
Params for bucket 78 (117440512 elements, 117440512 padded size):
	module.decoder.layers.6.mlp.linear_fc1.weight
Params for bucket 79 (41951232 elements, 41951232 padded size):
	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.6.self_attention.linear_proj.weight
	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.6.self_attention.linear_qkv.weight
Params for bucket 80 (58720256 elements, 58720256 padded size):
	module.decoder.layers.5.mlp.linear_fc2.weight
Params for bucket 81 (117440512 elements, 117440512 padded size):
	module.decoder.layers.5.mlp.linear_fc1.weight
Params for bucket 82 (41951232 elements, 41951232 padded size):
	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.5.self_attention.linear_qkv.weight
	module.decoder.layers.5.self_attention.linear_proj.weight
Params for bucket 83 (58720256 elements, 58720256 padded size):
	module.decoder.layers.4.mlp.linear_fc2.weight
Params for bucket 84 (117440512 elements, 117440512 padded size):
	module.decoder.layers.4.mlp.linear_fc1.weight
Params for bucket 85 (41951232 elements, 41951232 padded size):
	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.4.self_attention.linear_proj.weight
	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.4.self_attention.linear_qkv.weight
Params for bucket 86 (58720256 elements, 58720256 padded size):
	module.decoder.layers.3.mlp.linear_fc2.weight
Params for bucket 87 (117440512 elements, 117440512 padded size):
	module.decoder.layers.3.mlp.linear_fc1.weight
Params for bucket 88 (41951232 elements, 41951232 padded size):
	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.3.self_attention.linear_proj.weight
	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.3.self_attention.linear_qkv.weight
Params for bucket 89 (58720256 elements, 58720256 padded size):
	module.decoder.layers.2.mlp.linear_fc2.weight
Params for bucket 90 (117440512 elements, 117440512 padded size):
	module.decoder.layers.2.mlp.linear_fc1.weight
Params for bucket 91 (41951232 elements, 41951232 padded size):
	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.2.self_attention.linear_proj.weight
	module.decoder.layers.2.self_attention.linear_qkv.weight
	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
Params for bucket 92 (58720256 elements, 58720256 padded size):
	module.decoder.layers.1.mlp.linear_fc2.weight
Params for bucket 93 (117440512 elements, 117440512 padded size):
	module.decoder.layers.1.mlp.linear_fc1.weight
Params for bucket 94 (41951232 elements, 41951232 padded size):
	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.1.self_attention.linear_proj.weight
	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.1.self_attention.linear_qkv.weight
Params for bucket 95 (58720256 elements, 58720256 padded size):
	module.decoder.layers.0.mlp.linear_fc2.weight
Params for bucket 96 (117440512 elements, 117440512 padded size):
	module.decoder.layers.0.mlp.linear_fc1.weight
Params for bucket 97 (41951232 elements, 41951232 padded size):
	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.0.self_attention.linear_qkv.weight
	module.decoder.layers.0.self_attention.linear_proj.weight
Params for bucket 98 (525336576 elements, 525336576 padded size):
	module.embedding.word_embeddings.weight[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1mSetting up optimizer with config OptimizerConfig(optimizer='adam', lr=1e-05, min_lr=0.0, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, store_param_remainders=True, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296.0, min_loss_scale=1.0, loss_scale_window=1000.0, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=1.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x7ecfc9baf1f0>, config_logger_dir='')[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m> learning rate decay style: cosine[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : [after model, optimizer, and learning rate scheduler are built] datetime: 2025-08-14 05:36:24 [0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : > building train, validation, and test datasets ...[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] :  > datasets target sizes (minimum size):[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] :     train:      6400[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] :     validation: 0[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] :     test:       0[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1mLet mock = True, as both blend and blend_per_split are None[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1mLet split = 1,1,1, an arbitrarily even split, as mock is True[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1mLet split_matrix = [(0, 0.3333333333333333), (0.3333333333333333, 0.6666666666666666), (0.6666666666666666, 1.0)][0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : > building train, validation, and test datasets for GPT ...[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1mBuilding MockGPTDataset splits with sizes=(6400, 0, 0) and config=GPTDatasetConfig(random_seed=1234, sequence_length=8192, blend=None, blend_per_split=None, split='1,1,1', split_matrix=[(0, 0.3333333333333333), (0.3333333333333333, 0.6666666666666666), (0.6666666666666666, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=True, tokenizer=<megatron.training.tokenizer.tokenizer._HuggingFaceTokenizer object at 0x7ecfc9baddb0>, mid_level_dataset_surplus=0.005, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=True, create_attention_mask=True, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, object_storage_cache_path=None)[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1mBuild and save the MockGPTDataset train indices[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m> separate_final_epoch: False[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1mUnable to save MockGPTDataset indexes because path_to_cache is None[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m	> time elapsed: 0.003736 seconds[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m> total number of samples: 8324[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m> total number of epochs: 1[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1mBuild and save the MockGPTDataset valid indices[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m> separate_final_epoch: False[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1mUnable to save MockGPTDataset indexes because path_to_cache is None[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m	> time elapsed: 0.001326 seconds[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m> total number of samples: 8320[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m> total number of epochs: 1[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1mBuild and save the MockGPTDataset test indices[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m> separate_final_epoch: False[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[33m[1mWARNING[0m] [33m[1mUnable to save MockGPTDataset indexes because path_to_cache is None[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m	> time elapsed: 0.001326 seconds[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m> total number of samples: 8335[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[1mINFO [0m] [1m> total number of epochs: 1[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : > finished creating GPT datasets ...[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : [after dataloaders are built] datetime: 2025-08-14 05:36:24 [0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : done with setup ...[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : training ...[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : Setting rerun_state_machine.current_iteration to 0...[0m
[[32m20250814 05:36:24[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[----------timers.py:417] : (min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (169.08, 198.03)
    train/valid/test-data-iterators-setup ..........: (14.41, 15.42)[0m
[[32m20250814 05:36:24[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : [before the start of training step] datetime: 2025-08-14 05:36:24 [0m
[[32m20250814 05:36:47[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:36:47] iteration        1/      50 | consumed samples:          128 | elapsed time per iteration (ms): 23102.2 | throughput per GPU (TFLOP/s/GPU): 328.6 | learning rate: 5.000000E-06 | global batch size:   128 | lm loss: 1.189762E+01 | loss scale: 1.0 | grad norm: 7.721 | num zeros: 508582464.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:36:47[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:136] : Number of parameters in transformer block in billions:  6.98[0m
[[32m20250814 05:36:47[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:145] : Number of parameters in embedding layers in billions: 1.05[0m
[[32m20250814 05:36:47[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:149] : Total number of parameters in billions: 8.03[0m
[[32m20250814 05:36:47[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:162] : Number of parameters in most loaded shard in billions: 8.0305[0m
[[32m20250814 05:36:47[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[theoretical_memory_usage.py:273] : Theoretical memory footprints: weight and optimizer=57438.81 MB[0m
[[32m20250814 05:36:47[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:263] : [Rank 0] (after 1 iterations) memory (MB) | allocated: 57474.271484375 | max allocated: 136020.943359375 | reserved: 141650.0 | max reserved: 141650.0[0m
[[32m20250814 05:37:02[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:37:02] iteration        2/      50 | consumed samples:          256 | elapsed time per iteration (ms): 14992.8 | throughput per GPU (TFLOP/s/GPU): 506.3 | learning rate: 1.000000E-05 | global batch size:   128 | lm loss: 1.189780E+01 | loss scale: 1.0 | grad norm: 7.329 | num zeros: 508587168.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:37:17[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:37:17] iteration        3/      50 | consumed samples:          384 | elapsed time per iteration (ms): 15169.7 | throughput per GPU (TFLOP/s/GPU): 500.4 | learning rate: 9.989295E-06 | global batch size:   128 | lm loss: 1.127075E+01 | loss scale: 1.0 | grad norm: 11.533 | num zeros: 508715840.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:37:32[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:37:32] iteration        4/      50 | consumed samples:          512 | elapsed time per iteration (ms): 15158.6 | throughput per GPU (TFLOP/s/GPU): 500.8 | learning rate: 9.957224E-06 | global batch size:   128 | lm loss: 1.075739E+01 | loss scale: 1.0 | grad norm: 23.871 | num zeros: 508577600.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:37:47[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:37:47] iteration        5/      50 | consumed samples:          640 | elapsed time per iteration (ms): 15081.3 | throughput per GPU (TFLOP/s/GPU): 503.3 | learning rate: 9.903926E-06 | global batch size:   128 | lm loss: 9.704892E+00 | loss scale: 1.0 | grad norm: 86.969 | num zeros: 508638176.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:38:02[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:38:02] iteration        6/      50 | consumed samples:          768 | elapsed time per iteration (ms): 15002.5 | throughput per GPU (TFLOP/s/GPU): 506.0 | learning rate: 9.829630E-06 | global batch size:   128 | lm loss: 8.584443E+00 | loss scale: 1.0 | grad norm: 39.302 | num zeros: 508589408.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:38:17[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:38:17] iteration        7/      50 | consumed samples:          896 | elapsed time per iteration (ms): 14980.8 | throughput per GPU (TFLOP/s/GPU): 506.7 | learning rate: 9.734651E-06 | global batch size:   128 | lm loss: 7.419193E+00 | loss scale: 1.0 | grad norm: 58.927 | num zeros: 508578624.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:38:32[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:38:32] iteration        8/      50 | consumed samples:         1024 | elapsed time per iteration (ms): 14957.8 | throughput per GPU (TFLOP/s/GPU): 507.5 | learning rate: 9.619398E-06 | global batch size:   128 | lm loss: 6.547956E+00 | loss scale: 1.0 | grad norm: 63.474 | num zeros: 508605056.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:38:47[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:38:47] iteration        9/      50 | consumed samples:         1152 | elapsed time per iteration (ms): 14980.2 | throughput per GPU (TFLOP/s/GPU): 506.7 | learning rate: 9.484364E-06 | global batch size:   128 | lm loss: 5.493181E+00 | loss scale: 1.0 | grad norm: 79.904 | num zeros: 508581888.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:39:02[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:39:02] iteration       10/      50 | consumed samples:         1280 | elapsed time per iteration (ms): 14955.1 | throughput per GPU (TFLOP/s/GPU): 507.6 | learning rate: 9.330127E-06 | global batch size:   128 | lm loss: 4.407827E+00 | loss scale: 1.0 | grad norm: 59.689 | num zeros: 508591296.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:39:17[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:39:17] iteration       11/      50 | consumed samples:         1408 | elapsed time per iteration (ms): 14979.5 | throughput per GPU (TFLOP/s/GPU): 506.7 | learning rate: 9.157348E-06 | global batch size:   128 | lm loss: 3.368375E+00 | loss scale: 1.0 | grad norm: 50.234 | num zeros: 508762944.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:39:32[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:39:32] iteration       12/      50 | consumed samples:         1536 | elapsed time per iteration (ms): 14981.4 | throughput per GPU (TFLOP/s/GPU): 506.7 | learning rate: 8.966766E-06 | global batch size:   128 | lm loss: 2.579489E+00 | loss scale: 1.0 | grad norm: 40.599 | num zeros: 508582848.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:39:47[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:39:47] iteration       13/      50 | consumed samples:         1664 | elapsed time per iteration (ms): 14973.0 | throughput per GPU (TFLOP/s/GPU): 507.0 | learning rate: 8.759199E-06 | global batch size:   128 | lm loss: 2.027591E+00 | loss scale: 1.0 | grad norm: 33.508 | num zeros: 508712704.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:40:02[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:40:02] iteration       14/      50 | consumed samples:         1792 | elapsed time per iteration (ms): 14991.5 | throughput per GPU (TFLOP/s/GPU): 506.3 | learning rate: 8.535534E-06 | global batch size:   128 | lm loss: 1.397127E+00 | loss scale: 1.0 | grad norm: 23.444 | num zeros: 508626176.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:40:17[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:40:17] iteration       15/      50 | consumed samples:         1920 | elapsed time per iteration (ms): 14974.0 | throughput per GPU (TFLOP/s/GPU): 506.9 | learning rate: 8.296729E-06 | global batch size:   128 | lm loss: 1.148622E+00 | loss scale: 1.0 | grad norm: 18.228 | num zeros: 508588928.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:40:32[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:40:32] iteration       16/      50 | consumed samples:         2048 | elapsed time per iteration (ms): 14976.0 | throughput per GPU (TFLOP/s/GPU): 506.9 | learning rate: 8.043807E-06 | global batch size:   128 | lm loss: 8.670896E-01 | loss scale: 1.0 | grad norm: 14.105 | num zeros: 508580288.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:40:47[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:40:47] iteration       17/      50 | consumed samples:         2176 | elapsed time per iteration (ms): 14965.0 | throughput per GPU (TFLOP/s/GPU): 507.2 | learning rate: 7.777851E-06 | global batch size:   128 | lm loss: 6.604484E-01 | loss scale: 1.0 | grad norm: 10.290 | num zeros: 508591712.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:41:02[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:41:02] iteration       18/      50 | consumed samples:         2304 | elapsed time per iteration (ms): 14963.3 | throughput per GPU (TFLOP/s/GPU): 507.3 | learning rate: 7.500000E-06 | global batch size:   128 | lm loss: 4.553697E-01 | loss scale: 1.0 | grad norm: 7.695 | num zeros: 508578880.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:41:17[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:41:17] iteration       19/      50 | consumed samples:         2432 | elapsed time per iteration (ms): 14958.2 | throughput per GPU (TFLOP/s/GPU): 507.5 | learning rate: 7.211444E-06 | global batch size:   128 | lm loss: 4.014392E-01 | loss scale: 1.0 | grad norm: 7.046 | num zeros: 508587040.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:41:32[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:41:32] iteration       20/      50 | consumed samples:         2560 | elapsed time per iteration (ms): 14974.0 | throughput per GPU (TFLOP/s/GPU): 506.9 | learning rate: 6.913417E-06 | global batch size:   128 | lm loss: 2.942573E-01 | loss scale: 1.0 | grad norm: 5.438 | num zeros: 508579328.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:41:47[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:41:47] iteration       21/      50 | consumed samples:         2688 | elapsed time per iteration (ms): 14960.3 | throughput per GPU (TFLOP/s/GPU): 507.4 | learning rate: 6.607198E-06 | global batch size:   128 | lm loss: 2.663080E-01 | loss scale: 1.0 | grad norm: 4.679 | num zeros: 508590848.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:42:02[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:42:02] iteration       22/      50 | consumed samples:         2816 | elapsed time per iteration (ms): 14962.2 | throughput per GPU (TFLOP/s/GPU): 507.3 | learning rate: 6.294095E-06 | global batch size:   128 | lm loss: 1.935463E-01 | loss scale: 1.0 | grad norm: 3.172 | num zeros: 508596800.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:42:17[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:42:17] iteration       23/      50 | consumed samples:         2944 | elapsed time per iteration (ms): 14980.9 | throughput per GPU (TFLOP/s/GPU): 506.7 | learning rate: 5.975452E-06 | global batch size:   128 | lm loss: 1.942864E-01 | loss scale: 1.0 | grad norm: 3.318 | num zeros: 508579616.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:42:32[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:42:32] iteration       24/      50 | consumed samples:         3072 | elapsed time per iteration (ms): 14948.2 | throughput per GPU (TFLOP/s/GPU): 507.8 | learning rate: 5.652631E-06 | global batch size:   128 | lm loss: 1.490396E-01 | loss scale: 1.0 | grad norm: 2.429 | num zeros: 508584320.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:42:47[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:42:47] iteration       25/      50 | consumed samples:         3200 | elapsed time per iteration (ms): 14967.7 | throughput per GPU (TFLOP/s/GPU): 507.1 | learning rate: 5.327016E-06 | global batch size:   128 | lm loss: 1.563600E-01 | loss scale: 1.0 | grad norm: 2.466 | num zeros: 508628832.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:43:02[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:43:02] iteration       26/      50 | consumed samples:         3328 | elapsed time per iteration (ms): 14969.7 | throughput per GPU (TFLOP/s/GPU): 507.1 | learning rate: 5.000000E-06 | global batch size:   128 | lm loss: 1.415503E-01 | loss scale: 1.0 | grad norm: 2.209 | num zeros: 508580512.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:43:17[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:43:17] iteration       27/      50 | consumed samples:         3456 | elapsed time per iteration (ms): 14962.0 | throughput per GPU (TFLOP/s/GPU): 507.3 | learning rate: 4.672984E-06 | global batch size:   128 | lm loss: 1.256710E-01 | loss scale: 1.0 | grad norm: 2.034 | num zeros: 508589312.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:43:32[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:43:32] iteration       28/      50 | consumed samples:         3584 | elapsed time per iteration (ms): 14961.0 | throughput per GPU (TFLOP/s/GPU): 507.4 | learning rate: 4.347369E-06 | global batch size:   128 | lm loss: 1.106647E-01 | loss scale: 1.0 | grad norm: 1.760 | num zeros: 508577952.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:43:47[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:43:47] iteration       29/      50 | consumed samples:         3712 | elapsed time per iteration (ms): 14958.3 | throughput per GPU (TFLOP/s/GPU): 507.5 | learning rate: 4.024549E-06 | global batch size:   128 | lm loss: 1.031251E-01 | loss scale: 1.0 | grad norm: 1.427 | num zeros: 508578048.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:44:02[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:44:02] iteration       30/      50 | consumed samples:         3840 | elapsed time per iteration (ms): 14943.0 | throughput per GPU (TFLOP/s/GPU): 508.0 | learning rate: 3.705905E-06 | global batch size:   128 | lm loss: 1.030112E-01 | loss scale: 1.0 | grad norm: 1.368 | num zeros: 508594592.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:44:17[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:44:17] iteration       31/      50 | consumed samples:         3968 | elapsed time per iteration (ms): 14942.9 | throughput per GPU (TFLOP/s/GPU): 508.0 | learning rate: 3.392803E-06 | global batch size:   128 | lm loss: 8.510015E-02 | loss scale: 1.0 | grad norm: 1.278 | num zeros: 508635968.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:44:31[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:44:31] iteration       32/      50 | consumed samples:         4096 | elapsed time per iteration (ms): 14949.7 | throughput per GPU (TFLOP/s/GPU): 507.8 | learning rate: 3.086583E-06 | global batch size:   128 | lm loss: 8.467266E-02 | loss scale: 1.0 | grad norm: 1.120 | num zeros: 508644416.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:44:46[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:44:46] iteration       33/      50 | consumed samples:         4224 | elapsed time per iteration (ms): 14940.2 | throughput per GPU (TFLOP/s/GPU): 508.1 | learning rate: 2.788556E-06 | global batch size:   128 | lm loss: 8.677052E-02 | loss scale: 1.0 | grad norm: 1.268 | num zeros: 508639136.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:45:01[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:45:01] iteration       34/      50 | consumed samples:         4352 | elapsed time per iteration (ms): 14956.1 | throughput per GPU (TFLOP/s/GPU): 507.5 | learning rate: 2.500000E-06 | global batch size:   128 | lm loss: 7.862242E-02 | loss scale: 1.0 | grad norm: 1.162 | num zeros: 508595264.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:45:16[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:45:16] iteration       35/      50 | consumed samples:         4480 | elapsed time per iteration (ms): 14935.2 | throughput per GPU (TFLOP/s/GPU): 508.2 | learning rate: 2.222149E-06 | global batch size:   128 | lm loss: 7.620388E-02 | loss scale: 1.0 | grad norm: 0.982 | num zeros: 508595776.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:45:31[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:45:31] iteration       36/      50 | consumed samples:         4608 | elapsed time per iteration (ms): 14963.8 | throughput per GPU (TFLOP/s/GPU): 507.3 | learning rate: 1.956193E-06 | global batch size:   128 | lm loss: 6.910028E-02 | loss scale: 1.0 | grad norm: 1.148 | num zeros: 508587072.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:45:46[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:45:46] iteration       37/      50 | consumed samples:         4736 | elapsed time per iteration (ms): 14946.7 | throughput per GPU (TFLOP/s/GPU): 507.9 | learning rate: 1.703271E-06 | global batch size:   128 | lm loss: 6.830523E-02 | loss scale: 1.0 | grad norm: 0.826 | num zeros: 508584352.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:46:01[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:46:01] iteration       38/      50 | consumed samples:         4864 | elapsed time per iteration (ms): 14967.8 | throughput per GPU (TFLOP/s/GPU): 507.1 | learning rate: 1.464466E-06 | global batch size:   128 | lm loss: 6.864508E-02 | loss scale: 1.0 | grad norm: 1.020 | num zeros: 508579328.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:46:16[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:46:16] iteration       39/      50 | consumed samples:         4992 | elapsed time per iteration (ms): 14946.5 | throughput per GPU (TFLOP/s/GPU): 507.9 | learning rate: 1.240801E-06 | global batch size:   128 | lm loss: 6.407010E-02 | loss scale: 1.0 | grad norm: 0.804 | num zeros: 508584832.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:46:31[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:46:31] iteration       40/      50 | consumed samples:         5120 | elapsed time per iteration (ms): 14956.0 | throughput per GPU (TFLOP/s/GPU): 507.5 | learning rate: 1.033233E-06 | global batch size:   128 | lm loss: 5.875165E-02 | loss scale: 1.0 | grad norm: 0.714 | num zeros: 508589792.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:46:46[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:46:46] iteration       41/      50 | consumed samples:         5248 | elapsed time per iteration (ms): 14959.7 | throughput per GPU (TFLOP/s/GPU): 507.4 | learning rate: 8.426520E-07 | global batch size:   128 | lm loss: 6.403349E-02 | loss scale: 1.0 | grad norm: 1.177 | num zeros: 508598976.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:47:01[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:47:01] iteration       42/      50 | consumed samples:         5376 | elapsed time per iteration (ms): 14946.4 | throughput per GPU (TFLOP/s/GPU): 507.9 | learning rate: 6.698730E-07 | global batch size:   128 | lm loss: 5.736997E-02 | loss scale: 1.0 | grad norm: 0.685 | num zeros: 508599232.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:47:16[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:47:16] iteration       43/      50 | consumed samples:         5504 | elapsed time per iteration (ms): 14971.5 | throughput per GPU (TFLOP/s/GPU): 507.0 | learning rate: 5.156363E-07 | global batch size:   128 | lm loss: 6.101957E-02 | loss scale: 1.0 | grad norm: 0.799 | num zeros: 508586368.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:47:31[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:47:31] iteration       44/      50 | consumed samples:         5632 | elapsed time per iteration (ms): 14944.8 | throughput per GPU (TFLOP/s/GPU): 507.9 | learning rate: 3.806023E-07 | global batch size:   128 | lm loss: 6.350552E-02 | loss scale: 1.0 | grad norm: 0.816 | num zeros: 508585920.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:47:46[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:47:46] iteration       45/      50 | consumed samples:         5760 | elapsed time per iteration (ms): 14963.1 | throughput per GPU (TFLOP/s/GPU): 507.3 | learning rate: 2.653493E-07 | global batch size:   128 | lm loss: 6.195934E-02 | loss scale: 1.0 | grad norm: 0.850 | num zeros: 508581056.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:48:01[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:48:01] iteration       46/      50 | consumed samples:         5888 | elapsed time per iteration (ms): 14956.2 | throughput per GPU (TFLOP/s/GPU): 507.5 | learning rate: 1.703709E-07 | global batch size:   128 | lm loss: 5.619948E-02 | loss scale: 1.0 | grad norm: 0.559 | num zeros: 508605696.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:48:16[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:48:16] iteration       47/      50 | consumed samples:         6016 | elapsed time per iteration (ms): 14962.0 | throughput per GPU (TFLOP/s/GPU): 507.3 | learning rate: 9.607360E-08 | global batch size:   128 | lm loss: 5.694133E-02 | loss scale: 1.0 | grad norm: 0.622 | num zeros: 508633408.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:48:31[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:48:31] iteration       48/      50 | consumed samples:         6144 | elapsed time per iteration (ms): 14961.9 | throughput per GPU (TFLOP/s/GPU): 507.3 | learning rate: 4.277569E-08 | global batch size:   128 | lm loss: 5.657647E-02 | loss scale: 1.0 | grad norm: 0.596 | num zeros: 508588224.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:48:46[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:48:46] iteration       49/      50 | consumed samples:         6272 | elapsed time per iteration (ms): 14961.7 | throughput per GPU (TFLOP/s/GPU): 507.3 | learning rate: 1.070538E-08 | global batch size:   128 | lm loss: 5.522132E-02 | loss scale: 1.0 | grad norm: 0.694 | num zeros: 508590912.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:49:01[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:382] :  [2025-08-14 05:49:01] iteration       50/      50 | consumed samples:         6400 | elapsed time per iteration (ms): 14947.2 | throughput per GPU (TFLOP/s/GPU): 507.8 | learning rate: 0.000000E+00 | global batch size:   128 | lm loss: 5.631782E-02 | loss scale: 1.0 | grad norm: 0.721 | num zeros: 508582528.0 | number of skipped iterations:   0 | number of nan iterations:   0 |[0m
[[32m20250814 05:49:01[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : [after training is done] datetime: 2025-08-14 05:49:01 [0m
[[32m20250814 05:49:01[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] : saving checkpoint at iteration      50 to /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/output/amd/root/llama3.1_8B-pretrain/checkpoints in torch format[0m
[[32m20250814 05:52:10[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1mrank: 0, takes 189.55723643302917 to prepare state dict for ckpt [0m
[[32m20250814 05:52:36[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1m[-----------utils.py:364] :   successfully saved checkpoint from iteration      50 to /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/output/amd/root/llama3.1_8B-pretrain/checkpoints [ t 1/1, p 1/1 ][0m
[[32m20250814 05:52:36[0m][[36mrank-0/8[0m][[34m[1mDEBUG[0m] [34m[1mrank: 0, takes 0.010795831680297852 to finalize ckpt save [0m
[[32m20250814 05:52:36[0m][[36mrank-4/8[0m][[34m[1mDEBUG[0m] [34m[1mrank: 4, takes 150.18639636039734 to finalize ckpt save [0m
[[32m20250814 05:52:36[0m][[36mrank-7/8[0m][[34m[1mDEBUG[0m] [34m[1mrank: 7, takes 150.18491983413696 to finalize ckpt save [0m
[[32m20250814 05:52:36[0m][[36mrank-6/8[0m][[34m[1mDEBUG[0m] [34m[1mrank: 6, takes 150.14492630958557 to finalize ckpt save [0m
[[32m20250814 05:52:36[0m][[36mrank-1/8[0m][[34m[1mDEBUG[0m] [34m[1mrank: 1, takes 150.19222283363342 to finalize ckpt save [0m
[[32m20250814 05:52:36[0m][[36mrank-3/8[0m][[34m[1mDEBUG[0m] [34m[1mrank: 3, takes 150.19033980369568 to finalize ckpt save [0m
[[32m20250814 05:52:36[0m][[36mrank-2/8[0m][[34m[1mDEBUG[0m] [34m[1mrank: 2, takes 150.13353514671326 to finalize ckpt save [0m
[[32m20250814 05:52:36[0m][[36mrank-5/8[0m][[34m[1mDEBUG[0m] [34m[1mrank: 5, takes 150.18352675437927 to finalize ckpt save [0m
[rank0]:[W814 05:52:38.262609622 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W814 05:52:39.461808911 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W814 05:52:39.532642863 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W814 05:52:39.603649935 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W814 05:52:39.623762556 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W814 05:52:39.644324520 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W814 05:52:39.734903842 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W814 05:52:39.735141779 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[NODE-0(gpu-40)] [INFO] torchrun exited with code 0
