# AssertionError

**Error Message:**
`use_transformer_engine_full_layer_spec must be True`

---

# LlamaModelProvider Configuration

## Parallelism Configuration
- `tensor_model_parallel_size`: 1
- `pipeline_model_parallel_size`: 1
- `pipeline_model_parallel_comm_backend`: None
- `virtual_pipeline_model_parallel_size`: None
- `sequence_parallel`: False
- `context_parallel_size`: 1
- `hierarchical_context_parallel_sizes`: None
- `max_seqlen_per_dp_cp_rank`: None
- `hybrid_context_parallel`: False
- `expert_model_parallel_size`: 1
- `expert_tensor_parallel_size`: None

## Model Architecture
- `num_layers`: 80
- `hidden_size`: 8192
- `num_attention_heads`: 64
- `num_query_groups`: 8 (GQA)
- `kv_channels`: 128
- `ffn_hidden_size`: 28672
- `seq_length`: 8192
- `vocab_size`: 32000
- `should_pad_vocab`: False
- `make_vocab_size_divisible_by`: 128

## Attention Configuration
- `attention_backend`: AttnBackend.auto (5) *
- `softmax_scale`: None
- `softmax_type`: 'vanilla'
- `attention_dropout`: 0.0
- `attention_softmax_in_fp32`: False
- `flash_decode`: False
- `fp8_dot_product_attention`: False *
- `fp8_multi_head_attention`: False
- `multi_latent_attention`: False

## Position Embeddings
- `position_embedding_type`: 'rope'
- `rotary_base`: 10000.0
- `rotary_percent`: 1.0
- `rotary_interleaved`: False
- `seq_len_interpolation_factor`: None
- `apply_rope_fusion`: True
- `fused_single_qkv_rope`: False
- `no_rope_freq`: None
- `mrope_section`: None

## Normalization & Activation
- `normalization`: 'RMSNorm'
- `layernorm_epsilon`: 1e-05
- `layernorm_zero_centered_gamma`: False
- `qk_layernorm`: False
- `memory_efficient_layer_norm`: False
- `persist_layer_norm`: True
- `activation_func`: <function silu>
- `activation_func_fp8_input_store`: False
- `gated_linear_unit`: True
- `glu_linear_offset`: 0.0
- `activation_func_clamp_value`: None
- `use_te_activation_func`: False *

## Precision & Data Types
- `fp16`: True
- `bf16`: False
- `params_dtype`: torch.float16
- `pipeline_dtype`: None
- `disable_bf16_reduced_precision_matmul`: False

## FP8 Configuration
- `fp8`: None *
- `fp8_recipe`: 'delayed'*
- `fp8_param`: False
- `fp8_quantizer_factory`: None
- `fp8_margin`: 0
- `fp8_interval`: 1
- `fp8_amax_history_len`: 1*
- `fp8_amax_compute_algo`: 'most_recent'
- `fp8_wgrad`: True
- `tp_only_amax_red`: False
- `first_last_layers_bf16`: False
- `num_layers_at_start_in_bf16`: 1
- `num_layers_at_end_in_bf16`: 1

## FP4 Configuration
- `fp4`: None
- `fp4_recipe`: 'nvfp4'
- `fp4_param`: False
- `fp4_quantizer_factory`: None

## Dropout & Regularization
- `hidden_dropout`: 0.0
- `bias_dropout_fusion`: True

## Bias Configuration
- `add_bias_linear`: False
- `add_qkv_bias`: False
- `moe_router_enable_expert_bias`: False

## Residual Connections
- `fp32_residual_connection`: False
- `apply_residual_connection_post_layernorm`: False

## Initialization
- `perform_initialization`: False
- `use_cpu_initialization`: False
- `init_method`: None
- `output_layer_init_method`: None
- `init_method_std`: 0.02
- `embedding_init_method`: None
- `embedding_init_method_std`: None
- `init_model_with_meta_device`: False

## Activation Checkpointing
- `recompute_granularity`: None
- `recompute_method`: None
- `recompute_num_layers`: None
- `distribute_saved_activations`: None
- `recompute_modules`: None
- `num_microbatches_with_partial_activation_checkpoints`: None

## Communication Optimization
- `gradient_accumulation_fusion`: True
- `async_tensor_model_parallel_allreduce`: False
- `tp_comm_overlap`: False
- `tp_comm_bulk_wgrad`: True
- `tp_comm_bulk_dgrad`: True
- `tp_comm_overlap_ag`: True
- `tp_comm_overlap_rs`: True
- `tp_comm_overlap_rs_dgrad`: False
- `tp_comm_split_ag`: True
- `tp_comm_atomic_ag`: False
- `tp_comm_split_rs`: True
- `tp_comm_atomic_rs`: False
- `tp_comm_overlap_disable_qkv`: False
- `tp_comm_overlap_disable_fc1`: False
- `tp_comm_bootstrap_backend`: 'nccl'
- `tp_comm_overlap_cfg`: None

## Pipeline Parallelism
- `variable_seq_lengths`: False
- `overlap_p2p_comm`: False
- `batch_p2p_comm`: True
- `batch_p2p_sync`: True
- `use_ring_exchange_p2p`: False
- `deallocate_pipeline_outputs`: True
- `overlap_p2p_comm_warmup_flush`: False
- `microbatch_group_size_per_vp_stage`: None
- `num_layers_in_first_pipeline_stage`: None
- `num_layers_in_last_pipeline_stage`: None
- `pipeline_model_parallel_layout`: None
- `account_for_embedding_in_pipeline_split`: False
- `account_for_loss_in_pipeline_split`: False

## Mixture of Experts (MoE)
- `num_moe_experts`: None
- `moe_extended_tp`: False
- `moe_shared_expert_intermediate_size`: None
- `moe_shared_expert_gate`: False
- `moe_shared_expert_overlap`: False
- `moe_layer_freq`: 1
- `moe_ffn_hidden_size`: None
- `moe_router_load_balancing_type`: 'aux_loss'
- `moe_router_topk`: 2
- `moe_router_topk_limited_devices`: None
- `moe_router_padding_for_quantization`: False
- `moe_router_padding_for_fp8`: False
- `moe_router_num_groups`: None
- `moe_router_group_topk`: None
- `moe_router_pre_softmax`: False
- `moe_router_topk_scaling_factor`: None
- `moe_router_score_function`: 'softmax'
- `moe_router_dtype`: None
- `moe_router_bias_update_rate`: 0.001
- `moe_router_force_load_balancing`: False
- `moe_grouped_gemm`: False
- `moe_use_legacy_grouped_gemm`: False
- `moe_aux_loss_coeff`: 0.0
- `moe_z_loss_coeff`: None
- `moe_input_jitter_eps`: None
- `moe_token_dropping`: False
- `moe_token_dispatcher_type`: 'allgather'
- `moe_enable_deepep`: False
- `moe_flex_dispatcher_backend`: 'deepep'
- `moe_per_layer_logging`: False
- `moe_expert_capacity_factor`: None
- `moe_pad_expert_input_to_capacity`: False
- `moe_token_drop_policy`: 'probs'
- `moe_layer_recompute`: False
- `moe_permute_fusion`: False
- `moe_router_fusion`: False
- `moe_apply_probs_on_input`: False
- `overlap_moe_expert_parallel_comm`: False
- `moe_deepep_num_sms`: 20
- `moe_hybridep_num_sms`: 16

## Loss & Training
- `cross_entropy_loss_fusion`: True
- `cross_entropy_fusion_impl`: 'native'
- `calculate_per_token_loss`: False
- `fp16_lm_cross_entropy`: False
- `parallel_output`: True
- `share_embeddings_and_output_weights`: False

## Gradient & Weight Computation
- `defer_embedding_wgrad_compute`: False
- `wgrad_deferral_limit`: 0
- `delay_wgrad_compute`: False

## Fusions
- `bias_activation_fusion`: True
- `masked_softmax_fusion`: True
- `use_fused_weighted_squared_relu`: False

## CPU Offloading
- `cpu_offloading`: False *
- `cpu_offloading_num_layers`: 0
- `_cpu_offloading_context`: None
- `cpu_offloading_activations`: True
- `cpu_offloading_weights`: False
- `cpu_offloading_double_buffering`: False
- `fine_grained_activation_offloading`: False
- `offload_modules`: None
- `min_offloaded_tensor_size`: 1048576

## CUDA Graph
- `enable_cuda_graph`: False
- `cuda_graph_use_single_mempool`: False
- `cuda_graph_retain_backward_graph`: False
- `cuda_graph_warmup_steps`: 3
- `external_cuda_graph`: False
- `cuda_graph_impl`: 'none'
- `cuda_graph_scope`: None

## Transformer Engine
- `use_transformer_engine_full_layer_spec`: **False** ⚠️ (causing error) *
- `transformer_impl`: 'transformer_engine' *
- `use_transformer_engine_op_fuser`: None *
- `transformer_layer_spec`: <function default_layer_spec>
- `use_te_rng_tracker`: False *

## Embeddings
- `clone_scatter_output_in_embedding`: True
- `scatter_embedding_sequence_parallel`: True

## Attention Variants
- `window_size`: None
- `window_attn_skip_freq`: None
- `qk_clip`: False
- `qk_clip_alpha`: 0.5
- `qk_clip_threshold`: 100
- `log_max_attention_logit`: False
- `attention_output_gate`: False
- `experimental_attention_variant`: None
- `linear_attention_type`: None
- `linear_attention_freq`: None
- `linear_conv_kernel_dim`: None
- `linear_key_head_dim`: None
- `linear_value_head_dim`: None
- `linear_num_key_heads`: None
- `linear_num_value_heads`: None

## DSA (Dynamic Sparse Attention)
- `dsa_indexer_n_heads`: None
- `dsa_indexer_head_dim`: None
- `dsa_indexer_topk`: None
- `dsa_indexer_loss_coeff`: None
- `dsa_indexer_use_sparse_loss`: None

## Mamba Configuration
- `mamba_state_dim`: 128
- `mamba_head_dim`: 64
- `mamba_num_groups`: 8
- `mamba_num_heads`: None
- `use_mamba_mem_eff_path`: True

## Context Parallel
- `cp_comm_type`: None

## Hybrid & Heterogeneous Models
- `is_hybrid_model`: False
- `heterogeneous_block_specs`: False
- `hetereogenous_dist_checkpoint`: False

## MTP (Multi-Task Pretraining)
- `mtp_standalone`: False
- `mtp_num_layers`: None
- `mtp_loss_scaling_factor`: None
- `mtp_enabled`: False

## Expert Parallel
- `ep_overlap_early_attn_memory_release`: False

## Inference
- `inference_rng_tracker`: False
- `inference_sampling_seed`: 42
- `use_inference_optimized_layers`: False
- `mlp_chunks_for_prefill`: 1

## Miscellaneous
- `timers`: None
- `finalize_model_grads_func`: None
- `grad_scale_func`: None
- `no_sync_func`: None
- `grad_sync_func`: None
- `param_sync_func`: None
- `deterministic_mode`: False
- `enable_autocast`: False
- `autocast_dtype`: None
- `barrier_with_L1_time`: True
- `test_mode`: False
- `disable_parameter_transpose_cache`: False
- `config_logger_dir`: ''
- `symmetric_ar_type`: None
- `use_kitchen`: False
- `quant_recipe`: None
- `fallback_to_eager_attn`: False
- `use_arbitrary_attention_mask`: None
- `restore_modelopt_state`: False
- `_pg_collection`: None

## HuggingFace Integration
- `hf_model_id`: 'meta-llama/Llama-2-70b-hf'
- `generation_config`: GenerationConfig {
  - `bos_token_id`: 1
  - `do_sample`: true
  - `eos_token_id`: 2
  - `max_length`: 4096
  - `pad_token_id`: 0
  - `temperature`: 0.6
  - `top_p`: 0.9
  - `trust_remote_code`: false
}

---

## ⚠️ Root Cause
The assertion fails because:
- **Required:** `use_transformer_engine_full_layer_spec = True`
- **Actual:** `use_transformer_engine_full_layer_spec = False`

This needs to be set to `True` to proceed with the current configuration.