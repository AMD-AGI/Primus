work_group: ${TEAM:amd}
user_name: ${USER:root}
exp_name: ${EXP_NAME:gpt_oss_20b}
workspace: ./output

modules:
  pre_trainer:
    framework: megatron
    config: pre_trainer.yaml

    # model to run
    model: ${PRIMUS_MODEL:gpt_oss_20B}.yaml
    overrides:
      # log
      wandb_project: "Primus_DeepSeek_Pretrain"
      stderr_sink_level: DEBUG

      # debug
      moe_router_force_load_balancing: true
      log_avg_skip_iterations: 2
      log_avg_reset_interval: 50

      # profile
      profile: false
      use_pytorch_profiler: false
      profile_step_end: 7
      profile_step_start: 6

      # precision (mixed precision training)
      bf16: true
      fp16: false
      fp8: null  # Set to "e4m3" or "hybrid" for FP8 training

      # hyper parameters
      train_iters: ${PRIMUS_TRAIN_ITERS:200}
      micro_batch_size: ${PRIMUS_MICRO_BATCH_SIZE:4}
      global_batch_size: ${PRIMUS_GLOBAL_BATCH_SIZE:640}
      seq_length: ${PRIMUS_SEQ_LENGTH:8192}
      max_position_embeddings: ${PRIMUS_MAX_POSITION_EMBEDDINGS:8192}
      lr: ${PRIMUS_LR:1.0e-5}
      min_lr: ${PRIMUS_MIN_LR:0.0}
      lr_warmup_iters: 2
      lr_decay_iters: ${PRIMUS_LR_DECAY_ITERS:null}
      lr_decay_style: ${PRIMUS_LR_DECAY_STYLE:cosine}
      weight_decay: ${PRIMUS_WEIGHT_DECAY:0.1}
      adam_beta1: ${PRIMUS_ADAM_BETA1:0.9}
      adam_beta2: ${PRIMUS_ADAM_BETA2:0.95}
      eod_mask_loss: ${PRIMUS_EOD_MASK_LOSS:true}
      init_method_std: ${PRIMUS_INIT_METHOD_STD:0.008}
      norm_epsilon: ${PRIMUS_NORM_EPSILON:1.0e-6}

      # parallel
      tensor_model_parallel_size: ${PRIMUS_TP:1}
      pipeline_model_parallel_size: ${PRIMUS_PP:1}
      expert_model_parallel_size: ${PRIMUS_EP:8}
      overlap_grad_reduce: ${PRIMUS_OVERLAP_GRAD_REDUCE:true}
      overlap_param_gather: ${PRIMUS_OVERLAP_PARAM_GATHER:true}

      # data
      mock_data: ${PRIMUS_MOCK_DATA:false}
      train_data_path: "10 /data/c4-train.en_6_text_document"
      valid_data_path: "/data/c4-validation-91205-samples.en_text_document"
      test_data_path: "/data/c4-validation-91205-samples.en_text_document"

      # fusion
      # 20250321: need latest megatron docker image
      moe_permute_fusion: ${PRIMUS_MOE_PERMUTE_FUSION:false}
      # fused wgrad gemm and accumulation
      gradient_accumulation_fusion: ${PRIMUS_GRADIENT_ACCUMULATION_FUSION:false}
      # recommend set `false` in fp8
      moe_use_legacy_grouped_gemm: ${PRIMUS_MOE_USE_LEGACY_GROUPED_GEMM:true}
      # fused topk router with aux score
      moe_use_fused_router_with_aux_score: ${PRIMUS_MOE_USE_FUSED_ROUTER_WITH_AUX_SCORE:false}
      # MLA
      multi_latent_attention: ${PRIMUS_MULTI_LATENT_ATTENTION:false}
      # rope fusion
      apply_rope_fusion: ${PRIMUS_APPLY_ROPE_FUSION:false}

      # DeepEP does not support moe_shared_expert_overlap
      moe_shared_expert_overlap: ${PRIMUS_MOE_SHARED_EXPERT_OVERLAP:false}
      # DeepEP only supports float32 probs
      moe_router_dtype: ${PRIMUS_MOE_ROUTER_DTYPE:fp32}

      # ckpt
      finetune: ${PRIMUS_FINETUNE:false}
      auto_continue_train: ${PRIMUS_AUTO_CONTINUE_TRAIN:false}
      load: ${PRIMUS_LOAD:null}
      no_load_optim: ${PRIMUS_NO_LOAD_OPTIM:null}
      no_load_rng: ${PRIMUS_NO_LOAD_RNG:null}
      save: ${PRIMUS_SAVE:null}
      save_interval: ${PRIMUS_SAVE_INTERVAL:20000}
      no_save_optim: ${PRIMUS_NO_SAVE_OPTIM:null}
      no_save_rng: ${PRIMUS_NO_SAVE_RNG:null}
      disable_last_saving: true
      exit_on_missing_checkpoint: ${PRIMUS_EXIT_ON_MISSING_CHECKPOINT:false}
      ckpt_format: torch
      eval_iters: ${PRIMUS_EVAL_ITERS:32}
      eval_interval: ${PRIMUS_EVAL_INTERVAL:10}
      num_workers: ${PRIMUS_NUM_WORKERS:1}

      # Turbo (Primus)  
      enable_primus_turbo: ${PRIMUS_ENABLE_PRIMUS_TURBO:true}
      use_turbo_attention: ${PRIMUS_USE_TURBO_ATTENTION:true}
      use_turbo_grouped_mlp: ${PRIMUS_USE_TURBO_GROUPED_MLP:true}

      # deepep
      use_turbo_deepep: ${PRIMUS_USE_TURBO_DEEPPEP:true}

      # 64 or 80 for ep8, 32 for ep16-64 is best practice
      turbo_deepep_num_cu: ${PRIMUS_TURBO_DEEPPEP_NUM_CU:64}
      turbo_deepep_use_comm_stream: ${PRIMUS_TURBO_DEEPPEP_USE_COMM_STREAM:false}

      # sync-free moe support stage 0-3, 0 means not use sync-free moe
      # stage 3 is completely no gpu-cpu sync in MoE, but cost more memory
      # stage 2 is recommended for better performance
      turbo_sync_free_moe_stage: ${PRIMUS_TURBO_SYNC_FREE_MOE_STAGE:2}

      # Cross entropy flags
      # cross_entropy_fusion_impl: ${PRIMUS_CROSS_ENTROPY_FUSION_IMPL:"te"}
      # cross_entropy_loss_fusion: ${PRIMUS_CROSS_ENTROPY_LOSS_FUSION:true}


