work_group: ${PRIMUS_TEAM:amd}
user_name: ${PRIMUS_USER:root}
exp_name: ${PRIMUS_EXP_NAME:llama2_7B-sft}
workspace: ${PRIMUS_WORKSPACE:./output}

modules:
  post_trainer:
    framework: megatron
    config: post_trainer.yaml

    # model to run
    model: llama2_7B.yaml
    overrides:
      # SFT-specific marker
      is_instruction_dataset: true
      
      # log
      wandb_project: "Primus_LLaMA2_SFT"
      stderr_sink_level: DEBUG

      eval_iters: 10
      eval_interval: 50

      log_avg_skip_iterations: 2
      log_avg_reset_interval: 50

      # SFT typically uses fewer iterations and smaller batches
      train_iters: 200
      micro_batch_size: 1
      global_batch_size: 128

      seq_length: 2048
      max_position_embeddings: 2048

      # SFT uses smaller learning rates
      finetune_lr: 5.0e-6
      min_lr: 0.0
      lr_warmup_iters: 50
      lr_decay_iters: null
      lr_decay_style: cosine
      weight_decay: 0.1
      adam_beta1: 0.9
      adam_beta2: 0.95
      eod_mask_loss: true
      init_method_std: 0.008
      norm_epsilon: 1.0e-6

      # parallel - SFT typically uses less parallelism for smaller datasets
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
      overlap_grad_reduce: true
      overlap_param_gather: true
      gradient_accumulation_fusion: false

      # data - users should replace with actual SFT dataset
      mock_data: true
      train_data_path: null  # Path to SFT training data
      valid_data_path: null  # Path to SFT validation data
      test_data_path: null

      # ckpt - typically load pretrained model for SFT
      finetune: true
      auto_continue_train: false
      load: null  # Path to pretrained checkpoint
      no_load_optim: true  # Don't load optimizer state for SFT
      no_load_rng: true  # Don't load RNG state for SFT
      save: null  # Path to save SFT checkpoints
      save_interval: 50
      no_save_optim: null
      no_save_rng: null
      disable_last_saving: true
      ckpt_format: torch

      # Turbo
      enable_primus_turbo: true
      use_turbo_attention: true
      use_turbo_grouped_mlp: true
