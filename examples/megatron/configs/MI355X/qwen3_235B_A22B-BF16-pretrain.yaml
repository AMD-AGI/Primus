work_group: ${PRIMUS_TEAM:amd}
user_name: ${PRIMUS_USER:root}
exp_name: ${PRIMUS_EXP_NAME:qwen3_235B_A22B-pretrain}
workspace: ${PRIMUS_WORKSPACE:./output}

modules:
  pre_trainer:
    framework: megatron
    config: pre_trainer.yaml

    # model to run
    model: qwen3_235B_A22B.yaml
    overrides:
      # log
      wandb_project: "Primus_Qwen3_235B_A22B_Pretrain"
      # disable_wandb: false
      # disable_tensorboard: false
      stderr_sink_level: DEBUG

      log_avg_skip_iterations: 2
      log_avg_reset_interval: 50
      moe_router_force_load_balancing: true

      train_iters: 50
      micro_batch_size: 1
      global_batch_size: 256

      seq_length: ${PRIMUS_SEQ_LENGTH:4096}
      max_position_embeddings: ${PRIMUS_MAX_POSITION_EMBEDDINGS:131072}

      lr: 1.0e-4
      min_lr: 1.0e-5
      lr_warmup_iters: 2
      lr_decay_iters: 320000
      lr_decay_style: cosine
      weight_decay: 1.0e-1
      adam_beta1: 0.9
      adam_beta2: 0.95
      eod_mask_loss: true
      init_method_std: 0.008
      norm_epsilon: 1.0e-6

      # parallel
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 4
      expert_model_parallel_size: 8
      sequence_parallel: 1
      overlap_grad_reduce: true

      overlap_param_gather: true
      use_torch_fsdp2: false
      use_distributed_optimizer: true
      gradient_accumulation_fusion: true
      ckpt_format: torch

      # data
      mock_data: true
      train_data_path: null
      valid_data_path: null
      test_data_path: null

      # ckpt
      finetune: false
      auto_continue_train: false
      load: null
      no_load_optim: null
      no_load_rng: null
      save: null
      save_interval: 20000
      no_save_optim: null
      no_save_rng: null
      disable_last_saving: true

      # PP4 VPP2
      # 2 virtual stages per pipeline rank, so 8 total pipeline segments (PP4*VPP2)
      # To balance 94 layers: 94/8 = approx 11-12 layers per stage
      # This gives 11+12*6+11=94
      # 1+6+1 = 8 chunks
      pipeline_model_parallel_layout: 'Et*11|(t*12|)*6,t*11,L'

      # Pipeline warmup
      pp_warmup: true

      # Load balance
      moe_router_force_load_balancing: true

      # Grouped GEMM
      moe_use_legacy_grouped_gemm: true

      # Turbo
      enable_primus_turbo: true
      use_turbo_attention: false
      use_turbo_grouped_mlp: false

      # Cross entropy flags
      cross_entropy_fusion_impl: "te"
      cross_entropy_loss_fusion: true

      # DeepEp
      use_turbo_deepep: true
      turbo_deepep_num_cu: 32
      turbo_deepep_use_comm_stream: false
      moe_shared_expert_overlap: false
      moe_router_dtype: fp32
      turbo_sync_free_moe_stage: 1

      # Manual GC
      manual_gc: true
      manual_gc_interval: 1