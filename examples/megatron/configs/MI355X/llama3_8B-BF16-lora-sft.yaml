# Llama3 8B LoRA SFT Configuration for MI355X
# Parameter-efficient fine-tuning using LoRA

extends:
  - llama3_8B-BF16-sft.yaml

# Enable LoRA for parameter-efficient fine-tuning
lora_enabled: true
lora_rank: 32          # Low-rank dimension (smaller = fewer params, less expressiveness)
lora_alpha: 32         # Scaling factor (typically equal to rank)
lora_dropout: 0.05     # Dropout for regularization
lora_target_modules:   # Which modules to apply LoRA to
  - linear_qkv         # Query, Key, Value projections
  - linear_proj        # Output projection
  - linear_fc1         # MLP first layer
  - linear_fc2         # MLP second layer

# LoRA typically uses higher learning rate
lr: 2.0e-4
min_lr: 2.0e-5

# Can use larger batch size with LoRA (fewer trainable params)
micro_batch_size: 2
global_batch_size: 256

# Training iterations (LoRA often needs fewer iterations)
train_iters: 500
lr_warmup_iters: 20
