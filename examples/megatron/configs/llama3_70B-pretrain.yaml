work_group: ${TEAM:amd}
user_name: ${USER:root}
exp_name: exp-llama3_70B-pretrain
workspace: ./output

platform:
  config: platform_azure.yaml
  overrides:
    master_sink_level: INFO

modules:
  pre_trainer:
    framework: megatron
    config: pre_trainer.yaml
    model: ${PRIMUS_MODEL:llama3_70B}.yaml
    overrides:
      # log
      wandb_project: "Primus_DeepSeek_Pretrain"
      stderr_sink_level: DEBUG

      moe_router_force_load_balancing: true
      moe_router_dtype: fp32
      log_avg_skip_iterations: 2
      log_avg_reset_interval: 5

      train_iters: ${PRIMUS_TRAIN_ITERS:20}
      micro_batch_size: ${PRIMUS_MBS:4}
      global_batch_size: ${PRIMUS_GBS:128}

      seq_length: ${PRIMUS_SEQ_LENGTH:8192}
      max_position_embeddings: ${PRIMUS_MAX_POSITION_EMBEDDINGS:8192}

      lr: 1.0e-5
      min_lr: 0.0
      lr_warmup_iters: 2
      lr_decay_iters: null
      lr_decay_style: cosine
      weight_decay: 0.1
      adam_beta1: 0.9
      adam_beta2: 0.95
      eod_mask_loss: true
      init_method_std: 0.008
      norm_epsilon: 1.0e-6

      # parallel
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
      overlap_grad_reduce: true

      # data
      mock_data: true
      train_data_path: null
      valid_data_path: null
      test_data_path: null

      # ckpt
      finetune: false
      auto_continue_train: true
      load: null
      no_load_optim: null
      no_load_rng: null
      save: null
      save_interval: 20000
      no_save_optim: null
      no_save_rng: null
      disable_last_saving: true

      use_torch_fsdp2: true
      use_distributed_optimizer: false
      overlap_param_gather: false
      ckpt_format: torch_dist
      sequence_parallel: 1
      gradient_accumulation_fusion: false
      # deprecated_use_mcore_models: true

      # use_custom_fsdp: true
      # use_distributed_optimizer: true
      # overlap_param_gather: true 
      # gradient_accumulation_fusion: true 
      
      # recompute
      recompute_granularity: full # full, selective
      recompute_method: block # uniform, block
      recompute_num_layers: 80 # int

      transformer_impl: transformer_engine
      fp8: hybrid
      fp8_margin: 0
      fp8_interval: 1
      fp8_amax_history_len: 1024
      fp8_amax_compute_algo: max
      attention_softmax_in_fp32: true

      no_fp8_weight_transpose_cache: true

      # use_precision_aware_optimizer: true
      # fp8_param_gather: true 

