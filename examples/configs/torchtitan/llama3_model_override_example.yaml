# Example: Using model_overrides patch
#
# This configuration demonstrates how to dynamically override TorchTitan model parameters
# using the model_override patch.

name: llama3_model_override_example
platform: mi300x

modules:
  - name: pre_trainer
    framework: torchtitan

    model:
      name: llama3
      flavor: debugmodel  # Or: 1B, 8B, 70B, etc.

    params:
      # Standard TorchTitan parameters
      training:
        steps: 1000
        data_parallel_shard_degree: 8
        tensor_parallel_degree: 1

      optimizer:
        name: AdamW
        lr: 3.0e-4

      # Model overrides (dynamically applied via patch)
      # All keys MUST start with "model." prefix
      model_overrides:
        model.n_layers: 8        # Override number of layers
        model.dim: 2048          # Override hidden dimension
        model.n_heads: 16        # Override number of attention heads
        model.vocab_size: 32000  # Override vocabulary size

      # Alternative nested format (automatically flattened):
      # model_overrides:
      #   model:
      #     n_layers: 8
      #     dim: 2048
      #     n_heads: 16
      #     vocab_size: 32000
