work_group: ${PRIMUS_TEAM:amd}
user_name: ${PRIMUS_USER:root}
exp_name: ${PRIMUS_EXP_NAME:qwen3_30B_A3B_posttrain}
workspace: ${PRIMUS_WORKSPACE:./output}

modules:
  post_trainer:
    framework: megatron_bridge
    config: post_trainer.yaml

    # Model to run
    model: ${PRIMUS_MODEL:qwen3_30B_A3B}.yaml

    overrides:
      # ============================================================================
      # Logging Configuration
      # ============================================================================
      wandb_project: "Primus_Qwen3_30B_A3B_Posttrain"
      stderr_sink_level: INFO
      log_interval: 10
      log_throughput: true
      tensorboard_dir: ${PRIMUS_TENSORBOARD_DIR:/path/to/tensorboard/qwen3_30B_A3B_sft}

      # ============================================================================
      # Model Loading Configuration
      # ============================================================================
      # Load from pretrained Megatron checkpoint
      load: ${PRIMUS_LOAD_CKPT:/path/to/pretrained/qwen3_30B_A3B/checkpoint}

      # Or load from HuggingFace and convert
      # convert_from_hf: true
      # hf_model_name_or_path: "Qwen/Qwen3-30B-A3B"

      # ============================================================================
      # LoRA Configuration (Optional - for parameter-efficient fine-tuning)
      # ============================================================================
      # Enable LoRA for memory efficiency
      use_lora: false          # Set to true for LoRA fine-tuning
      lora_rank: 16
      lora_alpha: 32
      lora_dropout: 0.05

      # Target modules (if using LoRA)
      # lora_target_modules:
      #   - q_proj
      #   - k_proj
      #   - v_proj
      #   - o_proj

      # ============================================================================
      # Training Hyperparameters (Full Fine-tuning)
      # ============================================================================
      # Batch sizes
      micro_batch_size: 2
      global_batch_size: 128

      # Training iterations
      train_iters: 5000

      # Sequence length
      seq_length: ${PRIMUS_SEQ_LENGTH:4096}
      max_position_embeddings: ${PRIMUS_MAX_POSITION_EMBEDDINGS:4096}

      # Learning rate (lower for fine-tuning)
      lr: 2.0e-6              # Very low LR for full fine-tuning of large MoE
      min_lr: 2.0e-7
      lr_decay_style: cosine
      lr_warmup_iters: 100
      lr_decay_iters: null

      # Optimization
      weight_decay: 0.01      # Small weight decay for fine-tuning
      clip_grad: 1.0
      adam_beta1: 0.9
      adam_beta2: 0.95
      adam_eps: 1.0e-8

      # Initialization
      init_method_std: 0.008
      norm_epsilon: 1.0e-6

      # MoE specific
      moe_router_force_load_balancing: true
      log_avg_skip_iterations: 2
      log_avg_reset_interval: 50

      # EOD mask loss
      eod_mask_loss: true

      # Seed
      seed: 1234

      # ============================================================================
      # Parallelism Strategy (For Full Fine-tuning)
      # ============================================================================
      tensor_model_parallel_size: ${PRIMUS_TP:1}
      pipeline_model_parallel_size: ${PRIMUS_PP:1}

      # Expert parallelism (critical for 128 experts)
      expert_model_parallel_size: ${PRIMUS_EP:8}  # 128 experts / 8 = 16 experts per GPU
      expert_tensor_parallel_size: 1

      # Context and sequence parallelism
      context_parallel_size: 1
      sequence_parallel: false

      # Communication overlap
      overlap_grad_reduce: true
      overlap_param_gather: true

      # ============================================================================
      # Data Configuration
      # ============================================================================
      # Mock data for testing
      mock_data: false

      # Real data paths
      train_data_path: ${PRIMUS_TRAIN_DATA_PATH:/path/to/instruction/data}
      valid_data_path: ${PRIMUS_VALID_DATA_PATH:null}
      test_data_path: ${PRIMUS_TEST_DATA_PATH:null}

      # Dataset format and prompt template
      dataset_format: "alpaca"    # or "sharegpt", "oasst", "dolly"
      prompt_template: "qwen"
      chat_format: "chatml"

      # Data split
      split: "95,5,0"

      # ============================================================================
      # Mixed Precision
      # ============================================================================
      bf16: true
      fp16: false

      # Optional: FP8 for even better performance (if supported)
      # fp8: hybrid

      # ============================================================================
      # Checkpointing Configuration
      # ============================================================================
      # Checkpoint paths
      save: ${PRIMUS_SAVE_CKPT:/path/to/finetuned/qwen3_30B_A3B/checkpoints}
      save_interval: 500

      # Checkpoint format
      ckpt_format: torch

      # Checkpoint options
      finetune: true
      auto_continue_train: false
      no_save_optim: false
      no_save_rng: false
      no_load_optim: null
      no_load_rng: null
      disable_last_saving: false

      # Save only LoRA adapters if using LoRA
      # save_lora_only: true

      # Evaluation
      eval_iters: 10
      eval_interval: 100

      # ============================================================================
      # Optimization Features
      # ============================================================================
      # Gradient accumulation fusion
      gradient_accumulation_fusion: true

      # MoE optimization
      moe_use_legacy_grouped_gemm: false

      # Activation checkpointing (recommended for memory efficiency)
      recompute_activations: false
      # Enable if OOM:
      # recompute_activations: true
      # recompute_granularity: "selective"
      # recompute_method: "uniform"

      # ============================================================================
      # Primus Turbo Extensions (if available)
      # ============================================================================
      enable_primus_turbo: false
      use_turbo_attention: false
      use_turbo_grouped_mlp: false

      # ============================================================================
      # Cross Entropy Optimization (Optional)
      # ============================================================================
      # cross_entropy_fusion_impl: "te"
      # cross_entropy_loss_fusion: true

      # ============================================================================
      # HuggingFace Export Configuration
      # ============================================================================
      # Convert to HuggingFace format after training
      convert_to_hf: true
      hf_save_path: ${PRIMUS_HF_SAVE_PATH:/path/to/output/qwen3_30B_A3B_finetuned_hf}

# ==============================================================================
# Performance Notes
# ==============================================================================
# Full fine-tuning configuration:
# - Requires 8 GPUs with expert_model_parallel_size=8
# - GPU memory: ~70GB per GPU (A100 80GB recommended)
# - Checkpoint size: ~170GB (full model)
# - Training time: 1-2 days for 5000 iterations
#
# GPU Recommendations:
# - 8x A100 80GB: Comfortable
# - 8x A100 40GB: Tight, may need activation checkpointing
# - 4x A100 80GB: Possible with TP=2 and EP=4
#
# For lower resources, consider using LoRA:
# - Set use_lora: true
# - Reduce expert_model_parallel_size to 4
# - See qwen3_30B_A3B_lora_posttrain.yaml
#
# Environment Variables:
# - PRIMUS_TEAM: Team name (default: amd)
# - PRIMUS_USER: User name (default: root)
# - PRIMUS_EXP_NAME: Experiment name
# - PRIMUS_WORKSPACE: Workspace directory (default: ./output)
# - PRIMUS_MODEL: Model config name (default: qwen3_30B_A3B)
# - PRIMUS_SEQ_LENGTH: Sequence length (default: 4096)
# - PRIMUS_TP: Tensor parallelism (default: 1)
# - PRIMUS_PP: Pipeline parallelism (default: 1)
# - PRIMUS_EP: Expert parallelism (default: 8)
# - PRIMUS_TRAIN_DATA_PATH: Training data path
# - PRIMUS_LOAD_CKPT: Pretrained checkpoint path
# - PRIMUS_SAVE_CKPT: Checkpoint save path
# - PRIMUS_HF_SAVE_PATH: HuggingFace export path
#
# Notes:
# 1. This configuration assumes full fine-tuning with all parameters trainable
# 2. Qwen3-30B-A3B has 128 experts, 8 active per token (~30B activated params)
# 3. Total parameters: ~87B, Activated parameters per token: ~30B
# 4. Expert parallelism (EP) is critical - must divide 128 evenly
