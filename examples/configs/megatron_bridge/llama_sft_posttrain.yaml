# Megatron-Bridge Post-training (SFT) Configuration
# This is an example configuration for supervised fine-tuning (SFT)
# using the Megatron-Bridge backend

# Framework Configuration
framework: megatron_bridge

# Task type - specify posttrain trainer
task: posttrain

# Model Configuration
# Load a pretrained checkpoint
model:
  # If loading from pretrained Megatron checkpoint
  load: /path/to/pretrained/checkpoint

  # Or load from HuggingFace and convert
  # convert_from_hf: true
  # hf_model_name_or_path: meta-llama/Llama-3-8B

# Training Hyperparameters (Post-training specific)
training:
  # Smaller batch sizes for fine-tuning
  micro_batch_size: 1
  global_batch_size: 128

  # Fewer training steps for fine-tuning
  train_iters: 5000

  # Lower learning rate for fine-tuning
  lr: 5.0e-6
  min_lr: 5.0e-7
  lr_decay_style: cosine
  lr_warmup_iters: 100

  # Optimization
  weight_decay: 0.01
  clip_grad: 1.0
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_eps: 1.0e-8

  # Seed
  seed: 1234

# LoRA Configuration (Optional - for parameter-efficient fine-tuning)
lora:
  use_lora: true
  lora_rank: 8
  lora_alpha: 16
  lora_dropout: 0.05
  # Target modules (model-dependent)
  lora_target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj

# Mixed Precision
precision:
  bf16: true
  fp16: false

# Parallelism Strategy (usually smaller for fine-tuning)
parallelism:
  tensor_model_parallel_size: 2
  pipeline_model_parallel_size: 1
  context_parallel_size: 1

# Data Configuration (Instruction/SFT dataset)
data:
  # Path to instruction/SFT dataset
  data_path: /path/to/sft/data

  # Dataset format (e.g., alpaca, dolly, oasst, sharegpt)
  dataset_format: "alpaca"

  # Train/val split
  split: "95,5,0"

  # Tokenizer
  tokenizer_type: "GPT2BPETokenizer"
  vocab_file: /path/to/vocab.json
  merge_file: /path/to/merges.txt

  # Sequence length (often shorter for instructions)
  seq_length: 2048

  # Prompt template configuration
  prompt_template: "alpaca"  # or "chatml", "llama2", etc.

  # Chat format (if applicable)
  chat_format: "chatml"

# Checkpointing
checkpointing:
  save: /path/to/finetuned/checkpoints
  # Load pretrained checkpoint
  load: /path/to/pretrained/checkpoint
  save_interval: 500
  no_save_optim: false
  no_save_rng: false

  # Optional: Save only LoRA adapters
  save_lora_only: true

# Logging
logging:
  log_interval: 10
  tensorboard_dir: /path/to/tensorboard/sft
  log_throughput: true

  # Evaluation during training
  eval_interval: 100
  eval_iters: 10

# Distributed Backend
distributed:
  distributed_backend: "nccl"

# Optional: Convert to HuggingFace format after training
convert_to_hf: true
hf_save_path: /path/to/output/hf_model
