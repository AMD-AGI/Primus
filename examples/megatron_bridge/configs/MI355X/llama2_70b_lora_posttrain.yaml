work_group: ${PRIMUS_TEAM:amd}
user_name: ${PRIMUS_USER:root}
exp_name: ${PRIMUS_EXP_NAME:llama2_70b_lora_posttrain}
workspace: ${PRIMUS_WORKSPACE:./output}

modules:
  post_trainer:
    framework: megatron_bridge
    config: sft_trainer.yaml

    # Model to run
    model: llama2_70b_lora.yaml

    overrides:
      stderr_sink_level: DEBUG
      # Parallelism configuration
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      pipeline_dtype: null
      virtual_pipeline_model_parallel_size: null
      context_parallel_size: 1
      sequence_parallel: false
      use_megatron_fsdp: false

      # Finetuning-specific params
      # Use pretrained_checkpoint to load base model weights without LoRA adapters
      pretrained_checkpoint: /data/megatron_checkpoints/Llama-2-70b-hf/iter_0000000
      peft: lora
      dataset_type: mlperf_dataset # can be squad or mlperf_dataset
      # for mlperf_dataset, you need to set the following parameters
      packed_sequence: true
      packed_train_data_path: /data/train.npy
      packed_val_data_path: /data/validation.npy
      packed_metadata_path: /data/packed_metadata.jsonl

      # Training configuration
      train_iters: 1000
      global_batch_size: 8
      micro_batch_size: 1
      seq_length: 8192
      eval_interval: 48
      eval_iters: 48
      save_interval: null

      # Optimizer configuration
      adam_beta1: 0.9
      adam_beta2: 0.999
      adam_eps: 1.0e-8
      weight_decay: 0.0001
      min_lr: 0.0
      lr: 4.0e-4
      lr_warmup_iters: 0
      lr_decay_iters: null

      # W&B logging
      wandb_project: null
      wandb_entity: null
      wandb_exp_name: null

      # Precision
      precision_config: bf16_with_fp8_hybrid
      comm_overlap_config: null

      no_fp8_weight_transpose_cache: true
