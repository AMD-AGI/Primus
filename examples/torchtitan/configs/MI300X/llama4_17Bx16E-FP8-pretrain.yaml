work_group: ${PRIMUS_TEAM:amd}
user_name: ${PRIMUS_USER:root}
exp_name: ${PRIMUS_EXP_NAME:llama4_17Bx16E-pretrain}
workspace: ./output

modules:
  pre_trainer:
    framework: torchtitan
    config: pre_trainer.yaml

    # model to run
    model: llama4_17Bx16E-fp8.yaml
    overrides:
      sink_level: null
      file_sink_level: DEBUG
      stderr_sink_level: INFO

      metrics:
        log_freq: 1
        enable_wandb: false

      lr_scheduler:
        # lr scheduler warm up
        warmup_steps: 10

      training:
        local_batch_size: 8
        seq_len: 8192
        mock_data: false
        steps: 50

      activation_checkpoint:
        mode: "full"  # ["none", "selective", "full"]
        selective_ac_option: "op"  # "int" = ac every positive int layer or 'op', ac based on ops policy

      quantize:
        linear:
          float8:
            enable_fsdp_float8_all_gather: true
            precompute_float8_dynamic_scale_for_fsdp: true
            filter_fqns: ["output"]

      primus_turbo:
        enable_primus_turbo: true
        use_turbo_float8_linear: true
        enable_attention_float8: false
        use_turbo_grouped_mm: true