work_group: ${TEAM:amd}
user_name: ${USER:root}
exp_name: ${EXP_NAME:llama3_70B-pretrain}
workspace: ./output

modules:
  pre_trainer:
    framework: torchtitan
    config: pre_trainer.yaml

    # model to run
    model: llama3.1_70B.yaml
    overrides:
      sink_level: null
      file_sink_level: DEBUG
      stderr_sink_level: INFO

      training:
        batch_size: 8

      optimizer:
        lr: 1.5e-4

      parallelism:
        tensor_parallel_degree: 8

      activation_checkpoint:
        mode: full
