work_group: ${TEAM:amd}
user_name: ${USER:root}
exp_name: &exp_name ${EXP:exp-dsv3-pretrain}
workspace: ./output

platform:
  config: platform_azure.yaml
  overrides:
    master_sink_level: INFO

modules:
  pre_trainer:
    framework: megatron
    config: pre_trainer.yaml
    model: deepseek_v3_45BA3B.yaml
    overrides:
      # log
      wandb_project: "Primus_DeepSeekV3_Pretrain"
      disable_wandb: false
      stderr_sink_level: DEBUG

      # debug
      num_layers: 4

      # hyber parameters
      train_iters: 10
      micro_batch_size: 1
      global_batch_size: 16
      seq_length: 4096
      max_position_embeddings: 4096
      lr: 1.0e-5
      min_lr: 0.0
      lr_warmup_iters: 2
      lr_decay_iters: null
      lr_decay_style: cosine

      # parallel
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 8

      # data
      train_data_path: /home/azureuser/tas-public/data/deepseek-datasets/mmap_deepseekv2_datasets_text_document
      valid_data_path: /home/azureuser/tas-public/data/deepseek-datasets/mmap_deepseekv2_datasets_text_document
      test_data_path: /home/azureuser/tas-public/data/deepseek-datasets/mmap_deepseekv2_datasets_text_document

      # fusion
      # 20250317: need latest apex in docker image
      gradient_accumulation_fusion: false
      # 20250317: TE grouped gemm has numerical issue
      moe_use_legacy_grouped_gemm: true

      # ckpt
      finetune: false
      auto_continue_train: true
      load: null
      no_load_optim: null
      no_load_rng: null
      save: null
      save_interval: 20000
      no_save_optim: null
      no_save_rng: null
      disable_last_saving: true
