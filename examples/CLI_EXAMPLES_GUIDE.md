# Primus CLI Examples - Quick Start Guide

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•ä½¿ç”¨ Primus CLI ç¤ºä¾‹è„šæœ¬è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚

## ğŸ“š æ¦‚è¿°

Primus æä¾›äº†ä¸‰ç§è®­ç»ƒæ¨¡å¼çš„ç¤ºä¾‹è„šæœ¬ï¼š

| è„šæœ¬ | æ¨¡å¼ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| `run_pretrain_cli.sh` | Direct | ç›´æ¥åœ¨ä¸»æœºä¸Šè¿è¡Œï¼Œæ— å®¹å™¨å¼€é”€ |
| `run_local_pretrain_cli.sh` | Container | ä½¿ç”¨ Docker/Podman å®¹å™¨ï¼Œç¯å¢ƒéš”ç¦» |
| `run_slurm_pretrain_cli.sh` | Slurm | é›†ç¾¤ç¯å¢ƒï¼Œå¤šèŠ‚ç‚¹è®­ç»ƒ |

---

## 1ï¸âƒ£ Direct Mode - ç›´æ¥æ¨¡å¼

**é€‚ç”¨åœºæ™¯**: åœ¨å·²é…ç½®å¥½çš„ç¯å¢ƒä¸­å¿«é€Ÿæµ‹è¯•å’Œè®­ç»ƒ

### ä½¿ç”¨æ–¹æ³•

```bash
# åŸºæœ¬ä½¿ç”¨
EXP=examples/megatron/exp_pretrain.yaml bash examples/run_pretrain_cli.sh

# æˆ–è€…å…ˆå¯¼å‡ºç¯å¢ƒå˜é‡
export EXP=examples/megatron/exp_pretrain.yaml
bash examples/run_pretrain_cli.sh
```

### å¿…éœ€å‚æ•°

- `EXP`: å®éªŒé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¿…é¡»å­˜åœ¨ï¼‰

### ç¤ºä¾‹

#### åœºæ™¯ 1: å¿«é€Ÿæµ‹è¯•ï¼ˆé»˜è®¤é…ç½®ï¼‰

```bash
# Megatron è®­ç»ƒï¼Œä½¿ç”¨é»˜è®¤é…ç½®
export EXP=examples/megatron/exp_pretrain.yaml
bash examples/run_pretrain_cli.sh
```

#### åœºæ™¯ 2: ä½¿ç”¨è‡ªå®šä¹‰é…ç½®

```bash
# è‡ªå®šä¹‰å®éªŒé…ç½®
export EXP=my_experiments/custom_config.yaml
bash examples/run_pretrain_cli.sh
```

#### åœºæ™¯ 3: ä¼ é€’é¢å¤–å‚æ•°

```bash
# ä¼ é€’è‡ªå®šä¹‰å‚æ•°åˆ°è®­ç»ƒå‘½ä»¤
export EXP=examples/megatron/exp_pretrain.yaml
bash examples/run_pretrain_cli.sh \
  --checkpoint-interval 500 \
  --log-level DEBUG \
  --enable-profiling
```

#### åœºæ™¯ 4: å®Œæ•´å‘½ä»¤è¡Œè°ƒç”¨ï¼ˆç»•è¿‡è„šæœ¬ï¼‰

å¦‚æœéœ€è¦æ›´å¤šæ§åˆ¶ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨ `primus-cli-direct.sh`:

```bash
# ç›´æ¥æ¨¡å¼ + NUMA ç»‘å®š + è‡ªå®šä¹‰æ—¥å¿—
bash $PRIMUS_PATH/runner/primus-cli-direct.sh \
  --numa \
  --log_file /tmp/training.log \
  -- train pretrain \
  --config examples/megatron/exp_pretrain.yaml \
  --checkpoint-interval 1000
```

---

## 2ï¸âƒ£ Container Mode - å®¹å™¨æ¨¡å¼

**é€‚ç”¨åœºæ™¯**: éœ€è¦ç¯å¢ƒéš”ç¦»ï¼Œæˆ–ä½¿ç”¨ç‰¹å®šçš„ Docker é•œåƒ

### ä½¿ç”¨æ–¹æ³•

```bash
# åŸºæœ¬ä½¿ç”¨ (PyTorch)
bash examples/run_local_pretrain_cli.sh

# MaxText/JAX è®­ç»ƒ
BACKEND=MaxText bash examples/run_local_pretrain_cli.sh
```

### ç¯å¢ƒå˜é‡

| å˜é‡ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `EXP` | `examples/megatron/exp_pretrain.yaml` | å®éªŒé…ç½®æ–‡ä»¶ |
| `BACKEND` | (ç©ºï¼Œä½¿ç”¨ PyTorch) | è®¾ç½®ä¸º `MaxText` ä½¿ç”¨ JAX é•œåƒ |
| `DOCKER_IMAGE` | PyTorch: `docker.io/rocm/primus:v25.10`<br>MaxText: `docker.io/rocm/jax-training:maxtext-v25.9` | Docker é•œåƒ |
| `DATA_PATH` | `$(pwd)/data` | æ•°æ®ç›®å½•ï¼ˆè‡ªåŠ¨æŒ‚è½½åˆ°å®¹å™¨ï¼‰ |
| `MASTER_ADDR` | `localhost` | ä¸»èŠ‚ç‚¹åœ°å€ |
| `MASTER_PORT` | `1234` | ä¸»èŠ‚ç‚¹ç«¯å£ |
| `NNODES` | `1` | èŠ‚ç‚¹æ•°é‡ |
| `NODE_RANK` | `0` | å½“å‰èŠ‚ç‚¹ç¼–å· |
| `GPUS_PER_NODE` | `8` | æ¯èŠ‚ç‚¹ GPU æ•°é‡ |

### ç¤ºä¾‹

#### åœºæ™¯ 1: åŸºæœ¬ä½¿ç”¨ï¼ˆé»˜è®¤é…ç½®ï¼‰

```bash
# PyTorch è®­ç»ƒï¼Œä½¿ç”¨é»˜è®¤é•œåƒå’Œé…ç½®
bash examples/run_local_pretrain_cli.sh

# æŒ‡å®šé…ç½®æ–‡ä»¶
EXP=examples/megatron/exp_pretrain.yaml \
bash examples/run_local_pretrain_cli.sh
```

#### åœºæ™¯ 2: ä½¿ç”¨è‡ªå®šä¹‰é•œåƒ

```bash
# ä½¿ç”¨è‡ªå®šä¹‰ Docker é•œåƒ
DOCKER_IMAGE=my-registry.com/custom-image:v1.0 \
DATA_PATH=/mnt/shared/datasets \
bash examples/run_local_pretrain_cli.sh
```

#### åœºæ™¯ 3: æ·»åŠ ç¯å¢ƒå˜é‡ï¼ˆæ€§èƒ½è°ƒä¼˜ï¼‰

```bash
# æ·»åŠ  NCCL å’Œ PyTorch æ€§èƒ½ç›¸å…³ç¯å¢ƒå˜é‡
bash $PRIMUS_PATH/runner/primus-cli container \
  --image docker.io/rocm/primus:v25.10 \
  --volume $(pwd)/data:/data \
  --env HSA_NO_SCRATCH_RECLAIM=1 \
  --env NVTE_CK_USES_BWD_V3=1 \
  --env GPU_MAX_HW_QUEUES=2 \
  --env NCCL_SOCKET_IFNAME=eth0 \
  --env TORCH_DISTRIBUTED_DEBUG=OFF \
-- train pretrain --config examples/megatron/exp_pretrain.yaml
```

**è¯´æ˜**:
- `HSA_NO_SCRATCH_RECLAIM=1`: ROCm æ€§èƒ½ä¼˜åŒ–
- `NVTE_CK_USES_BWD_V3=1`: TransformerEngine åå‘ä¼ æ’­ä¼˜åŒ–
- `GPU_MAX_HW_QUEUES=2`: GPU ç¡¬ä»¶é˜Ÿåˆ—é…ç½®
- `NCCL_SOCKET_IFNAME=eth0`: æŒ‡å®š NCCL ä½¿ç”¨çš„ç½‘ç»œæ¥å£

#### åœºæ™¯ 4: MaxText/JAX è®­ç»ƒ + è‡ªå®šä¹‰é…ç½®

```bash
# MaxText åç«¯ + è‡ªå®šä¹‰é•œåƒ + ç¯å¢ƒå˜é‡
bash $PRIMUS_PATH/runner/primus-cli container \
  --image my-registry.com/jax-maxtext:custom \
  --volume /data/tokenized:/workspace/data \
  --env JAX_PLATFORMS=rocm \
  --env XLA_FLAGS="--xla_gpu_enable_async_all_gather=true --xla_gpu_all_reduce_combine_threshold_bytes=134217728" \
  --env NVTE_FUSED_ATTN=1 \
-- train pretrain --config examples/maxtext/config.yaml
```

#### åœºæ™¯ 5: å¤šèŠ‚ç‚¹è®­ç»ƒï¼ˆæœ¬åœ°å¤šå®¹å™¨ååŒï¼‰

åœ¨å¤šå°ç‰©ç†æœºä¸Šåˆ†åˆ«è¿è¡Œï¼š

```bash
# ä¸»èŠ‚ç‚¹ (Node 0)
NNODES=2 \
NODE_RANK=0 \
MASTER_ADDR=192.168.1.100 \
MASTER_PORT=29500 \
GPUS_PER_NODE=8 \
bash examples/run_local_pretrain_cli.sh

# ä»èŠ‚ç‚¹ (Node 1)
NNODES=2 \
NODE_RANK=1 \
MASTER_ADDR=192.168.1.100 \
MASTER_PORT=29500 \
GPUS_PER_NODE=8 \
bash examples/run_local_pretrain_cli.sh
```

#### åœºæ™¯ 6: å®Œæ•´çš„é•œåƒ + å·æŒ‚è½½ + ç¯å¢ƒå˜é‡ç¤ºä¾‹

```bash
#!/bin/bash
# advanced_container_training.sh

export DOCKER_IMAGE=docker.io/rocm/primus:v25.10
export DATA_PATH=/shared/datasets/llama
export EXP=experiments/llama3_70b.yaml

bash $PRIMUS_PATH/runner/primus-cli container \
  --image $DOCKER_IMAGE \
  --volume $DATA_PATH:/data:ro \
  --volume $(pwd)/checkpoints:/checkpoints \
  --volume $(pwd)/output:/workspace/output \
  --env NCCL_DEBUG=WARN \
  --env TORCH_DISTRIBUTED_DEBUG=OFF \
  --env CUDA_DEVICE_MAX_CONNECTIONS=1 \
  --env HF_TOKEN \
  --env WANDB_API_KEY \
  --gpus all \
  --shm-size 16g \
-- train pretrain --config $EXP --checkpoint-dir /checkpoints
```

**é…ç½®è¯´æ˜**:
- `/data` åªè¯»æŒ‚è½½ (`:ro`)
- `/checkpoints` å¯è¯»å†™æŒ‚è½½
- `HF_TOKEN` å’Œ `WANDB_API_KEY` ä»ä¸»æœºç¯å¢ƒä¼ é€’
- `--gpus all`: ä½¿ç”¨æ‰€æœ‰å¯ç”¨ GPU
- `--shm-size 16g`: å¢åŠ å…±äº«å†…å­˜ï¼ˆå¤§æ¨¡å‹è®­ç»ƒå¿…éœ€ï¼‰

---

## 3ï¸âƒ£ Slurm Mode - é›†ç¾¤æ¨¡å¼

**é€‚ç”¨åœºæ™¯**: ä½¿ç”¨ Slurm ç®¡ç†çš„é›†ç¾¤ç¯å¢ƒï¼Œå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒ

### ä½¿ç”¨æ–¹æ³•

```bash
# åŸºæœ¬ä½¿ç”¨
bash examples/run_slurm_pretrain_cli.sh

# æŒ‡å®šèŠ‚ç‚¹æ•°å’ŒèŠ‚ç‚¹åˆ—è¡¨
NNODES=4 NODES_LIST="node[01-04]" \
bash examples/run_slurm_pretrain_cli.sh
```

### ç¯å¢ƒå˜é‡

| å˜é‡ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `EXP` | `examples/megatron/exp_pretrain.yaml` | å®éªŒé…ç½®æ–‡ä»¶ |
| `NNODES` | `1` | ä½¿ç”¨çš„èŠ‚ç‚¹æ•°é‡ |
| `NODES_LIST` | `node[02,03,10,14,15,34,38]` | Slurm èŠ‚ç‚¹åˆ—è¡¨ |
| `MASTER_PORT` | `12345` | ä¸»èŠ‚ç‚¹ç«¯å£ |
| `LOG_DIR` | `./output` | æ—¥å¿—è¾“å‡ºç›®å½• |

### ç¤ºä¾‹

#### åœºæ™¯ 1: å•èŠ‚ç‚¹å¿«é€Ÿæµ‹è¯•

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®ï¼Œå•èŠ‚ç‚¹è®­ç»ƒ
bash examples/run_slurm_pretrain_cli.sh
```

#### åœºæ™¯ 2: æ ‡å‡†å¤šèŠ‚ç‚¹è®­ç»ƒ

```bash
# 4 èŠ‚ç‚¹è®­ç»ƒ
export NNODES=4
export NODES_LIST="node[01-04]"
export EXP=examples/megatron/exp_pretrain.yaml
bash examples/run_slurm_pretrain_cli.sh
```

#### åœºæ™¯ 3: ä½¿ç”¨å®¹å™¨é•œåƒ + ç¯å¢ƒå˜é‡

```bash
# Slurm æ¨¡å¼ä¸‹ä½¿ç”¨è‡ªå®šä¹‰ Docker é•œåƒå’Œç¯å¢ƒå˜é‡
bash $PRIMUS_PATH/runner/primus-cli slurm srun \
  -N 4 \
  --nodelist "node[01-04]" \
-- \
  --image docker.io/rocm/primus:v25.10 \
  --env NCCL_DEBUG=INFO \
  --env TORCH_DISTRIBUTED_DEBUG=DETAIL \
  --env CUDA_DEVICE_MAX_CONNECTIONS=1 \
-- train pretrain --config examples/megatron/exp_pretrain.yaml
```

**è¯´æ˜**:
- `--image`: æŒ‡å®š Docker é•œåƒ
- `--env`: è®¾ç½®å®¹å™¨å†…ç¯å¢ƒå˜é‡ï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰
- ç¬¬ä¸€ä¸ª `--` åˆ†éš” Slurm é€‰é¡¹
- ç¬¬äºŒä¸ª `--` åˆ†éš”å®¹å™¨é€‰é¡¹
- æœ€åæ˜¯ Primus è®­ç»ƒå‘½ä»¤

#### åœºæ™¯ 4: å¤§è§„æ¨¡è®­ç»ƒ + å®Œæ•´é…ç½®

```bash
#!/bin/bash
# large_scale_training.sh - å¤§è§„æ¨¡ Llama3 70B è®­ç»ƒ

# å®éªŒé…ç½®
export EXP=experiments/llama3_70b.yaml
export LOG_DIR=/shared/experiments/llama3_70b_$(date +%Y%m%d_%H%M%S)
mkdir -p "$LOG_DIR"

# é›†ç¾¤é…ç½®
export NNODES=32
export NODES_LIST="gpu[001-032]"
export MASTER_PORT=29500

# Slurm + å®¹å™¨ + ç¯å¢ƒå˜é‡ å®Œæ•´ç¤ºä¾‹
bash $PRIMUS_PATH/runner/primus-cli slurm srun \
  -N $NNODES \
  --nodelist "$NODES_LIST" \
  --gpus-per-node=8 \
  --ntasks-per-node=8 \
-- \
  --image docker.io/rocm/primus:v25.10 \
  --volume /shared/datasets:/data \
  --volume /shared/checkpoints:/checkpoints \
  --env NCCL_DEBUG=INFO \
  --env NCCL_IB_HCA=mlx5_0,mlx5_1,mlx5_2,mlx5_3 \
  --env TORCH_DISTRIBUTED_DEBUG=OFF \
  --env CUDA_DEVICE_MAX_CONNECTIONS=1 \
  --env HF_TOKEN \
-- train pretrain --config $EXP 2>&1 | tee "$LOG_DIR/training.log"
```

**é…ç½®è¯´æ˜**:
- 32 èŠ‚ç‚¹ï¼Œæ¯èŠ‚ç‚¹ 8 GPU (256 GPUs æ€»è®¡)
- æŒ‚è½½å…±äº«æ•°æ®é›†å’Œ checkpoint ç›®å½•
- é…ç½® NCCL é€šä¿¡å‚æ•°
- HF_TOKEN ä»ä¸»æœºç¯å¢ƒä¼ é€’ï¼ˆç”¨äºä¸‹è½½æ¨¡å‹ï¼‰
- æ—¥å¿—åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œç»ˆç«¯

#### åœºæ™¯ 5: MaxText/JAX è®­ç»ƒ

```bash
# MaxText åç«¯è®­ç»ƒï¼ˆä½¿ç”¨ JAX é•œåƒï¼‰
export EXP=examples/maxtext/config.yaml
export NNODES=8
export NODES_LIST="gpu[01-08]"

bash $PRIMUS_PATH/runner/primus-cli slurm srun \
  -N $NNODES \
  --nodelist "$NODES_LIST" \
-- \
  --image docker.io/rocm/jax-training:maxtext-v25.9 \
  --env JAX_COORDINATOR_IP \
  --env JAX_COORDINATOR_PORT \
  --env XLA_FLAGS="--xla_gpu_enable_async_all_gather=true" \
  --env NVTE_FUSED_ATTN=1 \
-- train pretrain --config $EXP
```

#### åœºæ™¯ 6: ä½¿ç”¨ç‰¹å®š GPU èŠ‚ç‚¹ + è°ƒè¯•æ¨¡å¼

```bash
# æŒ‡å®šé«˜æ€§èƒ½ GPU èŠ‚ç‚¹ + å¼€å¯è°ƒè¯•
LOG_DIR=/tmp/debug_run \
bash $PRIMUS_PATH/runner/primus-cli slurm srun \
  -N 2 \
  --nodelist "mi300x[01-02]" \
  --constraint="mi300x" \
-- \
  --env NCCL_DEBUG=INFO \
  --env TORCH_DISTRIBUTED_DEBUG=DETAIL \
  --env PRIMUS_LOG_LEVEL=DEBUG \
-- train pretrain \
  --config examples/megatron/exp_pretrain.yaml \
  --dry-run
```

#### åœºæ™¯ 7: ç®€åŒ–ç‰ˆæœ¬ï¼ˆä½¿ç”¨è„šæœ¬çš„é»˜è®¤è¡Œä¸ºï¼‰

å¦‚æœä½ çš„éœ€æ±‚ç®€å•ï¼Œå¯ä»¥ç¼–è¾‘ `run_slurm_pretrain_cli.sh` è„šæœ¬ï¼š

```bash
# 1. ç¼–è¾‘è„šæœ¬ï¼Œä¿®æ”¹é»˜è®¤å€¼
nano examples/run_slurm_pretrain_cli.sh

# 2. ä¿®æ”¹è¿™äº›è¡Œï¼š
# export NNODES=${NNODES:-4}              # é»˜è®¤ 4 èŠ‚ç‚¹
# export NODES_LIST=${NODES_LIST:-"gpu[01-04]"}  # ä½ çš„èŠ‚ç‚¹åˆ—è¡¨

# 3. æ·»åŠ è‡ªå®šä¹‰ç¯å¢ƒå˜é‡ï¼ˆåœ¨ç¬¬ 27 è¡Œé™„è¿‘ï¼‰ï¼š
#    --env NCCL_DEBUG=INFO \
#    --env CUDA_DEVICE_MAX_CONNECTIONS=1 \

# 4. ç›´æ¥è¿è¡Œ
bash examples/run_slurm_pretrain_cli.sh
```

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### ä¼ é€’é¢å¤–å‚æ•°

æ‰€æœ‰è„šæœ¬éƒ½æ”¯æŒä¼ é€’é¢å¤–å‚æ•°åˆ° `primus train` å‘½ä»¤ï¼š

```bash
# Direct mode
bash examples/run_pretrain_cli.sh --extra-param value

# Container mode
bash examples/run_local_pretrain_cli.sh --debug --dry-run

# Slurm mode
bash examples/run_slurm_pretrain_cli.sh --checkpoint-interval 100
```

### ç¯å¢ƒå˜é‡ä¼ é€’ï¼ˆContainer Modeï¼‰

`run_local_pretrain_cli.sh` æ”¯æŒä¼ é€’ç¯å¢ƒå˜é‡åˆ°å®¹å™¨ï¼š

```bash
# è„šæœ¬ä¸­å·²åŒ…å«çš„ç¯å¢ƒå˜é‡ç¤ºä¾‹ï¼š
# --env HSA_NO_SCRATCH_RECLAIM     # ä»ä¸»æœºä¼ é€’
# --env NVTE_CK_USES_BWD_V3        # ä»ä¸»æœºä¼ é€’
# --env GPU_MAX_HW_QUEUES          # ä»ä¸»æœºä¼ é€’
# --env GLOO_SOCKET_IFNAME         # ä»ä¸»æœºä¼ é€’

# åœ¨ä¸»æœºè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå®¹å™¨ä¼šè‡ªåŠ¨è·å–
export HSA_NO_SCRATCH_RECLAIM=1
export GPU_MAX_HW_QUEUES=2
bash examples/run_local_pretrain_cli.sh
```

---

## ğŸ“ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©ä½¿ç”¨å“ªä¸ªè„šæœ¬ï¼Ÿ

**A**:
- ğŸƒ **å¿«é€Ÿæµ‹è¯•**: ä½¿ç”¨ `run_pretrain_cli.sh`ï¼ˆç›´æ¥æ¨¡å¼ï¼‰
- ğŸ³ **ç¯å¢ƒéš”ç¦»**: ä½¿ç”¨ `run_local_pretrain_cli.sh`ï¼ˆå®¹å™¨æ¨¡å¼ï¼‰
- ğŸ–¥ï¸ **å¤šèŠ‚ç‚¹è®­ç»ƒ**: ä½¿ç”¨ `run_slurm_pretrain_cli.sh`ï¼ˆSlurm æ¨¡å¼ï¼‰

### Q: å®¹å™¨æ¨¡å¼çš„æ•°æ®è·¯å¾„å¦‚ä½•è®¾ç½®ï¼Ÿ

**A**: ä½¿ç”¨ `DATA_PATH` ç¯å¢ƒå˜é‡ï¼Œè¯¥è·¯å¾„ä¼šè‡ªåŠ¨æŒ‚è½½åˆ°å®¹å™¨å†…ï¼š

```bash
DATA_PATH=/mnt/shared/datasets bash examples/run_local_pretrain_cli.sh
```

### Q: Slurm æ¨¡å¼å¦‚ä½•æŸ¥çœ‹æ—¥å¿—ï¼Ÿ

**A**: æ—¥å¿—ä¼šä¿å­˜åˆ° `LOG_DIR/log_slurm_pretrain.txt`ï¼š

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f ./output/log_slurm_pretrain.txt

# æˆ–æŒ‡å®šæ—¥å¿—ç›®å½•
LOG_DIR=/tmp/my_logs bash examples/run_slurm_pretrain_cli.sh
tail -f /tmp/my_logs/log_slurm_pretrain.txt
```

### Q: å¦‚ä½•éªŒè¯è„šæœ¬é…ç½®æ˜¯å¦æ­£ç¡®ï¼Ÿ

**A**: ä½¿ç”¨ `--dry-run` å‚æ•°ï¼ˆDirect å’Œ Container æ¨¡å¼æ”¯æŒï¼‰ï¼š

```bash
# éªŒè¯é…ç½®ä½†ä¸å®é™…æ‰§è¡Œ
bash examples/run_pretrain_cli.sh --dry-run
bash examples/run_local_pretrain_cli.sh --dry-run
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [Primus CLI å®Œæ•´æ–‡æ¡£](../runner/README.md)
- [é…ç½®æ–‡ä»¶ç¤ºä¾‹](../examples/)
- [æ•…éšœæ’æŸ¥æŒ‡å—](../docs/troubleshooting.md)

---

## ğŸ¯ å¿«é€Ÿå‚è€ƒ

```bash
# ===== Direct Mode =====
EXP=config.yaml bash examples/run_pretrain_cli.sh

# ===== Container Mode (PyTorch) =====
bash examples/run_local_pretrain_cli.sh

# ===== Container Mode (MaxText) =====
BACKEND=MaxText bash examples/run_local_pretrain_cli.sh

# ===== Slurm Mode =====
NNODES=4 NODES_LIST="node[01-04]" bash examples/run_slurm_pretrain_cli.sh
```

---

**æ›´æ–°æ—¶é—´**: 2026-01-09
**ç‰ˆæœ¬**: v1.0
