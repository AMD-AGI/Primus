# Megatron-Bridge AMD GPU ä¾èµ–å®‰è£…æŒ‡å—

ä» `pyproject.toml` æå–çš„ä¾èµ–ï¼Œæ ‡æ³¨ AMD GPU ä¸Šçš„å®‰è£…ç­–ç•¥ã€‚

## âœ… å¿…éœ€çš„é€šç”¨ä¾èµ–ï¼ˆAMD + NVIDIA é€šç”¨ï¼‰

```bash
# Python ç¯å¢ƒè¦æ±‚
# requires-python = ">=3.10"

# æ ¸å¿ƒä¾èµ– - ç›´æ¥å®‰è£…
pip install "transformers<5.0.0"
pip install datasets
pip install accelerate
pip install "omegaconf>=2.3.0"
pip install "tensorboard>=2.19.0"
pip install typing-extensions
pip install rich
pip install "wandb>=0.19.10"
pip install "six>=1.17.0"
pip install "regex>=2024.11.6"
pip install "pyyaml>=6.0.2"
pip install "tqdm>=4.67.1"
pip install "hydra-core>1.3,<=1.3.2"

# æ¨¡å‹ç‰¹å®šä¾èµ–
pip install qwen-vl-utils
pip install timm
pip install "open-clip-torch>=3.2.0"
```

## ğŸ”§ éœ€è¦ç‰¹æ®Šå¤„ç†çš„ä¾èµ–ï¼ˆAMD ç‰ˆæœ¬ï¼‰

### 1. Megatron-Coreï¼ˆAMD å®šåˆ¶ç‰ˆæœ¬ï¼‰

```bash
# åŸå§‹ä¾èµ–:
# megatron-core[dev,mlm]>=0.15.0a0,<0.17.0

# AMD å®‰è£…æ–¹å¼:
# ä½¿ç”¨ third_party/Megatron-LM (AMD é€‚é…ç‰ˆæœ¬)
cd ${PRIMUS_ROOT}/third_party/Megatron-LM
pip install -e ".[dev,mlm]"
```

**ä½ç½®**: `third_party/Megatron-LM/`ï¼ˆå·²åŒ…å«åœ¨ Primus é¡¹ç›®ä¸­ï¼‰

### 2. Transformer-Engineï¼ˆAMD ROCm ç‰ˆæœ¬ï¼‰

```bash
# åŸå§‹ä¾èµ–:
# transformer-engine[pytorch]>=2.10.0a0,<2.12.0

# AMD å®‰è£…æ–¹å¼:
# å®‰è£… ROCm ç‰ˆæœ¬çš„ transformer-engine-torch
pip install transformer-engine-torch  # ROCm ç‰¹å®šç‰ˆæœ¬
```

**æ³¨æ„**: 
- NVIDIA ç‰ˆæœ¬: `transformer-engine`
- AMD ç‰ˆæœ¬: `transformer-engine-torch`
- åŠŸèƒ½ç›¸åŒï¼Œä½†é’ˆå¯¹ä¸åŒçš„ç¡¬ä»¶åç«¯

### 3. Flash Attention ç›¸å…³ï¼ˆå¯é€‰ï¼Œæ€§èƒ½ä¼˜åŒ–ï¼‰

```bash
# åŸå§‹ä¾èµ–:
# flash-linear-attention

# AMD å®‰è£…æ–¹å¼:
# æ£€æŸ¥æ˜¯å¦æœ‰ ROCm æ”¯æŒï¼Œæˆ–ä»æºç ç¼–è¯‘
# å¦‚æœä¸æ”¯æŒï¼Œå¯ä»¥è·³è¿‡ï¼ˆä¼šä½¿ç”¨ fallback å®ç°ï¼‰
pip install flash-linear-attention  # å°è¯•å®‰è£…ï¼Œå¦‚æœå¤±è´¥åˆ™è·³è¿‡
```

### 4. Mamba ç›¸å…³ï¼ˆå¯é€‰ï¼Œç‰¹å®šæ¨¡å‹æ¶æ„ï¼‰

```bash
# åŸå§‹ä¾èµ–:
# mamba-ssm
# causal-conv1d

# AMD å®‰è£…æ–¹å¼:
# è¿™äº›åŒ…ä¸»è¦ç”¨äº Mamba æ¶æ„æ¨¡å‹
# å¦‚æœä¸ä½¿ç”¨ Mamba æ¨¡å‹ï¼Œå¯ä»¥è·³è¿‡
# pip install mamba-ssm        # å¦‚æœéœ€è¦ Mamba æ¨¡å‹
# pip install causal-conv1d    # å¦‚æœéœ€è¦ Mamba æ¨¡å‹
```

**æ³¨æ„**: Mamba æ¨¡å‹ä¸æ˜¯å¿…éœ€çš„ï¼Œåªæœ‰ä½¿ç”¨ç‰¹å®šæ¨¡å‹æ—¶æ‰éœ€è¦ã€‚

## âŒ NVIDIA ç‰¹å®šä¾èµ–ï¼ˆAMD ä¸Šè·³è¿‡ï¼‰

```bash
# âŒ è·³è¿‡ - NVIDIA ç‰¹å®š
# nvidia-resiliency-ext      # NVIDIA å®¹é”™æ‰©å±•
# nvidia-modelopt[torch]     # NVIDIA æ¨¡å‹ä¼˜åŒ–å·¥å…·
# nv-grouped-gemm            # NVIDIA GEMM ä¼˜åŒ–
```

### nvidia-resiliency-ext

**åŸå§‹ä¾èµ–**:
```toml
nvidia-resiliency-ext
```

**AMD å¤„ç†**:
- âŒ è·³è¿‡å®‰è£…
- åŠŸèƒ½: æä¾›å®¹é”™å’Œæ£€æŸ¥ç‚¹æ¢å¤
- AMD æ›¿ä»£: ä½¿ç”¨ PyTorch åŸç”Ÿçš„åˆ†å¸ƒå¼å®¹é”™æœºåˆ¶

### nvidia-modelopt

**åŸå§‹ä¾èµ–**:
```toml
nvidia-modelopt[torch]>=0.37.0
```

**AMD å¤„ç†**:
- âŒ è·³è¿‡å®‰è£…ï¼ˆå·²éªŒè¯æœ‰å…¼å®¹æ€§é—®é¢˜ï¼‰
- åŠŸèƒ½: æ¨¡å‹é‡åŒ–ã€å‰ªæã€ä¼˜åŒ–
- é—®é¢˜: ä¾èµ– `torch.onnx._type_utils`ï¼ˆæ–°ç‰ˆ PyTorch ä¸­å·²ç§»é™¤ï¼‰
- AMD æ›¿ä»£: ä½¿ç”¨ PyTorch åŸç”Ÿé‡åŒ–æˆ–å…¶ä»–å·¥å…·

## ğŸ“¦ å®Œæ•´çš„ AMD å®‰è£…è„šæœ¬

```bash
#!/usr/bin/env bash
set -euo pipefail

echo "================================================"
echo "Installing Megatron-Bridge Dependencies for AMD"
echo "================================================"

# 1. åŸºç¡€ä¾èµ–
echo "[1/6] Installing core dependencies..."
pip install -U pip setuptools wheel

pip install \
    "transformers<5.0.0" \
    datasets \
    accelerate \
    "omegaconf>=2.3.0" \
    "tensorboard>=2.19.0" \
    typing-extensions \
    rich \
    "wandb>=0.19.10" \
    "six>=1.17.0" \
    "regex>=2024.11.6" \
    "pyyaml>=6.0.2" \
    "tqdm>=4.67.1" \
    "hydra-core>1.3,<=1.3.2"

# 2. æ¨¡å‹ç‰¹å®šä¾èµ–
echo "[2/6] Installing model-specific dependencies..."
pip install \
    qwen-vl-utils \
    timm \
    "open-clip-torch>=3.2.0"

# 3. Transformer-Engine (AMD ROCm ç‰ˆæœ¬)
echo "[3/6] Installing Transformer-Engine for ROCm..."
pip install transformer-engine-torch

# 4. Megatron-Core (AMD é€‚é…ç‰ˆæœ¬)
echo "[4/6] Installing Megatron-Core (AMD version)..."
cd "${PRIMUS_ROOT}/third_party/Megatron-LM"
pip install -e ".[dev,mlm]"

# 5. å¯é€‰ï¼šFlash Attentionï¼ˆå¦‚æœæ”¯æŒï¼‰
echo "[5/6] Installing optional dependencies..."
pip install flash-linear-attention || echo "[WARNING] flash-linear-attention not available, using fallback"

# 6. Megatron-Bridge (ä»æºç å®‰è£…)
echo "[6/6] Installing Megatron-Bridge..."
cd "${PRIMUS_ROOT}/third_party/Megatron-Bridge"
pip install -e .

echo "================================================"
echo "âœ… Installation complete!"
echo "================================================"
```

## ğŸ” ä¾èµ–å¯¹æ¯”è¡¨

| ä¾èµ–åŒ… | NVIDIA ç‰ˆæœ¬ | AMD ç‰ˆæœ¬ | çŠ¶æ€ | è¯´æ˜ |
|--------|------------|----------|------|------|
| **æ ¸å¿ƒæ¡†æ¶** |
| PyTorch | `torch` | `torch+rocm` | âœ… å·²å®‰è£… | AMD ä½¿ç”¨ ROCm ç‰ˆæœ¬ |
| Transformers | `<5.0.0` | `<5.0.0` | âœ… é€šç”¨ | æ— å·®å¼‚ |
| Megatron-Core | `>=0.15.0` | AMD åˆ†æ”¯ | âœ… é€‚é… | ä½¿ç”¨ AMD å®šåˆ¶ç‰ˆæœ¬ |
| **åŠ é€Ÿåº“** |
| Transformer-Engine | `transformer-engine` | `transformer-engine-torch` | âœ… é€‚é… | ROCm ä¸“ç”¨ç‰ˆæœ¬ |
| Flash Attention | `flash-linear-attention` | åŒå·¦ | âš ï¸ å¯é€‰ | å¯èƒ½éœ€è¦æºç ç¼–è¯‘ |
| Mamba | `mamba-ssm` | åŒå·¦ | âš ï¸ å¯é€‰ | ä»…ç‰¹å®šæ¨¡å‹éœ€è¦ |
| **NVIDIA ç‰¹å®š** |
| nvidia-resiliency-ext | âœ… å®‰è£… | âŒ è·³è¿‡ | ğŸ”„ æ›¿ä»£ | ä½¿ç”¨ PyTorch åŸç”ŸåŠŸèƒ½ |
| nvidia-modelopt | âœ… å®‰è£… | âŒ è·³è¿‡ | âŒ ä¸å…¼å®¹ | æœ‰ ONNX API é—®é¢˜ |
| nv-grouped-gemm | âœ… å®‰è£… | âŒ è·³è¿‡ | ğŸ”„ æ›¿ä»£ | ROCm æœ‰å…¶ä»–ä¼˜åŒ– |
| **å·¥å…·åº“** |
| omegaconf | `>=2.3.0` | åŒå·¦ | âœ… é€šç”¨ | é…ç½®ç®¡ç† |
| hydra-core | `>1.3,<=1.3.2` | åŒå·¦ | âœ… é€šç”¨ | é…ç½®ç®¡ç† |
| wandb | `>=0.19.10` | åŒå·¦ | âœ… é€šç”¨ | å®éªŒè·Ÿè¸ª |
| tensorboard | `>=2.19.0` | åŒå·¦ | âœ… é€šç”¨ | å¯è§†åŒ– |

## ğŸ“ æ³¨æ„äº‹é¡¹

### 1. ROCm ç‰ˆæœ¬å…¼å®¹æ€§

ç¡®ä¿ä½ çš„ PyTorch æ˜¯ä¸º ROCm ç¼–è¯‘çš„ç‰ˆæœ¬ï¼š

```bash
python -c "import torch; print(torch.version.hip)"  # åº”è¯¥è¾“å‡º ROCm ç‰ˆæœ¬å·
```

### 2. ç¯å¢ƒå˜é‡

AMD GPU å¯èƒ½éœ€è¦è®¾ç½®ç‰¹å®šçš„ç¯å¢ƒå˜é‡ï¼š

```bash
export HSA_NO_SCRATCH_RECLAIM=1
export ROCM_PATH=/opt/rocm
export PATH=$ROCM_PATH/bin:$PATH
```

### 3. ä¸æ”¯æŒçš„åŠŸèƒ½

ä»¥ä¸‹åŠŸèƒ½åœ¨ AMD ä¸Šå¯èƒ½ä¸å¯ç”¨æˆ–æœ‰é™åˆ¶ï¼š

- âŒ NVIDIA-specific quantization (nvidia-modelopt)
- âš ï¸ æŸäº›è‡ªå®šä¹‰ CUDA kernels å¯èƒ½éœ€è¦æ›¿æ¢ä¸º ROCm ç‰ˆæœ¬
- âš ï¸ Flash Attention å¯èƒ½æœ‰æ€§èƒ½å·®å¼‚

### 4. æ€§èƒ½ä¼˜åŒ–

AMD GPU ä¸Šçš„æœ€ä½³å®è·µï¼š

1. **ä½¿ç”¨ transformer-engine-torch**: æä¾› ROCm ä¼˜åŒ–çš„ç®—å­
2. **å¯ç”¨ Flash Attention**: å¦‚æœå¯ç”¨ï¼Œä¼šæ˜¾è‘—æå‡æ€§èƒ½
3. **è°ƒæ•´ batch size**: AMD GPU å†…å­˜ç‰¹æ€§å¯èƒ½éœ€è¦ä¸åŒçš„é…ç½®
4. **ä½¿ç”¨ BF16**: AMD GPUs é€šå¸¸å¯¹ BF16 æœ‰è‰¯å¥½æ”¯æŒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

æœ€å°åŒ–å®‰è£…ï¼ˆä»…æ ¸å¿ƒåŠŸèƒ½ï¼‰ï¼š

```bash
# 1. æ ¸å¿ƒä¾èµ–
pip install transformers datasets accelerate omegaconf tensorboard

# 2. AMD Transformer-Engine
pip install transformer-engine-torch

# 3. Megatron-Core (AMD)
cd third_party/Megatron-LM && pip install -e .

# 4. Megatron-Bridge
cd third_party/Megatron-Bridge && pip install -e .
```

å®Œæ•´å®‰è£…ï¼ˆåŒ…å«æ‰€æœ‰åŠŸèƒ½ï¼‰ï¼š

```bash
# è¿è¡Œå®Œæ•´å®‰è£…è„šæœ¬
bash primus/backends/megatron_bridge/patches/install_amd_deps.sh
```

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: `ImportError: cannot import name '_type_utils' from 'torch.onnx'`

**åŸå› **: nvidia-modelopt ä¸å…¼å®¹  
**è§£å†³**: è·³è¿‡ nvidia-modelopt å®‰è£…ï¼ˆå·²å¤„ç†ï¼‰

### é—®é¢˜ 2: `ImportError: cannot import name 'Glm4vMoeForConditionalGeneration'`

**åŸå› **: Transformers ç‰ˆæœ¬ä¸æ”¯æŒæŸäº›æ¨¡å‹  
**è§£å†³**: åº”ç”¨ GLM-4V import patchï¼ˆå¯é€‰æ¨¡å‹ï¼‰

### é—®é¢˜ 3: Transformer-Engine ç¼–è¯‘å¤±è´¥

**åŸå› **: ROCm ç¯å¢ƒé…ç½®é—®é¢˜  
**è§£å†³**: 
```bash
# ç¡®ä¿ ROCm æ­£ç¡®å®‰è£…
export ROCM_PATH=/opt/rocm
export PATH=$ROCM_PATH/bin:$PATH
pip install transformer-engine-torch --no-cache-dir
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [Megatron-Bridge GitHub](https://github.com/NVIDIA/Megatron-Bridge)
- [Transformer-Engine ROCm](https://github.com/ROCm/TransformerEngine)
- [PyTorch ROCm](https://pytorch.org/get-started/locally/)
- [AMD ROCm Documentation](https://rocm.docs.amd.com/)
