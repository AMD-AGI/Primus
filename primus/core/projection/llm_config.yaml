
# runtime configuration
runtime:
  micro_batch_size: 1 # int
  hidden_size: null # int
  sequence_length: null # int

# embedding configuration
embedding:
  vocab_size: null # int

# layer configuration
layer:
  num_layers: null # int
  moe_layer_freq: 1 # int or str

  input_layernorm:
    normalization: null # RMSNorm, LayerNorm

  self_attention:
    type: null # mla, mha_gqa
    qk_layernorm: false
    apply_rope_fusion: false

    mha_gqa:
      num_attention_heads: null # int
      num_kv_heads: null # int
      # Projection weights dimension in multi-head attention.
      # This is set to hidden_size // num_attention_heads if not provided.
      kv_channels: null # int

    mla:
      num_attention_heads: null # int
      q_lora_rank: null
      kv_lora_rank: 512
      qk_head_dim: null # int
      qk_pos_emb_head_dim: null # int
      v_head_dim: null # int
      kv_channels: 128

    attention_dropout: 0.0 # float
    position_embedding_type: null # rope, alibi, none
    max_position_embeddings: null # int
    rope:
      rotary_base: 1000000 # int
      rotary_percent: 1.0 # float
      rotary_seq_len_interpolation_factor: 1 # int
      apply_rope_fusion: true # bool
