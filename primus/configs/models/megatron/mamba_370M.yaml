bases:
  - mamba_base.yaml

# Mamba 370M configuration
model_type: mamba  # CRITICAL: Mamba models must use mamba model type
tokenizer_type: GPT2BPETokenizer
vocab_size: 50257

# Model size parameters
num_layers: 48
hidden_size: 1024
num_attention_heads: 16  # Required by Megatron validation, even for pure Mamba models
ffn_hidden_size: null
mamba_state_dim: 16
mamba_head_dim: 64
mamba_num_groups: 8
