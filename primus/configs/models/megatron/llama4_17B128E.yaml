bases:
  - llama4_base.yaml


# huggingface model config: https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E/blob/main/config.json
# model parameters
max_position_embeddings: 262144
num_layers: 48
hidden_size: 5120
ffn_hidden_size: 16384
num_attention_heads: 40
qk_layernorm: false

# moe parameters
moe_layer_freq: 2
num_experts: 128
moe_ffn_hidden_size: 8192
moe_shared_expert_intermediate_size: 8192


tokenizer_type: Llama4Tokenizer
tokenizer_model: meta-llama/Llama-4-Maverick-17B-128E


# parallel and optimization
expert_model_parallel_size: 8
expert_tensor_parallel_size: null # int
moe_permute_fusion: true
moe_shared_expert_overlap: true