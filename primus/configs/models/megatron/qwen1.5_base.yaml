bases:
  - llama_base.yaml

#init_method_std: 0.01
#rotary_base: 1000000
qk_layernorm: false

group_query_attention: true
num_query_groups: null

# moe parameters
num_experts: 60
moe_router_topk: 8
moe_router_load_balancing_type: aux_loss
moe_aux_loss_coeff: 1e-3
moe_grouped_gemm: true
moe_token_dispatcher_type: alltoall
moe_shared_expert_overlap: true
moe_use_legacy_grouped_gemm: false

swiglu: true
add_bias_linear: false
attention_softmax_in_fp32: true
untie_embeddings_and_output_weights: true
hidden_dropout: 0.0
attention_dropout: 0.0

position_embedding_type: rope
rotary_base: 10000
rotary_percent: 1.0
rotary_seq_len_interpolation_factor: 1
normalization: RMSNorm

# Qwen 2.5 specific settings
max_position_embeddings: 8192
norm_epsilon: 1.0e-6
init_method_std: 0.008

apply_rope_fusion: true
masked_softmax_fusion: false
