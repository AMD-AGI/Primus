bases:
  - llama_base.yaml

# norm
norm_epsilon: 1.0e-06

# mla
multi_latent_attention: true
# multi_latent_attention does not support apply_rope_fusion
apply_rope_fusion: false

# moe
moe_layer_freq: 1
moe_router_topk: 6
moe_router_dtype: null # str, fp32, fp64
moe_router_score_function: softmax
moe_router_pre_softmax: true
# Two common use cases:
# 1) greedy: set null, no group
# 2) Device-limited routing: Set equal to expert parallel size (EP) to limit each token to experts on a subset of devices
#   (See DeepSeek-V2: https://arxiv.org/pdf/2405.04434)
# 3) Node-limited routing: Set equal to number of nodes in EP group to limit each token to experts on a subset of nodes
#   (See DeepSeek-V3: https://arxiv.org/pdf/2412.19437)')
moe_router_num_groups: null # int
moe_router_group_topk: null # int
moe_router_topk_scaling_factor: 1.0 # float
moe_router_enable_expert_bias: false
moe_router_bias_update_rate: 1.0e-3
moe_router_load_balancing_type: seq_aux_loss
moe_token_dispatcher_type: alltoall
moe_aux_loss_coeff: 0.001 # aux_loss_alpha

# rotary
rotary_base: 10000
rotary_scaling_factor: 40.0 # float
mscale: 0.707 # float
mscale_all_dim: 0.707 # float

# parallel and optimization
expert_model_parallel_size: 1
expert_tensor_parallel_size: null # int
moe_grouped_gemm: true
moe_use_legacy_grouped_gemm: false
moe_permute_fusion: true
moe_shared_expert_overlap: true
