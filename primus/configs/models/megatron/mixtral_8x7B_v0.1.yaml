bases:
  - mixtral_base.yaml

# model parameters
max_position_embeddings: 4096
num_layers: 32
hidden_size: 4096
ffn_hidden_size: 14336

num_attention_heads: 32
init_method_std: 0.01

tokenizer_type: MixtralTokenizer
tokenizer_model: mistralai/Mixtral-8x7B-v0.1
