bases:
  - deepseek_v2_base.yaml

tokenizer_type: DeepSeekV2Tokenizer
tokenizer_model: deepseek-ai/DeepSeek-V2-Lite

multi_latent_attention: false
apply_rope_fusion: true

# model
num_layers: 64 # full 64
hidden_size: 4096
ffn_hidden_size: 4096 # not used in moe layer
num_attention_heads: 64
group_query_attention: true
num_query_groups: 8
kv_channels: 128

# moe
moe_layer_freq: 1
num_experts: 256
# num_shared_experts: 1
moe_router_topk: 4
moe_ffn_hidden_size: 1024
moe_shared_expert_intermediate_size: null # num_shared_experts * moe_ffn_hidden_size
