bases:
  - mamba_base.yaml

# Mamba 1.4B configuration

tokenizer_type: GPT2BPETokenizer
vocab_size: 50257

# Model size parameters
num_layers: 48
hidden_size: 2048
ffn_hidden_size: null

max_position_embeddings: 2048

