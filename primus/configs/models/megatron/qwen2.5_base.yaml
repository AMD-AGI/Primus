bases:
  - llama_base.yaml

group_query_attention: true
num_query_groups: null
qk_layernorm: false

swiglu: true
add_bias_linear: false
attention_softmax_in_fp32: true
untie_embeddings_and_output_weights: true
hidden_dropout: 0.0
attention_dropout: 0.0

position_embedding_type: rope
rotary_base: 1000000
rotary_percent: 1.0
rotary_seq_len_interpolation_factor: 1
normalization: RMSNorm

# Qwen 2.5 specific settings
max_position_embeddings: 131072
norm_epsilon: 1.0e-6
init_method_std: 0.008

apply_rope_fusion: true
masked_softmax_fusion: false 