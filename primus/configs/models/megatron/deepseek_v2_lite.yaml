bases:
  - deepseek_v2_base.yaml

# https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite
# 16B total params, 2.4B active params

tokenizer_type: DeepSeekV2Tokenizer
tokenizer_model: deepseek-ai/DeepSeek-V2-Lite

# model
num_layers: 4
hidden_size: 2048
ffn_hidden_size: 10944
num_attention_heads: 16
# mla
q_lora_rank: null
kv_lora_rank: 512
qk_head_dim: 128
qk_pos_emb_head_dim: 64
v_head_dim: 128
kv_channels: 128
# moe
moe_layer_freq: 1
num_experts: 64
# num_shared_experts: 2
# moe_ffn_hidden_size: 1408
# moe_shared_expert_intermediate_size: 2816 # num_shared_experts * moe_ffn_hidden_size

# no group in deepseek v2 lite
expert_model_parallel_size: 8
moe_router_num_groups: null # int
moe_router_group_topk: null # int
moe_router_topk_scaling_factor: 1.0 # float
