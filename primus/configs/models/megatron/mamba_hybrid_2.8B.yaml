bases:
  - mamba_base.yaml

# Mamba 2.8B configuration with hybrid attention layers

tokenizer_type: GPT2BPETokenizer
vocab_size: 50257

# Model size parameters
num_layers: 64
hidden_size: 2560
ffn_hidden_size: 6827  # ~2.67x hidden_size

# Attention parameters (for hybrid layers)
num_attention_heads: 32
group_query_attention: true
num_query_groups: 8

# Hybrid configuration: override mamba_base defaults
hybrid_attention_ratio: 0.125
is_hybrid_model: true

# For hybrid models, position embeddings may be useful
position_embedding_type: rope
rotary_base: 10000
rotary_percent: 1.0

max_position_embeddings: 4096

