bases:
  - llama3_base.yaml

tokenizer_type: Llama3Tokenizer
tokenizer_model: meta-llama/Llama-3.1-8B

ffn_hidden_size: 28672
hidden_size: 8192
num_attention_heads: 64
num_layers: 80
num_query_groups: 8
