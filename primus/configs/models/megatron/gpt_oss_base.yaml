bases:
  - deepseek_v2_base.yaml


moe_router_topk: 4 # experts_per_token: 4
vocab_size: 201088
hidden_size: 2880 # 2880 # working 2048; inf 4096 # inf acddd, ; 
moe_ffn_hidden_size: 2880 # intermediate_size: 2880
multi_latent_attention: false
qk_head_dim: 64 
v_head_dim: 64 
# TODO
# kv_channels: 128
kv_channels: 64

group_query_attention: true
num_attention_heads: 64
num_query_groups: 8 # num_key_value_heads: 8
moe_shared_expert_intermediate_size: 2880 # num_shared_experts * moe_ffn_hidden_size

# window_size: [128, 0] # sliding_window: 128 #

# use_attention_with_yarn: true # set to true to use attention with yarn
# use_hybrid_sliding_window_attention: true
# mscale: 1.0
# rope_type: yarn # rope parameters
# beta_slow: 1.0 # yarn parameters
# beta_fast: 32.0 # yarn parameters
# original_max_position_embeddings: 4096 # yarn parameters