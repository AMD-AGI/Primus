bases:
  - grok1_base.yaml

# model parameters
max_position_embeddings: 131072
num_layers: 64
hidden_size: 6144
moe_ffn_hidden_size: 32768

num_attention_heads: 48
init_method_std: 0.01

rotary_base: 1000000
masked_softmax_fusion: false

tokenizer_type: MixtralTokenizer
tokenizer_model: mistralai/Mixtral-8x22B-v0.1
