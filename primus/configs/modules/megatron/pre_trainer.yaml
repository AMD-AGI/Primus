bases:
  - trainer_base.yaml

train_iters: 1000
global_batch_size: 16
eval_iters: 0
eval_interval: 1000
seq_length: 1024
num_workers: 8
split: null
dataloader_type: cyclic

finetune: true
save_interval: 1000
log_interval: 1
tensorboard_log_interval: 1
log_throughput: true
log_timers_to_tensorboard: true
log_batch_size_to_tensorboard: true
log_learning_rate_to_tensorboard: true

lr: 2.0e-05
min_lr: 0.0
lr_decay_iters: null
lr_warmup_iters: 40
weight_decay: 0.0
adam_beta2: 0.999
override_opt_param_scheduler: true

use_flash_attn: true
distributed_timeout_minutes: 60
use_distributed_optimizer: true
