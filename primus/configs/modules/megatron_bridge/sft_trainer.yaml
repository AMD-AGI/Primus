# Megatron-Bridge Post-training Base Configuration
# This configuration file defines the base settings for post-training tasks
# (SFT, instruction tuning, LoRA fine-tuning) using Megatron-Bridge

extends:
  - ../module_base.yaml

# # Mark as trainable module
trainable: true

stage: "sft"


# main control flag
enable_primus_turbo: true

# feature control flags
use_turbo_attention: false
use_turbo_parallel_linear: false
use_turbo_grouped_mlp: false
moe_use_fused_router_with_aux_score: false

# inner features flags
enable_turbo_attention_float8 : false


# deepep
use_turbo_deepep: false
turbo_deepep_num_cu: 32
turbo_deepep_use_comm_stream: false

# sync-free moe
turbo_sync_free_moe_stage: 0

# use turbo fused activation_with_probs to optmize redundant computation
use_turbo_fused_act_with_probs: false

# layer norm
use_turbo_rms_norm: false
