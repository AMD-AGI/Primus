# Megatron-Bridge Post-training Base Configuration
# This configuration file defines the base settings for post-training tasks
# (SFT, instruction tuning, LoRA fine-tuning) using Megatron-Bridge

extends:
  - ../module_base.yaml

# Mark as trainable module
trainable: true

# ============================================================================
# Training Configuration
# ============================================================================
# Post-training typically uses fewer iterations than pre-training
train_iters: 5000
eval_iters: 10
eval_interval: 100
full_validation: false

# Skip training flag (for testing)
skip_train: false

# ============================================================================
# Batch Configuration
# ============================================================================
# Post-training usually uses smaller batch sizes
micro_batch_size: 1
global_batch_size: 64
rampup_batch_size: null
decrease_batch_size_if_needed: false

# ============================================================================
# Sequence Length
# ============================================================================
# Shorter sequences are common for instruction/SFT tasks
seq_length: 2048
encoder_seq_length: null
decoder_seq_length: null

# ============================================================================
# Learning Rate Configuration
# ============================================================================
# Post-training uses lower learning rates than pre-training
lr: 5.0e-6
min_lr: 5.0e-7
lr_decay_style: cosine
lr_decay_iters: null
lr_decay_samples: null
lr_warmup_fraction: null
lr_warmup_iters: 100
lr_warmup_samples: 0
lr_warmup_init: 0.0

# ============================================================================
# Optimizer Configuration
# ============================================================================
optimizer: adam
weight_decay: 0.01          # Lower weight decay for fine-tuning
clip_grad: 1.0
adam_beta1: 0.9
adam_beta2: 0.95            # Lower beta2 for post-training
adam_eps: 1.0e-08
sgd_momentum: 0.9

# Override optimizer parameter scheduler
override_opt_param_scheduler: true
use_checkpoint_opt_param_scheduler: false

# ============================================================================
# Mixed Precision
# ============================================================================
fp16: false
bf16: true
grad_reduce_in_bf16: false
calculate_per_token_loss: false

# FP8 configuration (optional)
fp8: null                   # Set to 'e4m3' or 'hybrid' if needed
fp8_margin: 0
fp8_recipe: delayed
fp8_amax_history_len: 1024
fp8_amax_compute_algo: "max"
fp8_wgrad: true
fp8_param_gather: false

# ============================================================================
# Checkpointing Configuration
# ============================================================================
save: null
save_interval: 500          # Save more frequently for post-training
save_retain_interval: null
no_save_optim: null
no_save_rng: null
load: null
no_load_optim: null
no_load_rng: null

# Fine-tuning mode
finetune: true
use_checkpoint_args: false
use_mp_args_from_checkpoint_args: false
use_tokenizer_model_from_checkpoint_args: true
exit_on_missing_checkpoint: false

# Checkpoint format
ckpt_format: torch
auto_detect_ckpt_format: false
ckpt_fully_parallel_save: true
ckpt_fully_parallel_load: false
ckpt_assume_constant_structure: false
dist_ckpt_strictness: assume_ok_unexpected

# ============================================================================
# Data Configuration
# ============================================================================
# Data paths (will be overridden in specific configs)
data_path: null
train_data_path: null
valid_data_path: null
test_data_path: null
data_args_path: null
per_split_data_args_path: null

# Data processing
mock_data: false
data_sharding: true
split: "95,5,0"             # Train/Val/Test split for post-training
num_workers: 8
dataloader_type: null
mmap_bin_files: true
sample_rate: 1.0

# EOD (End of Document) mask loss
eod_mask_loss: false

# Create attention mask in dataloader
create_attention_mask_in_dataloader: true
num_dataset_builder_threads: 1

# ============================================================================
# Distributed Configuration
# ============================================================================
distributed_backend: nccl
distributed_timeout_minutes: 60

# Communication overlap (important for MoE)
overlap_p2p_comm: true
overlap_grad_reduce: true
overlap_param_gather: true
overlap_param_gather_with_optimizer_step: false

# Gradient reduction
align_grad_reduce: true
gradient_reduce_div_fusion: true

# Distributed optimizer
use_distributed_optimizer: false

# Pipeline parallelism
scatter_gather_tensors_in_pipeline: true
use_ring_exchange_p2p: false

# FSDP configuration
use_custom_fsdp: false
use_megatron_fsdp: false
data_parallel_sharding_strategy: no_shard

# ============================================================================
# Activation Recomputation (Optional - for memory efficiency)
# ============================================================================
recompute_activations: false
recompute_granularity: null     # 'full' or 'selective'
recompute_method: null          # 'uniform' or 'block'
recompute_num_layers: null
distribute_saved_activations: false

# ============================================================================
# Logging Configuration
# ============================================================================
log_interval: 10
log_avg_skip_iterations: 2
log_avg_reset_interval: 50

# Throughput logging
log_throughput: true
log_progress: false

# TensorBoard logging
tensorboard_log_interval: 1
tensorboard_queue_size: 1000
log_timers_to_tensorboard: true
log_batch_size_to_tensorboard: true
log_learning_rate_to_tensorboard: true
log_validation_ppl_to_tensorboard: false
log_memory_to_tensorboard: false
log_world_size_to_tensorboard: false
log_loss_scale_to_tensorboard: true

# Weights & Biases
wandb_project: null
wandb_exp_name: null
wandb_save_dir: null
wandb_entity: null

# TensorBoard directory
tensorboard_dir: null

# ============================================================================
# Initialization
# ============================================================================
seed: 1234
init_method_xavier_uniform: false
data_parallel_random_init: false

# ============================================================================
# Flash Attention
# ============================================================================
use_flash_attn: true

# ============================================================================
# Gradient and Loss Checking
# ============================================================================
check_for_nan_in_loss_and_grad: true
check_for_spiky_loss: false
check_for_large_grads: false

# ============================================================================
# Vocabulary Configuration
# ============================================================================
make_vocab_size_divisible_by: 128

# ============================================================================
# Exit Handling
# ============================================================================
exit_signal_handler: false
exit_duration_in_mins: null
exit_interval: null

# ============================================================================
# Auto-resume
# ============================================================================
adlr_autoresume: false
adlr_autoresume_interval: 1000

# ============================================================================
# Garbage Collection
# ============================================================================
manual_gc: false
manual_gc_interval: 1
manual_gc_eval: false

# ============================================================================
# Memory Management
# ============================================================================
empty_unused_memory_level: 0

# ============================================================================
# Profiling
# ============================================================================
profile: false
use_pytorch_profiler: false
profile_ranks: [0]
profile_step_start: 10
profile_step_end: 12
record_memory_history: false
memory_snapshot_path: snapshot.pickle

# ============================================================================
# Inference Configuration (for evaluation)
# ============================================================================
inference_batch_times_seqlen_threshold: -1
inference_dynamic_batching: false
flash_decode: false
enable_cuda_graph: false
cuda_graph_warmup_steps: 3

# ============================================================================
# Other Configurations
# ============================================================================
# Parallel output
parallel_output: false

# Test mode
test_mode: false

# Enable experimental features
enable_experimental: false

# Deterministic mode
deterministic_mode: false

# BERT specific (usually not needed for post-training)
bert_binary_head: false
onnx_safe: null

# ============================================================================
# Notes
# ============================================================================
# This base configuration is designed for post-training tasks including:
# - Supervised Fine-Tuning (SFT)
# - Instruction Tuning
# - LoRA Fine-tuning
# - Dialog Model Training
#
# Key differences from pre-training:
# 1. Lower learning rate (5e-6 vs 2.5e-4)
# 2. Shorter sequences (2048 vs 4096)
# 3. Smaller batches (64 vs 128)
# 4. More frequent checkpointing (500 vs 20000)
# 5. Finetune mode enabled
# 6. Different data split (95/5/0 vs 99/1/0)
#
# Override these values in your specific post-training configuration as needed.
