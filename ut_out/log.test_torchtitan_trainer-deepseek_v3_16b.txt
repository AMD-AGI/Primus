W1031 07:41:11.826000 118817 torch/distributed/run.py:803]
W1031 07:41:11.826000 118817 torch/distributed/run.py:803] *****************************************
W1031 07:41:11.826000 118817 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W1031 07:41:11.826000 118817 torch/distributed/run.py:803] *****************************************
[Primus CLI] HF_HOME already set: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/data/huggingface
[PrimusConfig] Detected unknown override keys: ['model']
[Primus] sys.path.insert: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/third_party/torchtitan
[[32m20251031 07:41:13[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][AMP] nn.Embedding.__init__ patched for AMP/mixed precision alignment.[0m
[[32m20251031 07:41:13[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][Dataset] Patched datasets.load_dataset successfully.[0m
[[32m20251031 07:41:14[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][DCP] Installed fallback for missing torch.distributed.checkpoint._consolidate_hf_safetensors.consolidate_safetensors_files_on_every_rank, HuggingFace safetensors export will be disabled.[0m
[[32m20251031 07:41:14[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][Pipe] Installed fallback: ScheduleDualPipeV -> Schedule1F1B[0m
[[32m20251031 07:41:14[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][FlexAttn] AuxOutput not found. This torch build predates the new debug/profiling return type. Injecting a lightweight stub so Titan model imports can succeed.[0m
[[32m20251031 07:41:14[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][FlexAttn] Injected fallback AuxOutput stub (Titan does not rely on this).[0m
[[32m20251031 07:41:14[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][Checkpoint] checkpoint_wrapper patched successfully[0m
[[32m20251031 07:41:14[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][ModelOverride] Applying model_overrides: {'model': {'n_layers': 4, 'n_dense_layers': 1, 'moe_args': {'use_grouped_mm': False}}}[0m
[[32m20251031 07:41:14[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][ModelOverride] Applying overrides: {'model.n_layers': 4, 'model.n_dense_layers': 1, 'model.moe_args.use_grouped_mm': False}[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][ModelOverride] get_train_spec for 'deepseek_v3' successfully monkey patched (flavor=16B).[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mMokey patch torchtitan logger...[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mLoaded and merged custom JobConfig from primus.backends.torchtitan.primus_turbo_extensions.config_extension[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1m========== TorchTitan Config ==========[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments activation_checkpoint.early_stop.................................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments activation_checkpoint.mode........................................ none[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments activation_checkpoint.per_op_sac_force_recompute_mm_shapes_by_fqns ['moe.router.gate'][0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments activation_checkpoint.selective_ac_option......................... op[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.async_mode............................................. disabled[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.create_seed_checkpoint................................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.enable................................................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.enable_first_step_checkpoint........................... False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.exclude_from_loading................................... [][0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.export_dtype........................................... float32[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.folder................................................. checkpoint[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.initial_load_in_hf..................................... False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.initial_load_model_only................................ True[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.initial_load_path...................................... None[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.interval............................................... 10[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.keep_latest_k.......................................... 10[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.last_save_in_hf........................................ False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.last_save_model_only................................... True[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments checkpoint.load_step.............................................. -1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments comm.init_timeout_seconds......................................... 300[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments comm.save_traces_folder........................................... comm_traces[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments comm.trace_buf_size............................................... 20000[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments comm.train_timeout_seconds........................................ 100[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments compile.components................................................ ['loss'][0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments compile.enable.................................................... True[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments experimental.custom_args_module................................... primus.backends.torchtitan.primus_turbo_extensions.config_extension[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments experimental.custom_import........................................ [0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments fault_tolerance.enable............................................ False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments fault_tolerance.group_size........................................ 0[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments fault_tolerance.min_replica_size.................................. 1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments fault_tolerance.process_group..................................... gloo[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments fault_tolerance.process_group_timeout_ms.......................... 10000[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments fault_tolerance.replica_id........................................ 0[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments fault_tolerance.semi_sync_method.................................. None[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments float8.emulate.................................................... False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments float8.enable_fsdp_float8_all_gather.............................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments float8.filter_fqns................................................ [][0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments float8.moe_fqns_prototype......................................... [][0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments float8.precompute_float8_dynamic_scale_for_fsdp................... False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments float8.recipe_name................................................ None[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments job.config_file................................................... None[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments job.description................................................... DeepSeek-V3 16B model training[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments job.dump_folder................................................... ./outputs[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments job.print_args.................................................... False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments lr_scheduler.decay_ratio.......................................... 0.8[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments lr_scheduler.decay_type........................................... cosine[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments lr_scheduler.min_lr_factor........................................ 0.1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments lr_scheduler.warmup_steps......................................... 200[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments memory_estimation.disable_fake_mode............................... False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments memory_estimation.enable.......................................... False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments metrics.disable_color_printing.................................... False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments metrics.enable_tensorboard........................................ False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments metrics.enable_wandb.............................................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments metrics.log_freq.................................................. 10[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments metrics.save_for_all_ranks........................................ False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments metrics.save_tb_folder............................................ tb[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments model.converters.................................................. [][0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments model.flavor...................................................... 16B[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments model.hf_assets_path.............................................. /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/data/torchtitan/deepseek-moe-16b-base[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments model.name........................................................ deepseek_v3[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments model.print_after_conversion...................................... False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments model.tokenizer_path.............................................. None[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments mx.filter_fqns.................................................... ['output'][0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments mx.moe_fqns_prototype............................................. [][0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments mx.mxfp8_dim1_cast_kernel_choice.................................. triton[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments mx.recipe_name.................................................... mxfp8_cublas[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments optimizer.beta1................................................... 0.9[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments optimizer.beta2................................................... 0.95[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments optimizer.early_step_in_backward.................................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments optimizer.eps..................................................... 1e-08[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments optimizer.implementation.......................................... fused[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments optimizer.lr...................................................... 0.00022[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments optimizer.name.................................................... AdamW[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments optimizer.weight_decay............................................ 0.1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.context_parallel_degree............................... 1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.context_parallel_rotate_method........................ allgather[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.data_parallel_replicate_degree........................ 1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.data_parallel_shard_degree............................ -1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.disable_loss_parallel................................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.enable_async_tensor_parallel.......................... False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.enable_compiled_autograd.............................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.expert_parallel_degree................................ 8[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.expert_tensor_parallel_degree......................... 1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.fsdp_reshard_after_forward............................ default[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.module_fqns_per_model_part............................ None[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.pipeline_parallel_degree.............................. 1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.pipeline_parallel_first_stage_less_layers............. 1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.pipeline_parallel_last_stage_less_layers.............. 1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.pipeline_parallel_layers_per_stage.................... None[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.pipeline_parallel_microbatch_size..................... 1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.pipeline_parallel_schedule............................ Interleaved1F1B[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.pipeline_parallel_schedule_csv........................ [0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.pipeline_parallel_split_points........................ [][0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments parallelism.tensor_parallel_degree................................ 1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments primus_turbo.enable_attention_float8.............................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments primus_turbo.enable_embedding_autocast............................ True[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments primus_turbo.enable_primus_turbo.................................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments primus_turbo.use_turbo_async_tp................................... True[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments primus_turbo.use_turbo_attention.................................. True[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments primus_turbo.use_turbo_mx_linear.................................. True[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments profiling.enable_memory_snapshot.................................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments profiling.enable_profiling........................................ False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments profiling.profile_freq............................................ 10[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments profiling.save_memory_snapshot_folder............................. memory_snapshot[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments profiling.save_traces_folder...................................... profile_trace[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.dataset.................................................. c4[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.dataset_path............................................. None[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.deterministic............................................ False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.enable_cpu_offload....................................... False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.gc_debug................................................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.gc_freq.................................................. 50[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.global_batch_size........................................ -1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.local_batch_size......................................... 4[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.max_norm................................................. 1.0[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.mixed_precision_param.................................... bfloat16[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.mixed_precision_reduce................................... float32[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.seed..................................................... None[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.seq_len.................................................. 4096[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments training.steps.................................................... 3[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments validation.dataset................................................ c4_validation[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments validation.dataset_path........................................... None[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments validation.enable................................................. False[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments validation.freq................................................... 10[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments validation.local_batch_size....................................... 8[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments validation.seq_len................................................ 2048[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1marguments validation.steps.................................................. -1[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mStarting job: DeepSeek-V3 16B model training[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1mENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config[0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mBuilding 2-D device mesh with ['dp_shard_mod_ep', 'dp_shard_in_ep'], [1, 8][0m
[[32m20251031 07:41:16[0m][[36mrank-1/8[0m][[1mINFO [0m] [1m[GC] Initial GC collection 0.00 seconds[0m
[[32m20251031 07:41:19[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][ModelOverride] Successfully patched model_args['16B'] for 'deepseek_v3' with {'model.n_layers': 4, 'model.n_dense_layers': 1, 'model.moe_args.use_grouped_mm': False}. Diff(beforeâ†’after): {'_enforced': 'This field is used to enforce all fields have defaults.', 'max_batch_size': 8, 'max_seq_len': 16384, 'vocab_size': 102400, 'dim': 2048, 'inter_dim': 10944, 'moe_inter_dim': 1408, 'n_layers': 27, 'n_dense_layers': 1, 'n_heads': 16, 'norm_eps': 1e-05, 'moe_args': {'num_experts': 64, 'num_shared_experts': 2, 'score_func': 'softmax', 'route_norm': True, 'route_scale': 1.0, 'score_before_experts': False, 'top_k': 6, 'use_grouped_mm': True, 'load_balance_coeff': 0.001}, 'n_expert_groups': 1, 'n_limited_groups': 1, 'q_lora_rank': 0, 'kv_lora_rank': 512, 'qk_nope_head_dim': 128, 'qk_rope_head_dim': 64, 'v_head_dim': 128, 'use_flex_attn': True, 'attn_mask_type': 'block_causal', 'original_seq_len': 4096, 'rope_theta': 10000.0, 'rope_factor': 40, 'beta_fast': 32, 'beta_slow': 1, 'mscale': 0.7} â†’ {'_enforced': 'This field is used to enforce all fields have defaults.', 'max_batch_size': 8, 'max_seq_len': 16384, 'vocab_size': 102400, 'dim': 2048, 'inter_dim': 10944, 'moe_inter_dim': 1408, 'n_layers': 4, 'n_dense_layers': 1, 'n_heads': 16, 'norm_eps': 1e-05, 'moe_args': {'num_experts': 64, 'num_shared_experts': 2, 'score_func': 'softmax', 'route_norm': True, 'route_scale': 1.0, 'score_before_experts': False, 'top_k': 6, 'use_grouped_mm': False, 'load_balance_coeff': 0.001}, 'n_expert_groups': 1, 'n_limited_groups': 1, 'q_lora_rank': 0, 'kv_lora_rank': 512, 'qk_nope_head_dim': 128, 'qk_rope_head_dim': 64, 'v_head_dim': 128, 'use_flex_attn': True, 'attn_mask_type': 'block_causal', 'original_seq_len': 4096, 'rope_theta': 10000.0, 'rope_factor': 40, 'beta_fast': 32, 'beta_slow': 1, 'mscale': 0.7}[0m
[[32m20251031 07:41:19[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mLoading tokenizer from tokenizer.json[0m
[[32m20251031 07:41:19[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mPreparing c4 dataset from allenai/c4[0m
[[32m20251031 07:41:19[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1m[PrimusPatch][MockDataset] load_dataset('allenai/c4') is mocked.[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mBuilding deepseek_v3 16B with DeepSeekV3ModelArgs(_enforced='This field is used to enforce all fields have defaults.', max_batch_size=8, max_seq_len=4096, vocab_size=102400, dim=2048, inter_dim=10944, moe_inter_dim=1408, n_layers=4, n_dense_layers=1, n_heads=16, norm_eps=1e-05, moe_args=MoEArgs(num_experts=64, num_shared_experts=2, score_func='softmax', route_norm=True, route_scale=1.0, score_before_experts=False, top_k=6, use_grouped_mm=False, load_balance_coeff=0.001), n_expert_groups=1, n_limited_groups=1, q_lora_rank=0, kv_lora_rank=512, qk_nope_head_dim=128, qk_rope_head_dim=64, v_head_dim=128, use_flex_attn=True, attn_mask_type='block_causal', original_seq_len=4096, rope_theta=10000.0, rope_factor=40, beta_fast=32, beta_slow=1, mscale=0.7)[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mCUDA capacity: AMD Instinct MI300X with 191.98GiB memory[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mTotal parameter count: dense 541,741,056, sparse 1,713,242,112, active 749,752,320[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[1mINFO [0m] [1m[34mModel deepseek_v3 16B [31msize: 2,254,983,168 total parameters[39m[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mCompiling the loss function with torch.compile[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mApplied FSDP to the model[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mPeak FLOPS used for computing MFU: 1.300e+15[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mCUDA memory usage for model: 1.06GiB(0.55%)[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1mWarmup steps (200) exceed total training steps (3). Adjusting warmup steps to 3.[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1mWarmup (3) + decay (2) steps exceed total training steps (3). Adjusting decay steps to 0.[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[33m[1mWARNING[0m] [33m[1mmodel.safetensors.index.json not found at hf_assets_path: /shared/amdgpu/home/xiaoming_peng_qle/workspace/dev/Primus-CLI/data/torchtitan/deepseek-moe-16b-base/model.safetensors.index.json.                     Defaulting to saving a single safetensors file if checkpoint is saved in HF format[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mMixed precision training is handled by fully_shard[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mTrainer is initialized with local batch size 4, global batch size 32, gradient accumulation steps 1, sequence length 4096, total steps 3 (warmup 200)[0m
[[32m20251031 07:41:20[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mTraining starts at step 1[0m
/opt/venv/lib/python3.10/site-packages/torch/__init__.py:1539: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  return _C._get_float32_matmul_precision()
Autotune Choices Stats:
{"num_choices": 24, "num_triton_choices": 24, "best_kernel": "triton_flex_attention_22", "best_kernel_desc": "BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=128, FLOAT32_PRECISION=\"'ieee'\", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, USE_TMA=False, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4", "best_time": 5.346622943878174, "best_triton_pos": 0}
AUTOTUNE flex_attention(4x16x4096x192, 4x16x4096x192, 4x16x4096x128, 4x16x4096, 4x1x32, 4x1x32x32, 4x1x32, 4x1x32x32, 4x4096)
strides: [12582912, 192, 3072, 1], [12582912, 192, 3072, 1], [16777216, 256, 4096, 1], [65536, 4096, 1], [32, 32, 1], [1024, 1024, 32, 1], [32, 32, 1], [1024, 1024, 32, 1], [4096, 1]
dtypes: torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.float32, torch.int32, torch.int32, torch.int32, torch.int32, torch.int32
  triton_flex_attention_22 5.3466 ms 100.0% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=128, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, USE_TMA=False, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_20 5.5979 ms 95.5% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=64, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, USE_TMA=False, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_18 6.0738 ms 88.0% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=32, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, USE_TMA=False, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_17 7.1755 ms 74.5% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=16, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, USE_TMA=False, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=8
  triton_flex_attention_8 7.6801 ms 69.6% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=64, BLOCK_N=16, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, USE_TMA=False, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_19 8.2491 ms 64.8% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=32, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, USE_TMA=False, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=8
  triton_flex_attention_21 8.5263 ms 62.7% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=64, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, USE_TMA=False, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=8
  triton_flex_attention_16 8.7133 ms 61.4% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=16, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, USE_TMA=False, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_9 9.5571 ms 55.9% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=64, BLOCK_N=16, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, USE_TMA=False, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=8
  triton_flex_attention_12 9.7610 ms 54.8% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=64, BLOCK_N=64, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, USE_TMA=False, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 2.9715 seconds and 1.6575 seconds precompiling for 24 choices
Autotune Choices Stats:
{"num_choices": 22, "num_triton_choices": 22, "best_kernel": "triton_flex_attention_backward_37", "best_kernel_desc": "BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=32, BLOCK_M2=128, BLOCK_N1=128, BLOCK_N2=32, FLOAT32_PRECISION=\"'ieee'\", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=1, num_warps=4", "best_time": 24.930908203125, "best_triton_pos": 0}
AUTOTUNE flex_attention_backward(4x16x4096x192, 4x16x4096x192, 4x16x4096x128, 4x16x4096, 4x16x4096, 4x16x4096x128, 4x16x4096x192, 4x16x4096x128, 4x1x32, 4x1x32x32, 4x1x32, 4x1x32x32, 4x1x32, 4x1x32x32, 4x1x32, 4x1x32x32, 4x4096)
strides: [12582912, 192, 3072, 1], [12582912, 192, 3072, 1], [16777216, 256, 4096, 1], [65536, 4096, 1], [65536, 4096, 1], [8388608, 524288, 128, 1], [12582912, 192, 3072, 1], [8388608, 128, 2048, 1], [32, 32, 1], [1024, 1024, 32, 1], [32, 32, 1], [1024, 1024, 32, 1], [32, 32, 1], [1024, 1024, 32, 1], [32, 32, 1], [1024, 1024, 32, 1], [4096, 1]
dtypes: torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.float32, torch.float32, torch.bfloat16, torch.bfloat16, torch.bfloat16, torch.int32, torch.int32, torch.int32, torch.int32, torch.int32, torch.int32, torch.int32, torch.int32, torch.int32
  triton_flex_attention_backward_37 24.9309 ms 100.0% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=32, BLOCK_M2=128, BLOCK_N1=128, BLOCK_N2=32, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_backward_36 25.3382 ms 98.4% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=32, BLOCK_M2=128, BLOCK_N1=128, BLOCK_N2=32, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_backward_28 26.2709 ms 94.9% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=16, BLOCK_M2=128, BLOCK_N1=128, BLOCK_N2=16, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_backward_30 29.2424 ms 85.3% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=16, BLOCK_M2=128, BLOCK_N1=128, BLOCK_N2=16, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=8
  triton_flex_attention_backward_35 30.6698 ms 81.3% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=32, BLOCK_M2=64, BLOCK_N1=64, BLOCK_N2=32, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_backward_34 33.9983 ms 73.3% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=32, BLOCK_M2=64, BLOCK_N1=64, BLOCK_N2=32, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_backward_39 38.6096 ms 64.6% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=32, BLOCK_M2=128, BLOCK_N1=128, BLOCK_N2=32, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=1, num_warps=8
  triton_flex_attention_backward_42 40.7584 ms 61.2% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=64, BLOCK_M2=128, BLOCK_N1=128, BLOCK_N2=64, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_backward_26 40.8196 ms 61.1% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=16, BLOCK_M2=64, BLOCK_N1=64, BLOCK_N2=16, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=0, waves_per_eu=0, num_stages=1, num_warps=4
  triton_flex_attention_backward_29 44.5512 ms 56.0% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M1=16, BLOCK_M2=128, BLOCK_N1=128, BLOCK_N2=16, FLOAT32_PRECISION="'ieee'", GQA_SHARED_HEADS=1, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=True, OUTPUT_LOGSUMEXP=True, PRESCALE_QK=False, QK_HEAD_DIM=192, QK_HEAD_DIM_ROUNDED=256, ROWS_GUARANTEED_SAFE=False, SAFE_HEAD_DIM=False, SM_SCALE=0.07216878364870322, SPARSE_KV_BLOCK_SIZE=128, SPARSE_Q_BLOCK_SIZE=128, V_HEAD_DIM=128, V_HEAD_DIM_ROUNDED=128, WRITE_DQ=True, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=1, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.1231 seconds and 3.6594 seconds precompiling for 22 choices
[[32m20251031 07:42:01[0m][[36mrank-1/8[0m][[1mINFO [0m] [1m[31mstep:  1  [32mloss: 12.0594  [38;2;180;60;0mgrad_norm: 10.4156  [38;2;54;234;195mmemory: 23.66GiB(12.32%)  [34mtps: 398  [36mtflops: 1.45  [35mmfu: 0.11%[39m[0m
[[32m20251031 07:42:01[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mSynchronizing and adjusting timeout for all ProcessGroups to 0:01:40[0m
[[32m20251031 07:42:01[0m][[36mrank-1/8[0m][[1mINFO [0m] [1mTraining completed[0m
[rank1]:[W1031 07:42:03.223850668 ProcessGroupNCCL.cpp:1522] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
