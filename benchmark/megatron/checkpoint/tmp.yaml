exp_name: exp_pretrain
modules:
  pre_trainer:
    config: pre_trainer.yaml
    framework: megatron
    model: mixtral_8x7B_v0.1.yaml
    overrides:
      adam_beta1: 0.9
      adam_beta2: 0.95
      auto_continue_train: true
      ckpt_format: torch
      disable_last_saving: true
      eod_mask_loss: true
      expert_model_parallel_size: 8
      finetune: false
      global_batch_size: 128
      gradient_accumulation_fusion: false
      init_method_std: 0.008
      load: null
      log_avg_reset_interval: 5
      log_avg_skip_iterations: 2
      lr: 1.0e-05
      lr_decay_iters: null
      lr_decay_style: cosine
      lr_warmup_iters: 2
      max_position_embeddings: 4096
      micro_batch_size: 2
      min_lr: 0.0
      moe_permute_fusion: false
      moe_router_dtype: fp32
      moe_router_force_load_balancing: true
      moe_use_legacy_grouped_gemm: true
      no_load_optim: null
      no_load_rng: null
      no_save_optim: null
      no_save_rng: null
      norm_epsilon: 1.0e-06
      overlap_grad_reduce: true
      overlap_param_gather: true
      pipeline_model_parallel_size: 1
      save: null
      save_interval: 100
      seq_length: 4096
      stderr_sink_level: DEBUG
      tensor_model_parallel_size: 1
      test_data_path: null
      train_data_path: null
      train_iters: 3
      valid_data_path: null
      wandb_project: Primus_DeepSeek_Pretrain
      weight_decay: 0.1
platform:
  config: platform_azure.yaml
  overrides:
    master_sink_level: INFO
user_name: limou102
work_group: amd
workspace: ./output
