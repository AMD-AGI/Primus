diff --git a/src/megatron/bridge/peft/lora.py b/src/megatron/bridge/peft/lora.py
index b1f8b55d..704684bc 100644
--- a/src/megatron/bridge/peft/lora.py
+++ b/src/megatron/bridge/peft/lora.py
@@ -80,15 +80,15 @@ class LoRA(PEFT, ModuleMatcher):
     """
 
     target_modules: List[str] = field(
-        default_factory=lambda: ["linear_qkv", "linear_proj", "linear_fc1", "linear_fc2"]
+        default_factory=lambda: ["linear_qkv", "linear_proj"]
     )
-    dim: int = 32
+    dim: int = 16
     alpha: int = 32
-    dropout: float = 0.0
+    dropout: float = 0.1
     dropout_position: Literal["pre", "post"] = "pre"
-    lora_A_init_method: str = "xavier"
+    lora_A_init_method: str = "kaiming"
     lora_B_init_method: str = "zero"
-    a2a_experimental: bool = False
+    a2a_experimental: bool = True
     lora_dtype: torch.dtype = None
 
     def transform(self, module: nn.Module, name: Optional[str] = None, prefix: Optional[str] = None) -> nn.Module:
diff --git a/src/megatron/bridge/recipes/llama/llama2.py b/src/megatron/bridge/recipes/llama/llama2.py
index 99d1b6c6..3834c9b5 100644
--- a/src/megatron/bridge/recipes/llama/llama2.py
+++ b/src/megatron/bridge/recipes/llama/llama2.py
@@ -19,7 +19,10 @@ import torch
 from typing_extensions import TypedDict, Unpack
 
 from megatron.bridge import AutoBridge
+from megatron.bridge.data.datasets.packed_sequence import PackedSequenceSpecs
+from megatron.bridge.peft.base import PEFT
 from megatron.bridge.recipes.utils.dataset_utils import get_blend_fields_from_data_paths
+from megatron.bridge.recipes.utils.finetune_utils import default_peft_config
 from megatron.bridge.recipes.utils.optimizer_utils import distributed_fused_adam_with_cosine_annealing
 from megatron.bridge.recipes.utils.tokenizer_utils import DEFAULT_NULL_TOKENIZER_VOCAB_SIZE
 from megatron.bridge.training.comm_overlap import CommOverlapConfig
@@ -27,7 +30,7 @@ from megatron.bridge.training.config import (
     CheckpointConfig,
     ConfigContainer,
     DistributedDataParallelConfig,
-    GPTDatasetConfig,
+    FinetuningDatasetConfig,
     LoggerConfig,
     RNGConfig,
     TokenizerConfig,
@@ -74,6 +77,17 @@ class Llama2CommonKwargs(TypedDict, total=False):
     # Precision / overlap configs
     precision_config: Optional[Union[MixedPrecisionConfig, str]]
     comm_overlap_config: Optional[CommOverlapConfig]
+    adam_beta1: float = 0.9
+    adam_beta2: float = 0.99
+    adam_eps: float = 1e-8
+    weight_decay: float = 0.0001
+    eval_iters: int = 32
+    pretrained_checkpoint: str | None
+    peft: str | PEFT | None
+    packed_sequence: bool
+    packed_train_data_path: str | None
+    packed_val_data_path: str | None
+    packed_metadata_path: str | None
 
 
 def llama2_7b_pretrain_config(**user_kwargs: Unpack[Llama2CommonKwargs]) -> ConfigContainer:
@@ -81,6 +95,7 @@ def llama2_7b_pretrain_config(**user_kwargs: Unpack[Llama2CommonKwargs]) -> Conf
 
     See `_llama2_common` for the full list of parameters.
     """
+    peft_value = user_kwargs.get("peft", "lora")
     recommended_kwargs: Llama2CommonKwargs = {
         "hf_path": "meta-llama/Llama-2-7b-hf",
         "tensor_model_parallel_size": 2,
@@ -91,6 +106,39 @@ def llama2_7b_pretrain_config(**user_kwargs: Unpack[Llama2CommonKwargs]) -> Conf
         "lr_warmup_iters": 2000,
         "eval_interval": 2000,
         "save_interval": 2000,
+        "adam_beta1": 0.9,
+        "adam_beta2": 0.99,
+        "adam_eps": 1e-8,
+        "weight_decay": 0.0001,
+        "eval_iters": 32,
+        "peft": peft_value,
+    }
+    # Combine defaults with user kwargs; user values take precedence.
+    combined_kwargs: Llama2CommonKwargs = {**recommended_kwargs, **user_kwargs}
+    return _llama2_common(**combined_kwargs)
+
+def llama2_70b_pretrain_config(**user_kwargs: Unpack[Llama2CommonKwargs]) -> ConfigContainer:
+    """Return a pre-training config for Llama-2 7B.
+
+    See `_llama2_common` for the full list of parameters.
+    """
+    peft_value = user_kwargs.get("peft", "lora")
+    recommended_kwargs: Llama2CommonKwargs = {
+        "hf_path": "meta-llama/Llama-2-7b-hf",
+        "tensor_model_parallel_size": 2,
+        "pipeline_model_parallel_size": 1,
+        "train_iters": 1_168_251,
+        "global_batch_size": 512,
+        "micro_batch_size": 1,
+        "lr_warmup_iters": 2000,
+        "eval_interval": 2000,
+        "save_interval": 2000,
+        "adam_beta1": 0.9,
+        "adam_beta2": 0.99,
+        "adam_eps": 1e-8,
+        "weight_decay": 0.0001,
+        "eval_iters": 32,
+        "peft": peft_value,
     }
     # Combine defaults with user kwargs; user values take precedence.
     combined_kwargs: Llama2CommonKwargs = {**recommended_kwargs, **user_kwargs}
@@ -128,7 +176,18 @@ def _llama2_common(
     lr_decay_iters: Optional[int] = None,
     eval_interval: int = 2000,
     save_interval: int = 2000,
-    use_null_tokenizer: bool = True,
+    use_null_tokenizer: bool = False,
+    pretrained_checkpoint: str | None = None,
+    peft: str | PEFT | None = "lora",
+    packed_sequence: bool = False,
+    packed_train_data_path: str | None = None,
+    packed_val_data_path: str | None = None,
+    packed_metadata_path: str | None = None,
+    adam_beta1: float = 0.9,
+    adam_beta2: float = 0.95,
+    adam_eps: float = 1e-5,
+    weight_decay: float = 0.1,
+    eval_iters: int = 32,
     # Precision recipe
     precision_config: Optional[Union[MixedPrecisionConfig, str]] = "bf16_mixed",
     comm_overlap_config: Optional[CommOverlapConfig] = None,
@@ -166,6 +225,11 @@ def _llama2_common(
         save_interval (int): Save interval.
         precision_config (Optional[Union[MixedPrecisionConfig, str]]): Precision configuration for the model.
         comm_overlap_config (Optional[CommOverlapConfig]): Communication overlap configuration for the model.
+        adam_beta1 (float): Beta1 parameter for Adam optimizer.
+        adam_beta2 (float): Beta2 parameter for Adam optimizer.
+        adam_eps (float): Epsilon parameter for Adam optimizer.
+        weight_decay (float): Weight decay parameter for Adam optimizer.
+        eval_iters (int): Number of iterations to run for evaluation validation/test for.
 
     Returns:
         ConfigContainer: Configuration for pre-training.
@@ -175,10 +239,6 @@ def _llama2_common(
     checkpoint_dir = os.path.join(run_output_dir, "checkpoints")
     tensorboard_dir = os.path.join(run_output_dir, "tb_logs")
 
-    blend, blend_per_split, split = get_blend_fields_from_data_paths(
-        data_paths, data_args_path, train_data_path, valid_data_path, test_data_path, per_split_data_args_path, mock
-    )
-
     bridge = AutoBridge.from_hf_pretrained(hf_path)
     model_cfg = bridge.to_megatron_provider(load_weights=False)
     model_cfg.tensor_model_parallel_size = tensor_model_parallel_size
@@ -192,21 +252,36 @@ def _llama2_common(
     opt_config, scheduler = distributed_fused_adam_with_cosine_annealing(
         lr_warmup_iters=lr_warmup_iters,
         lr_decay_iters=lr_decay_iters,
-        adam_beta1=0.9,
-        adam_beta2=0.95,
-        adam_eps=1e-5,
-        weight_decay=0.1,
+        adam_beta1=adam_beta1,
+        adam_beta2=adam_beta2,
+        adam_eps=adam_eps,
+        weight_decay=weight_decay,
         max_lr=lr,
         min_lr=min_lr,
     )
 
+    # PEFT config
+    peft_config = default_peft_config(peft)
+
+    # Packed sequence configuration
+    if packed_sequence:
+        packed_sequence_specs = PackedSequenceSpecs(
+            packed_sequence_size=seq_length,  # Must be > 0 to use packed files
+            tokenizer_model_name=hf_path,
+            packed_train_data_path=packed_train_data_path or "/data/train_packed.npy",
+            packed_val_data_path=packed_val_data_path or "/data/validation_packed.npy",
+            packed_metadata_path=packed_metadata_path or "/data/packed_metadata.jsonl",  # Metadata for packed sequences
+        )
+    else:
+        packed_sequence_specs = None
+
     # Config Container
     cfg = ConfigContainer(
         model=model_cfg,
         train=TrainingConfig(
             train_iters=train_iters,
             eval_interval=eval_interval,
-            eval_iters=32,
+            eval_iters=eval_iters,
             global_batch_size=global_batch_size,
             micro_batch_size=micro_batch_size,
             manual_gc=True,
@@ -224,22 +299,17 @@ def _llama2_common(
             use_distributed_optimizer=True,
             use_megatron_fsdp=use_megatron_fsdp,  # need use_distributed_optimizer=True
         ),
-        dataset=GPTDatasetConfig(
-            random_seed=1234,
-            reset_attention_mask=False,
-            reset_position_ids=False,
-            eod_mask_loss=False,
+        dataset=FinetuningDatasetConfig(
+            dataset_root="/data",  # Point to your .npy files directory
             seq_length=seq_length,
-            num_dataset_builder_threads=1,
-            blend=blend,
-            blend_per_split=blend_per_split,
-            split=split,
+            seed=1234,
+            packed_sequence_specs=packed_sequence_specs,
             # Dataloader config parameters
             data_sharding=True,
-            dataloader_type="single",
-            num_workers=8,
-            skip_getting_attention_mask_from_dataset=True,
+            dataloader_type="batch",  # "batch" is recommended for fine-tuning
+            num_workers=1,
         ),
+
         logger=LoggerConfig(
             log_interval=10,
             tensorboard_dir=tensorboard_dir,
@@ -251,14 +321,16 @@ def _llama2_common(
         ),
         checkpoint=CheckpointConfig(
             save_interval=save_interval,
-            save=checkpoint_dir,
-            load=checkpoint_dir,
+            save=None,
+            load=None,
+            pretrained_checkpoint=pretrained_checkpoint,
             ckpt_format="torch_dist",
             fully_parallel_save=True,
         ),
         rng=RNGConfig(seed=1234),
         comm_overlap=comm_overlap_config,
         mixed_precision=precision_config,
+        peft=peft_config,
     )
 
     if cfg.comm_overlap is None:
