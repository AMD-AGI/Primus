###############################################################################
# Copyright (c) 2025, Advanced Micro Devices, Inc.
# All rights reserved.
#
# Licensed under the MIT License. See LICENSE file in the project root for details.
###############################################################################
"""
Unit tests for TorchTitan model integration in Primus.
"""

import argparse
import os
import pytest
from pathlib import Path
# from primus.config import PrimusConfig
from primus.core.launcher.parser import load_primus_config
from primus.core.launcher.config import PrimusConfig
from primus.core.launcher.parser import add_pretrain_parser
from primus.pretrain import launch_pretrain_from_cli, launch_pretrain_trainer

CONFIG_ROOT = Path("examples/torchtitan/configs")
CONFIG_FILES = sorted(CONFIG_ROOT.rglob("*.yaml"))

def _make_args(config_path: str, backend_path: str = None):
    """Simulate CLI parser for pretrain mode."""
    parser = argparse.ArgumentParser()
    add_pretrain_parser(parser)
    cli_args = ["--config", config_path]
    if backend_path:
        cli_args += ["--backend_path", backend_path]
    return parser.parse_args(cli_args)

@pytest.mark.parametrize(
    "config_file",
    CONFIG_FILES,
    ids=[str(p.relative_to("examples/torchtitan/configs")) for p in CONFIG_FILES],
)
def test_torchtitan_model_config_load(config_file):
    """Ensure TorchTitan model configs can be loaded into PrimusConfig."""
    cfg_path = Path(config_file)
    assert cfg_path.exists(), f"Config file not found: {cfg_path}"

    args = _make_args(str(cfg_path))
    primus_cfg = load_primus_config(args, overrides=[])

    assert isinstance(primus_cfg, PrimusConfig)

@pytest.mark.parametrize(
    "config_file",
    CONFIG_FILES,
    ids=[str(p.relative_to("examples/torchtitan/configs")) for p in CONFIG_FILES],
)
def test_torchtitan_patch_args(tmp_path, config_file):
    """Verify patch args are generated and include backend_path for all configs."""
    patch_args_file = tmp_path / f"{Path(config_file).stem}_patch.yaml"
    backend_path = str(Path("third_party/torchtitan").absolute())
    os.environ["TORCHTITAN_PATH"] = backend_path

    os.environ.setdefault("NNODES", "1")
    os.environ.setdefault("NODE_RANK", "0")
    os.environ.setdefault("GPUS_PER_NODE", "1")
    os.environ.setdefault("MASTER_ADDR", "localhost")
    os.environ.setdefault("MASTER_PORT", "29500")
    os.environ.setdefault("RANK", "0")
    os.environ.setdefault("LOCAL_RANK", "0")

    args = _make_args(str(config_file), backend_path)

    launch_pretrain_from_cli(args, overrides=[])
    # primus_cfg = load_primus_config(args, overrides=[])

    # launch_pretrain_trainer(primus_cfg)

    # assert patch_args_file.exists(), f"Patch args file not generated for {config_file}"
    # content = patch_args_file.read_text()
    # assert "backend_path" in content, f"Expected backend_path missing in patch args for {config_file}"



# @pytest.mark.parametrize("model_cfg", TORCHTITAN_MODEL_CONFIGS)
# def test_torchtitan_patch_args(tmp_path, model_cfg):
#     """Verify TorchTitan backend patch args file is generated and contains backend_path."""
#     primus_cfg = PrimusConfig.load(model_cfg)
#     patch_args_file = tmp_path / "patch_args.yaml"
#     os.environ["TORCHTITAN_PATH"] = str(Path("third_party/torchtitan").absolute())

#     launch_pretrain_from_cli(
#         args=None,
#         overrides={
#             "exp.config": str(model_cfg),
#             "patch_args_file": str(patch_args_file),
#         },
#     )

#     assert patch_args_file.exists(), "Patch args file not generated"
#     content = patch_args_file.read_text()
#     assert "backend_path" in content, "Expected backend_path missing in patch args"


# @pytest.mark.parametrize("dtype", [torch.float16, torch.bfloat16])
# def test_torchtitan_forward_smoke(dtype):
#     """Run a minimal forward pass with dummy model to ensure Titan backend import works."""
#     try:
#         import torchtitan  # noqa: F401
#     except ImportError:
#         pytest.skip("TorchTitan not available in current environment")

#     from torchtitan.models.llama import build_model

#     device = "cuda" if torch.cuda.is_available() else "cpu"
#     model = build_model(hidden_size=64, num_layers=2, vocab_size=1000).to(dtype=dtype, device=device)
#     x = torch.randint(0, 1000, (2, 16), device=device)
#     with torch.no_grad():
#         y = model(x)
#     assert y is not None
#     assert isinstance(y, torch.Tensor)
#     assert y.shape[0] == 2
