includes:
  - ../module_base.yaml

trainable: true

# logging
disable_tensorboard: true
disable_wandb: true
disable_compile_dependencies: true

# finetune
auto_continue_train: true
disable_last_saving: false

# megatron arguments include
# training, optimzer, checkpoint, loss, distributed, recompute, data, profile, logging
# training
yaml_cfg: null # not support
spec: null
micro_batch_size: 2
batch_size: null # deprecated
global_batch_size: 128
rampup_batch_size: null
check_for_nan_in_loss_and_grad: true
num_layers_per_virtual_pipeline_stage: null

encoder_num_layers: null
decoder_num_layers: null
make_vocab_size_divisible_by: 128

exit_signal_handler: false
exit_duration_in_mins: null
exit_interval: null

onnx_safe: null
bert_binary_head: true

use_flash_attn: false
seed: 1234
data_parallel_random_init: false
init_method_xavier_uniform: false
test_mode: false

# Optimizer
optimizer: adam
lr: 2.5e-4
lr_decay_style: cosine
lr_decay_iters: null
lr_decay_samples: null
lr_warmup_fraction: null
lr_warmup_iters: 0
lr_warmup_samples: 0
lr_warmup_init: 0.0
min_lr: 2.5e-5
lr_wsd_decay_style: exponential
lr_wsd_decay_samples: null
lr_wsd_decay_iters: null
head_lr_mult: 1.0
weight_decay: 0.01
start_weight_decay: null
end_weight_decay: null
weight_decay_incr_style: constant
clip_grad: 1.0
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1.0e-08
sgd_momentum: 0.9
override_opt_param_scheduler: false
use_checkpoint_opt_param_scheduler: false
warmup: null
decoupled_lr: null
decoupled_min_lr: null

# checkpointing arguments
save: null
save_interval: 20000
no_save_optim: null
no_save_rng: null
load: null
no_load_optim: null
no_load_rng: null
finetune: false
use_checkpoint_args: false
exit_on_missing_checkpoint: true

pretrained_checkpoint: null
ckpt_step: null
use_dist_ckpt: false
auto_detect_ckpt_format: false
dist_ckpt_format: torch_dist
ckpt_fully_parallel_save_deprecated: false
ckpt_fully_parallel_save: true
async_save: null
ckpt_fully_parallel_load: false
ckpt_assume_constant_structure: false
dist_ckpt_strictness: assume_ok_unexpected

# loss arguments
calculate_per_token_loss: false
loss_scale: null
initial_loss_scale: 4294967296
min_loss_scale: 1.0
loss_scale_window: 1000
hysteresis: 2
accumulate_allreduce_grads_in_fp32: false
fp16_lm_cross_entropy: false

# distributed arguments
distributed_backend: nccl
distributed_timeout_minutes: 10
overlap_grad_reduce: false
delay_grad_reduce: true
overlap_param_gather: false
delay_param_gather: false
scatter_gather_tensors_in_pipeline: true
local_rank: null
lazy_mpu_init: null
empty_unused_memory_level: 0
standalone_embedding_stage: false
use_distributed_optimizer: false
nccl_communicator_config_path: null
deterministic_mode: false
check_weight_hash_across_dp_replicas_interval: null

defer_embedding_wgrad_compute: false
wgrad_deferral_limit: 0
ddp_bucket_size: null
ddp_average_in_collective: false

train_iters: null
eval_iters: 32
eval_interval: 2000
skip_train: false

adlr_autoresume: false
adlr_autoresume_interval: 1000

# activation recomputation
recompute_activations: false
recompute_granularity: null
recompute_method: null
recompute_num_layers: null
distribute_saved_activations: false
checkpoint_activations: false # deprecated

# garbage collection
manual_gc: false
manual_gc_interval: 0
manual_gc_eval: true

#data
data_path: null
data_sharding: true
split: "99,1,0"
train_data_path: null
valid_data_path: null
test_data_path: null
data_cache_path: null
mock_data: false
# Note(jingjie): vocab size is set in tokenizer yaml config
vocab_size: null
vocab_file: null
merge_file: null
vocab_extra_ids: 0
seq_length: 4096
encoder_seq_length: null
decoder_seq_length: null
retriever_seq_length: 256
sample_rate: 1.0
mask_prob: 0.15
short_seq_prob: 0.1
num_workers: 8
reset_position_ids: false
reset_attention_mask: false
eod_mask_loss: false
train_samples: null
dataloader_type: null
mmap_bin_files: true

#profile:
profile: false
profile_ranks: [0]
profile_step_end: 12
profile_step_start: 10

#logging:
log_params_norm: false
log_num_zeros_in_grad: false
log_throughput: false
log_progress: false
timing_log_level: 0
timing_log_option: minmax
tensorboard_log_interval: 1
tensorboard_queue_size: 1000
log_timers_to_tensorboard: false
log_batch_size_to_tensorboard: false
log_learning_rate_to_tensorboard: true
log_validation_ppl_to_tensorboard: false
log_memory_to_tensorboard: false
log_world_size_to_tensorboard: false
log_loss_scale_to_tensorboard: true
wandb_project: null
wandb_exp_name: null
wandb_save_dir: null
wandb_entity: null
enable_one_logger: true
one_logger_project: megatron-lm
one_logger_run_name: null
log_interval: 100
tensorboard_dir: null

one_logger_async: false
app_tag_run_name: null
app_tag_run_version: 0.0.0

# vision
vision_pretraining: false
vision_pretraining_type: classify
vision_backbone_type: vit
swin_backbone_type: tiny
num_classes: 1000
img_h: 224
img_w: 224
num_channels: 3
patch_dim: 16
classes_fraction: 1.0
data_per_class_fraction: 1.0

# others
retro_project_dir: null
retro_add_retriever: false
retro_cyclic_train_iters: null
retro_encoder_layers: 2
retro_encoder_hidden_dropout: 0.1
retro_encoder_attention_dropout: 0.1
retro_num_neighbors: 2
retro_num_retrieved_chunks: 2
retro_attention_gate: 1
retro_verify_neighbor_count: true
hybrid_attention_ratio: 0.0
hybrid_mlp_ratio: 0.0
hybrid_override_pattern: null
dino_local_img_size: 96
dino_local_crops_number: 10
dino_head_hidden_size: 2048
dino_bottleneck_size: 256
dino_freeze_last_layer: 1
dino_norm_last_layer: false
dino_warmup_teacher_temp: 0.04
dino_teacher_temp: 0.07
dino_warmup_teacher_temp_epochs: 30

mask_type: random
mask_factor: 1.0
iter_per_epoch: 1250
qk_layernorm: false
logging_level: null
log_straggler: false
disable_straggler_on_startup: false
straggler_ctrlr_port: 65535
straggler_minmax_count: 1
inference_batch_times_seqlen_threshold: 512
max_tokens_to_oom: 12000
output_bert_embeddings: false
bert_embedder_type: megatron

tiktoken_pattern: null
tiktoken_num_special_tokens: 1000
tiktoken_special_tokens: null
create_attention_mask_in_dataloader: true
num_dataset_builder_threads: 1
s3_cache_path: null
ict_head_size: null
biencoder_projection_dim: 0
biencoder_shared_query_context_model: false
ict_load: null
bert_load: null
titles_data_path: null
query_in_block_prob: 0.1
use_one_sent_docs: false
evidence_data_path: null
retriever_report_topk_accuracies: []
retriever_score_scaling: false
block_data_path: null
embedding_path: null
indexer_batch_size: 128
indexer_log_interval: 1000

parallel_output: false
