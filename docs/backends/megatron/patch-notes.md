## Megatron-LM Patch Notes & Extended Arguments

Primus keeps a curated patch layer on top of upstream Megatron-LM so CLI presets and configs can expose additional controls.
Use this page with:
- [`docs/backends/overview.md`](../overview.md) for shared module parameters
- [`docs/cli/PRIMUS-CLI-GUIDE.md`](../../cli/PRIMUS-CLI-GUIDE.md) for CLI/config usage patterns

> ℹ️ The **Version** column maps to Primus internal patch tags (v0.x.y) so you know when a flag landed.

### 1. Module-Level Parameters

These switches live in Primus’s Megatron wrapper (training loop, logging, resume logic, etc.).

| New Argument                         | Default Value | Version | Description                                                                                    | Patched Files                                                                                                                                                                                                                                                                                                | Notes                                            |
| ------------------------------------ | ------------- | ------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------ |
| `disable_tensorboard`                | `true`        | v0.1.0  | Disable TensorBoard (set to `false` to enable profiling/torch trace).                          | N/A                                                                                                                                                                                                                                                                                                          | Required for timeline/perf debugging.           |
| `disable_wandb`                      | `true`        | v0.1.0  | Disable Weights & Biases logging.                                                              | N/A                                                                                                                                                                                                                                                                                                          | Useful for internal benchmarking.                |
| `disable_compile_dependencies`       | `true`        | v0.1.0  | Skip Megatron custom kernel compilation (Primus already ships the needed ops).                 | N/A                                                                                                                                                                                                                                                                                                          | Avoids redundant compilation steps.              |
| `auto_continue_train`                | `false`       | v0.1.0  | Resume automatically from the latest checkpoint in the `--save` path.                          | N/A                                                                                                                                                                                                                                                                                                          | Simplifies job restarts.                         |
| `disable_last_saving`                | `false`       | v0.1.0  | Skip the final checkpoint write at the last iteration.                                         | N/A                                                                                                                                                                                                                                                                                                          | Handy for profiling-only runs.                   |
| `no_fp8_weight_transpose_cache`      | `false`       | v0.2.0  | Disable the FP8 weight transpose cache to save memory.                                         | `megatron.core.extensions.transformer_engine.TELinear`, `...TELayerNormColumnParallelLinear`, `...TEDelayedScaling`                                                                                                                                                                                           | Trades perf for lower memory usage.              |
| `decoder_pipeline_manual_split_list` | `null`        | v0.2.0  | Manual pipeline splits for interleaved 1F1B pipeline parallelism.                              | `megatron.core.transformer.transformer_block.get_num_layers_to_build`, `megatron.core.transformer.transformer_layer.get_transformer_layer_offset`                                                                                                                                                            | Useful until upstream exposes native hooks.      |
| `pp_warmup`                          | `false`       | v0.2.0  | Add fwd/bwd warmup to reduce the first-iteration cost when PP degree is large.                 | N/A                                                                                                                                                                                                                                                                                                          | Speeds up pipeline debugging.                    |
| `dump_pp_data`                       | `false`       | v0.2.0  | Dump pipeline scheduling data for visualization.                                               | `megatron.core.pipeline_parallel.schedules.forward_step`, `...backward_step`, `...forward_backward_pipelining_with_interleaving`, `...without_interleaving`                                                                                                                                                | Produces artifacts to study schedules.           |
| `disable_profiler_activity_cpu`      | `false`       | v0.2.0  | Skip CPU activity in Torch profiler traces.                                                    | N/A                                                                                                                                                                                                                                                                                                          | Smaller trace files when only GPU kernels matter. |
| `use_rocm_mem_info`                  | `false`       | v0.2.0  | Collect ROCm memory info each iteration via `rocm-smi`.                                        | N/A                                                                                                                                                                                                                                                                                                          | Set true to record every iter; see next flag.     |
| `use_rocm_mem_info_iters`            | `[1,2]`       | v0.2.0  | If `use_rocm_mem_info=false`, collect memory info on these iterations.                         | N/A                                                                                                                                                                                                                                                                                                          | Works even with periodic sampling.               |
| `patch_zero_bubble`                  | `false`       | v0.2.0  | Enable Zero-Bubble pipeline parallelism.                                                       | `megatron.core.optimizer.ChainedOptimizer`, `megatron.core.pipeline_parallel.get_forward_backward_func`, `megatron.core.tensor_parallel.layers.LinearWithGradAccumulationAndAsyncCommunication`, `megatron.core.parallel_stat.*`, `megatron.core.distributed.finalize_model_grads`, `megatron.core.transformer.transformer_layer.get_transformer_layer_offset` | Requires tuned configs; see ZeroBubble guide.    |
| `disable_mlflow`                     | `true`        | v0.3.0  | Toggle MLflow tracking (set to `false` to enable).                                             | N/A                                                                                                                                                                                                                                                                                                          | Needs Databricks env vars; see below.           |

> **MLflow environment variables**
> `export DATABRICKS_TOKEN=…`
> `export DATABRICKS_HOST=…`
> `export MLFLOW_TRACKING_URI=databricks`
> `export MLFLOW_REGISTRY_URI=databricks-uc`
> Optional config keys: `mlflow_run_name`, `mlflow_experiment_name`.

### 2. Model-Definition Parameters

These options change the Megatron model graph (routers, TE layers, MoE dispatcher, etc.).

| New Argument                        | Default Value | Version | Description                                                               | Patched Files                                                                                                                                                                                                                                                                                                                   | Notes                                       |
| ----------------------------------- | ------------- | ------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------- |
| `disable_primus_topk_router`        | `false`       | v0.1.0  | Disable `PrimusTopkRouter` and fall back to Megatron’s default router.    | `megatron.core.transformer.moe.router.TopKRouter`                                                                                                                                                                                                                                                                                | Use for debugging router differences.       |
| `moe_router_force_load_balancing`   | `false`       | v0.1.0  | Force token redistribution to balance MoE experts.                        | `megatron.core.transformer.moe.router.TopKRouter`                                                                                                                                                                                                                                                                                | Helps when diagnosing MoE imbalance.        |
| `use_deprecated_20241209_moe_layer` | `false`       | v0.1.0  | Enable the legacy MoE implementation for perf comparison.                 | `megatron.core.transformer.moe.moe_layer.*`, `megatron.core.transformer.moe.experts.*`, `megatron.core.transformer.moe.router.TopKRouter`                                                                                                                                                                                         | Deprecated; internal testing only.          |
| `moe_permute_fusion`                | `false`       | v0.1.0  | Fuse permutation/unpermutation to reduce kernel launches.                 | `megatron.core.extensions.transformer_engine.*`, `megatron.core.transformer.moe.moe_utils`                                                                                                                                                                                                                                        | Saves launch overhead in MoE layers.        |
| `fused_padded_mla_attention`        | `false`       | v0.1.0  | Pad V head dim to match Q head dim so fused MLA attention can run.        | `megatron.core.transformer.multi_latent_attention.PaddedMLASelfAttention`                                                                                                                                                                                                                                                        | Reduces memory footprint for MLA.           |
| `enable_primus_turbo`               | `false`       | v0.2.0  | Use Primus-Turbo kernels inside Megatron.                                 | `megatron.core.models.gpt.gpt_layer_specs.*`, `megatron.core.models.gpt.gpt_model.tensor_parallel.ColumnParallelLinear`, `megatron.core.models.gpt.moe_module_specs.*`                                                                                                                                                             | See Primus-Turbo config flags for details.   |
| `moe_use_fused_router_with_aux_score` | `false`     | v0.2.0  | Fuse router Top-K and aux loss scoring (requires Primus-Turbo).           | `megatron.core.transformer.moe.router.TopKRouter`                                                                                                                                                                                                                                                                                | Cuts small-kernel overhead.                 |
| `use_turbo_deepep`                  | `false`       | v0.4.0  | Use Primus-Turbo `DeepEPTokenDispatcher` for MoE token dispatch.          | `megatron.core.transformer.moe.token_dispatcher.MoEFlexTokenDispatcher`                                                                                                                                                                                                                                                          | Requires `enable_primus_turbo=true`.        |
| `turbo_deepep_num_cu`               | `32`          | v0.4.0  | Number of compute units used by Turbo DeepEP.                             | `megatron.core.transformer.moe.token_dispatcher.MoEFlexTokenDispatcher`                                                                                                                                                                                                                                                          | 64/80 for ep8; 32 for ep16–64.               |
| `turbo_deepep_use_comm_stream`      | `false`       | v0.4.0  | Use an internal stream for dispatch/combine in DeepEP.                    | `megatron.core.transformer.moe.token_dispatcher.MoEFlexTokenDispatcher`                                                                                                                                                                                                                                                          | Enable after `use_turbo_deepep=true`.       |
| `turbo_sync_free_moe_stage`         | `0`           | v0.4.0  | Enable Primus Sync-Free MoE pipeline stages (0–3).                        | `megatron.core.transformer.moe.token_dispatcher.MoEFlexTokenDispatcher`                                                                                                                                                                                                                                                          | Stage 2 recommended; requires Primus-Turbo. |
## Megatron-LM Patch Notes & Extended Arguments

Primus keeps a maintained fork/patch layer on top of upstream Megatron-LM so the CLI and configuration system can expose additional controls.
Use this page together with:
- [`docs/backends/overview.md`](../overview.md) for shared module parameters
- [`docs/cli/PRIMUS-CLI-GUIDE.md`](../../cli/PRIMUS-CLI-GUIDE.md) for CLI + config examples

> ℹ️ **Version column** follows Primus internal patch releases (v0.x.y) to hint when a flag first appeared.

### 1. Module-Level Parameters

These flags live in the Megatron module glue (training loop, logging, resume logic, etc.).

| New Argument                         | Default Value | Version | Description                                                                                    | Patched Files                                                                                                                                                                                                                                                                                                | Notes                                            |
| ------------------------------------ | ------------- | ------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------ |
| `disable_tensorboard`                | `true`        | v0.1.0  | Disable TensorBoard (set to `false` to enable profiling/torch trace).                          | N/A                                                                                                                                                                                                                                                                                                          | Required for timeline and performance debugging. |
| `disable_wandb`                      | `true`        | v0.1.0  | Disable Weights & Biases logging.                                                              | N/A                                                                                                                                                                                                                                                                                                          | Useful for internal benchmarking.                |
| `disable_compile_dependencies`       | `true`        | v0.1.0  | Skip Megatron custom kernel compilation (Primus already ships the needed ops).                 | N/A                                                                                                                                                                                                                                                                                                          | Avoids redundant compilation steps.              |
| `auto_continue_train`                | `false`       | v0.1.0  | Resume from the latest checkpoint in the `--save` path automatically.                          | N/A                                                                                                                                                                                                                                                                                                          | Simplifies job restarts.                         |
| `disable_last_saving`                | `false`       | v0.1.0  | Skip saving the final checkpoint at the last iteration.                                        | N/A                                                                                                                                                                                                                                                                                                          | Handy for profiling-only runs.                   |
| `no_fp8_weight_transpose_cache`      | `false`       | v0.2.0  | Disable the FP8 weight transpose cache to save memory.                                         | `megatron.core.extensions.transformer_engine.TELinear`, `megatron.core.extensions.transformer_engine.TELayerNormColumnParallelLinear`, `megatron.core.extensions.transformer_engine.TEDelayedScaling`                                                                                                        | Trades perf for lower memory usage.              |
| `decoder_pipeline_manual_split_list` | `null`        | v0.2.0  | Provide manual pipeline splits for interleaved 1F1B pipeline parallelism.                      | `megatron.core.transformer.transformer_block.get_num_layers_to_build`, `megatron.core.transformer.transformer_layer.get_transformer_layer_offset`                                                                                                                                                            | May change once upstream exposes native hooks.   |
| `pp_warmup`                          | `false`       | v0.2.0  | Add fwd/bwd warmup to reduce the first iteration cost when pipeline degree is large.           | N/A                                                                                                                                                                                                                                                                                                          | Speeds up pipeline debugging.                    |
| `dump_pp_data`                       | `false`       | v0.2.0  | Dump pipeline scheduling data for visualization.                                               | `megatron.core.pipeline_parallel.schedules.forward_step`, `...backward_step`, `...forward_backward_pipelining_with_interleaving`, `...without_interleaving`                                                                                                                                                | Produces artifacts to study schedules.           |
| `disable_profiler_activity_cpu`      | `false`       | v0.2.0  | Skip CPU activity in Torch profiler traces.                                                    | N/A                                                                                                                                                                                                                                                                                                          | Smaller trace files when only GPU kernels matter. |
| `use_rocm_mem_info`                  | `false`       | v0.2.0  | Collect ROCm memory info each iteration via `rocm-smi`.                                        | N/A                                                                                                                                                                                                                                                                                                          | Set true to record every iter; see next flag.     |
| `use_rocm_mem_info_iters`            | `[1,2]`       | v0.2.0  | Iterations where ROCm memory info should be collected (when the previous flag is false).       | N/A                                                                                                                                                                                                                                                                                                          | Works even if `use_rocm_mem_info=false`.         |
| `patch_zero_bubble`                  | `false`       | v0.2.0  | Enable Zero-Bubble pipeline parallelism.                                                       | `megatron.core.optimizer.ChainedOptimizer`, `megatron.core.pipeline_parallel.get_forward_backward_func`, `megatron.core.tensor_parallel.layers.LinearWithGradAccumulationAndAsyncCommunication`, `megatron.core.parallel_stat.*`, `megatron.core.distributed.finalize_model_grads`, `megatron.core.transformer.transformer_layer.get_transformer_layer_offset` | Requires tuned configs; see ZeroBubble guide.    |
| `disable_mlflow`                     | `true`        | v0.3.0  | Toggle MLflow tracking (set to `false` to enable).                                             | N/A                                                                                                                                                                                                                                                                                                          | Requires Databricks env vars (see note below).   |
| `patch_zero_bubble` note? already? (duplicate). Need ensure only once. oh already there.* Wait there is `use_rocm...` etc. Continue with remaining rows from README_patch: we already used zero bubble. Continue new ones from table: `disable_profiler...` etc done. Need also `enable_primus_turbo` etc but those are in second table maybe? Actually second table includes `enable_primus_turbo` etc—they belong to model-level. Continue module-level table add note for mlflow envs etc. Need row for `disable_mlflow` with note referencing env variables? Provide described text. Should mention databricks env.
 Add note for databricks env? We'll include in Notes column, referencing env variables.
